/**
 * èŠå¤©æ ¸å¿ƒé€»è¾‘ Hook
 * è´Ÿè´£æ¶ˆæ¯å‘é€ã€åœæ­¢ç”Ÿæˆã€åŠ è½½çŠ¶æ€ç®¡ç†ç­‰æ ¸å¿ƒåŠŸèƒ½
 * 
 * v3.1.0 æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨ Zustand Selectors é¿å…æµå¼è¾“å‡ºæ—¶çš„æ— æ•ˆé‡è®¡ç®—
 * v3.1.1 çŠ¶æ€æœºè§£æï¼šå®æ—¶åˆ†ç¦» thinking æ ‡ç­¾å’Œæ­£æ–‡å†…å®¹
 */

import { useCallback, useRef, useEffect, useState } from 'react'
import { 
  sendMessage as apiSendMessage, 
  resumeChat as apiResumeChat,
  type ApiMessage, 
  type StreamCallback,
  type ResumeChatParams
} from '@/services/chat'
import { normalizeAgentId } from '@/utils/agentUtils'
import { generateUUID } from '@/utils'
import { useTranslation } from '@/i18n'
import type { ExpertEvent } from '@/types'
import { errorHandler, logger } from '@/utils/logger'
import { isValidApiMessageRole } from '@/types'

// Performance Optimized Selectors (v3.1.0)
import {
  useMessages,
  useInputMessage,
  useSelectedAgentId,
  useCurrentConversationId,
  useIsGenerating,
  useChatActions,
} from '@/hooks/useChatSelectors'
import { useTaskMode, useTaskActions } from '@/hooks/useTaskSelectors'
import { useChatStore } from '@/store/chatStore'

// ============================================================================
// v3.1.1: æµå¼å†…å®¹çŠ¶æ€æœºè§£æå™¨
// ç”¨äºå®æ—¶åˆ†ç¦» <think> æ ‡ç­¾å†…å®¹å’Œæ­£æ–‡å†…å®¹
// ============================================================================
interface StreamingParserState {
  isInThinking: boolean
  thinkingBuffer: string
  contentBuffer: string
}

/**
 * å¤„ç†æµå¼ chunkï¼Œåˆ†ç¦» thinking å’Œæ­£æ–‡å†…å®¹
 * è¿”å› { content: æ­£æ–‡å†…å®¹, thinking: thinkingå†…å®¹, hasUpdate: æ˜¯å¦æœ‰æ›´æ–° }
 * 
 * v3.1.1 ä¿®å¤ï¼š
 * - åªåœ¨æµçš„æœ€å¼€å§‹æ£€æŸ¥ JSON å…ƒæ•°æ®ï¼ˆé¿å…è¯¯æ€ä»£ç ä¸­çš„ JSONï¼‰
 * - æ­£ç¡®å¤„ç† chunk ä¸­çš„æ ‡ç­¾åˆ†å‰²
 */
function processStreamingChunk(
  chunk: string,
  state: StreamingParserState,
  isFirstChunk: boolean = false
): { content: string; thinking: string; hasUpdate: boolean } {
  let outputContent = ''
  let outputThinking = ''
  
  // v3.1.1 ä¿®å¤ï¼šåªåœ¨æµçš„ç¬¬ä¸€ä¸ª chunk æ£€æŸ¥ JSON å…ƒæ•°æ®
  // é¿å…è¯¯æ€ AI å›å¤ä¸­çš„åˆæ³• JSON ä»£ç ç¤ºä¾‹
  if (isFirstChunk) {
    const trimmedChunk = chunk.trim()
    // ä¸¥æ ¼åŒ¹é…ï¼šä»¥ { å¼€å¤´ã€åŒ…å« "decision" å­—æ®µã€ä¸”æ˜¯ç³»ç»Ÿå…ƒæ•°æ®æ ¼å¼
    if (trimmedChunk.startsWith('{') && 
        trimmedChunk.includes('"decision"') && 
        trimmedChunk.includes('"decision_type"')) {
      // è¿™æ˜¯ç³»ç»Ÿå…ƒæ•°æ®ï¼Œä¸æ˜¾ç¤ºç»™ç”¨æˆ·
      return { content: '', thinking: '', hasUpdate: false }
    }
  }
  
  // çŠ¶æ€æœºè§£æ
  let i = 0
  while (i < chunk.length) {
    const remainingChunk = chunk.slice(i)
    
    if (!state.isInThinking) {
      // ä¸åœ¨ thinking æ ‡ç­¾å†…ï¼Œæ£€æŸ¥æ˜¯å¦è¿›å…¥
      const thinkStart = remainingChunk.indexOf('<think>')
      const thoughtStart = remainingChunk.indexOf('<thought>')
      
      const nextTagStart = thinkStart !== -1 ? thinkStart : thoughtStart
      const actualTagStart = thoughtStart !== -1 && (thinkStart === -1 || thoughtStart < thinkStart) 
        ? thoughtStart 
        : nextTagStart
      
      if (actualTagStart !== -1) {
        // æ‰¾åˆ°æ ‡ç­¾å¼€å§‹ï¼Œä¹‹å‰çš„å†…å®¹æ˜¯æ­£æ–‡
        outputContent += remainingChunk.slice(0, actualTagStart)
        state.isInThinking = true
        i += actualTagStart + (actualTagStart === thinkStart ? 7 : 9) // <think> æˆ– <thought> çš„é•¿åº¦
      } else {
        // æ²¡æœ‰æ ‡ç­¾ï¼Œå…¨éƒ¨ä½œä¸ºæ­£æ–‡
        outputContent += remainingChunk
        break
      }
    } else {
      // åœ¨ thinking æ ‡ç­¾å†…ï¼Œæ£€æŸ¥æ˜¯å¦é€€å‡º
      const thinkEnd = remainingChunk.indexOf('</think>')
      const thoughtEnd = remainingChunk.indexOf('</thought>')
      
      const nextTagEnd = thinkEnd !== -1 ? thinkEnd : thoughtEnd
      const actualTagEnd = thoughtEnd !== -1 && (thinkEnd === -1 || thoughtEnd < thinkEnd) 
        ? thoughtEnd 
        : nextTagEnd
      
      if (actualTagEnd !== -1) {
        // æ‰¾åˆ°æ ‡ç­¾ç»“æŸï¼Œä¹‹å‰çš„å†…å®¹æ˜¯ thinking
        outputThinking += remainingChunk.slice(0, actualTagEnd)
        state.isInThinking = false
        i += actualTagEnd + (actualTagEnd === thinkEnd ? 8 : 10) // </think> æˆ– </thought> çš„é•¿åº¦
      } else {
        // æ²¡æœ‰ç»“æŸæ ‡ç­¾ï¼Œå…¨éƒ¨ä½œä¸º thinking
        outputThinking += remainingChunk
        break
      }
    }
  }
  
  // æ›´æ–°çŠ¶æ€ç¼“å†²
  state.contentBuffer += outputContent
  state.thinkingBuffer += outputThinking
  
  return {
    content: outputContent,
    thinking: outputThinking,
    hasUpdate: outputContent.length > 0 || outputThinking.length > 0
  }
}

/**
 * é‡ç½®è§£æå™¨çŠ¶æ€
 */
function resetStreamingParser(state: StreamingParserState): void {
  state.isInThinking = false
  state.thinkingBuffer = ''
  state.contentBuffer = ''
}

// Dev environment check
const DEBUG = import.meta.env.VITE_DEBUG_MODE === 'true'

// Unified debug log function
const debug = DEBUG
  ? (...args: unknown[]) => logger.debug('[useChatCore]', ...args)
  : () => {}

/**
 * ApiMessage type guard function
 */
function isApiMessage(obj: any): obj is ApiMessage {
  return (
    obj &&
    typeof obj === 'object' &&
    'role' in obj &&
    'content' in obj &&
    isValidApiMessageRole(obj.role) &&
    typeof obj.content === 'string'
  )
}

interface UseChatCoreOptions {
  /** Handle expert event callback */
  onExpertEvent?: (event: ExpertEvent, conversationMode: 'simple' | 'complex') => Promise<void> | void
  /** Handle streaming content callback */
  onChunk?: (chunk: string) => void
  /** New conversation created callback */
  onNewConversation?: (conversationId: string, agentId: string) => void
}

/**
 * Chat core logic Hook
 */
export function useChatCore(options: UseChatCoreOptions = {}) {
  const { t } = useTranslation()
  const { onExpertEvent, onChunk, onNewConversation } = options

  // Refactored: Hook only manages AbortController
  const abortControllerRef = useRef<AbortController | null>(null)
  
  // Performance Optimized Selectors (v3.1.0)
  const conversationMode = useTaskMode() || 'simple'
  
  // Chat store selectors
  const messages = useMessages()
  const inputMessage = useInputMessage()
  const selectedAgentId = useSelectedAgentId()
  const currentConversationId = useCurrentConversationId()
  const isGenerating = useIsGenerating()
  
  // Actions
  const { 
    setInputMessage, 
    setCurrentConversationId, 
    addMessage, 
    updateMessage, 
    setMessages, 
    setGenerating 
  } = useChatActions()
  
  const { setMode } = useTaskActions()

  /**
   * Stop generation
   */
  const stopGeneration = useCallback(() => {
    if (abortControllerRef.current) {
      debug('Stop generation')
      abortControllerRef.current.abort()
    }
  }, [])

  /**
   * Send message core logic
   */
  const sendMessageCore = useCallback(async (
    content?: string,
    overrideAgentId?: string
  ) => {
    // Deduplication: prevent duplicate submissions
    if (isGenerating) {
      debug('Request in progress, ignoring duplicate submission')
      return
    }

    const userContent = (content || inputMessage || '').trim()
    if (!userContent) {
      debug('Message content is empty, skipping send')
      return
    }

    setGenerating(true)
    
    // Reset taskStore mode, wait for backend Router decision
    setMode('simple')
    
    // v3.1.1: åˆå§‹åŒ–æµå¼è§£æå™¨çŠ¶æ€
    const streamingParserState: StreamingParserState = {
      isInThinking: false,
      thinkingBuffer: '',
      contentBuffer: ''
    }

    const agentId = overrideAgentId || selectedAgentId
    if (!agentId) {
      logger.error('[useChatCore] No agent selected')
      setGenerating(false)
      return
    }
    const normalizedAgentId = normalizeAgentId(agentId)

    abortControllerRef.current = new AbortController()

    let assistantMessageId: string | undefined

    try {
      const storeState = useChatStore.getState()
      const validHistoryMessages = storeState.messages
        .filter((m): m is Message & { content: string } => 
          !!m && typeof m.content === 'string' && m.content.length > 0
        )
        .map((m): ApiMessage => ({
          role: m.role as 'user' | 'assistant',
          content: m.content
        }))
      
      const chatMessages: ApiMessage[] = [
        ...validHistoryMessages,
        { role: 'user', content: userContent }
      ]

      debug('Preparing to send message, history count:', storeState.messages.length, 'Current input:', userContent)

      assistantMessageId = generateUUID()
      debug('Preparing to add message, AI ID:', assistantMessageId, 'Type:', typeof assistantMessageId)

      const routingStepId = generateUUID()
      setMessages([...storeState.messages,
        { role: 'user', content: userContent },
        {
          id: assistantMessageId,
          role: 'assistant',
          content: '',
          timestamp: Date.now(),
          metadata: {
            thinking: [{
              id: routingStepId,
              expertType: 'router',
              expertName: 'æ™ºèƒ½è·¯ç”±',
              content: 'æ­£åœ¨åˆ†ææ„å›¾ï¼Œé€‰æ‹©æ‰§è¡Œæ¨¡å¼...',
              timestamp: new Date().toISOString(),
              status: 'running' as const,
              type: 'analysis' as const
            }]
          }
        }
      ])

      setInputMessage('')

      let finalResponseContent = ''
      const storeState2 = useChatStore.getState()
      let actualConversationId = storeState2.currentConversationId || currentConversationId

      debug('Preparing to call sendMessage')

      let hasProcessedComplexMode = false
      let isFirstChunk = true  // v3.1.1: æ ‡è®°æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ª chunk
      let hasCompletedRouterThinking = false  // ğŸ”¥ æ–°å¢ï¼šæ ‡è®°æ˜¯å¦å·²å®Œæˆè·¯ç”±åˆ†ææ­¥éª¤

      const streamCallback: StreamCallback = async (
        chunk: string | undefined,
        conversationId?: string,
        expertEvent?: ExpertEvent
      ) => {
        if (conversationId && conversationId !== actualConversationId) {
          actualConversationId = conversationId
          setCurrentConversationId(conversationId)
        }

        if (expertEvent) {
          onExpertEvent?.(expertEvent as any, conversationMode)
        }

        if (chunk) {
          // ğŸ”¥ ä¿®å¤ï¼šæ”¶åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆchunkæ—¶ï¼Œå¯¹äºç®€å•æ¨¡å¼å®Œæˆè·¯ç”±åˆ†ææ­¥éª¤
          if (isFirstChunk && !hasCompletedRouterThinking && assistantMessageId) {
            const currentMode = useTaskStore.getState().mode || 'simple'
            if (currentMode === 'simple') {
              const { messages, updateMessageMetadata } = useChatStore.getState()
              const message = messages.find(m => m.id === assistantMessageId)
              if (message?.metadata?.thinking) {
                const thinking = [...message.metadata.thinking]
                const routerStepIndex = thinking.findIndex((s: any) => s.expertType === 'router')
                if (routerStepIndex >= 0) {
                  thinking[routerStepIndex] = {
                    ...thinking[routerStepIndex],
                    status: 'completed',
                    content: 'æ„å›¾åˆ†æå®Œæˆï¼šå·²é€‰æ‹©ç®€å•æ¨¡å¼'
                  }
                  updateMessageMetadata(assistantMessageId, { thinking })
                  hasCompletedRouterThinking = true
                }
              }
            }
          }

          // v3.1.1: ä½¿ç”¨çŠ¶æ€æœºè§£æå™¨åˆ†ç¦» thinking å’Œæ­£æ–‡å†…å®¹
          const { content, thinking } = processStreamingChunk(chunk, streamingParserState, isFirstChunk)
          
          // æ ‡è®°ç¬¬ä¸€ä¸ª chunk å·²å¤„ç†
          if (isFirstChunk) {
            isFirstChunk = false
          }
          
          // ç´¯ç§¯å®Œæ•´å“åº”ï¼ˆåŒ…æ‹¬ thinkingï¼Œç”¨äºæœ€ç»ˆä¿å­˜ï¼‰
          finalResponseContent += chunk
          
          if (DEBUG) {
            logger.debug('[useChatCore] Received chunk, raw:', chunk.length, 'content:', content.length, 'thinking:', thinking.length, 'Message ID:', assistantMessageId)
          }
          
          // åªå°†æ­£æ–‡å†…å®¹ä¼ é€’ç»™ UI æ˜¾ç¤º
          if (content) {
            onChunk?.(content)
          }
          
          // å¦‚æœæœ‰ thinking å†…å®¹ï¼Œå®æ—¶æ›´æ–°åˆ°æ¶ˆæ¯ metadata
          if (thinking && assistantMessageId) {
            const { messages, updateMessageMetadata } = useChatStore.getState()
            const message = messages.find(m => m.id === assistantMessageId)
            if (message) {
              const existingThinking = message.metadata?.thinking || []
              // æŸ¥æ‰¾æˆ–åˆ›å»º thinking step
              const thinkStepIndex = existingThinking.findIndex((s: any) => s.id === 'streaming-think')
              let newThinking
              
              if (thinkStepIndex >= 0) {
                // è¿½åŠ åˆ°ç°æœ‰ thinking step
                newThinking = [...existingThinking]
                newThinking[thinkStepIndex] = {
                  ...newThinking[thinkStepIndex],
                  content: newThinking[thinkStepIndex].content + thinking
                }
              } else {
                // åˆ›å»ºæ–°çš„ thinking step
                newThinking = [...existingThinking, {
                  id: 'streaming-think',
                  expertType: 'thinking',
                  expertName: 'æ€è€ƒè¿‡ç¨‹',
                  content: thinking,
                  timestamp: new Date().toISOString(),
                  status: 'running',
                  type: 'default'
                }]
              }
              
              updateMessageMetadata(assistantMessageId, { thinking: newThinking })
            }
          }
        }
      }

      finalResponseContent = await apiSendMessage(
        chatMessages,
        normalizedAgentId,
        streamCallback,
        actualConversationId,
        abortControllerRef.current.signal,
        assistantMessageId
      )

      const storeState3 = useChatStore.getState()
      const initialConversationId = storeState3.currentConversationId
      if (actualConversationId !== initialConversationId) {
        onNewConversation?.(actualConversationId, selectedAgentId)
      }

      debug(`Task completed, final content length: ${finalResponseContent?.length || 0}`)

      return finalResponseContent

    } catch (error) {
      const isAbortError = 
        (error instanceof Error && error.name === 'AbortError') ||
        (error instanceof Error && error.message?.toLowerCase().includes('abort')) ||
        (error instanceof Error && error.message?.toLowerCase().includes('cancel')) ||
        abortControllerRef.current?.signal.aborted
      
      if (isAbortError) {
        debug('Request cancelled (user initiated)')
        if (assistantMessageId) {
          updateMessage(assistantMessageId, '', false)
        }
      } else {
        errorHandler.handle(error, 'sendMessageCore')

        const userMessage = errorHandler.getUserMessage(error)
        addMessage({
          role: 'assistant',
          content: userMessage
        })
      }
    } finally {
      setGenerating(false)
      abortControllerRef.current = null

      if (conversationMode === 'complex' && assistantMessageId) {
        const currentMessages = useChatStore.getState().messages
        const assistantMsg = currentMessages.find(m => m.id === assistantMessageId)
        if (assistantMsg && !assistantMsg.content?.trim()) {
          debug('Complex mode: keep empty AI message waiting for aggregator summary', assistantMessageId)
        }
      }
    }
  }, [
    isGenerating,
    inputMessage,
    selectedAgentId,
    currentConversationId,
    conversationMode,
    onExpertEvent,
    onChunk,
    onNewConversation,
    setGenerating,
    setMode,
    setMessages,
    setInputMessage,
    setCurrentConversationId,
    addMessage,
    updateMessage,
    t
  ])

  // Page visibility and lifecycle management
  const isPageHiddenRef = useRef(false)
  
  useEffect(() => {
    const handleVisibilityChange = () => {
      if (document.hidden) {
        isPageHiddenRef.current = true
        debug('Page hidden, keeping SSE connection')
      } else {
        isPageHiddenRef.current = false
        debug('Page visible, resuming UI updates')
      }
    }

    document.addEventListener('visibilitychange', handleVisibilityChange)
    
    return () => {
      document.removeEventListener('visibilitychange', handleVisibilityChange)
      if (abortControllerRef.current) {
        debug('Component unmounting, aborting ongoing request')
        abortControllerRef.current.abort()
        abortControllerRef.current = null
      }
    }
  }, [])

  /**
   * v3.1.0 HITL: Resume interrupted execution flow
   */
  const resumeExecution = useCallback(async (
    params: ResumeChatParams
  ): Promise<string> => {
    if (isGenerating) {
      debug('Request in progress, ignoring duplicate resume request')
      return ''
    }

    setGenerating(true)
    abortControllerRef.current = new AbortController()

    let fullContent = ''
    
    // v3.1.1: åˆå§‹åŒ–æµå¼è§£æå™¨çŠ¶æ€
    const streamingParserState: StreamingParserState = {
      isInThinking: false,
      thinkingBuffer: '',
      contentBuffer: ''
    }
    let isFirstChunk = true  // v3.1.1: æ ‡è®°æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ª chunk

    try {
      const streamCallback: StreamCallback = async (
        chunk: string | undefined,
        conversationId?: string,
        expertEvent?: ExpertEvent
      ) => {
        if (expertEvent) {
          onExpertEvent?.(expertEvent as any, conversationMode)
        }

        if (chunk) {
          // v3.1.1: ä½¿ç”¨çŠ¶æ€æœºè§£æå™¨åˆ†ç¦» thinking å’Œæ­£æ–‡å†…å®¹
          const { content, thinking } = processStreamingChunk(chunk, streamingParserState, isFirstChunk)
          
          // æ ‡è®°ç¬¬ä¸€ä¸ª chunk å·²å¤„ç†
          if (isFirstChunk) {
            isFirstChunk = false
          }
          
          // ç´¯ç§¯å®Œæ•´å“åº”
          fullContent += chunk
          
          // åªå°†æ­£æ–‡å†…å®¹ä¼ é€’ç»™ UI æ˜¾ç¤º
          if (content) {
            onChunk?.(content)
          }
          
          // å¦‚æœæœ‰ thinking å†…å®¹ï¼Œå®æ—¶æ›´æ–°åˆ°æ¶ˆæ¯ metadata
          if (thinking) {
            const { messages, updateMessageMetadata } = useChatStore.getState()
            // æŸ¥æ‰¾æœ€åä¸€æ¡ AI æ¶ˆæ¯
            const lastAiMessage = [...messages].reverse().find(m => m.role === 'assistant')
            
            if (lastAiMessage) {
              const existingThinking = lastAiMessage.metadata?.thinking || []
              const thinkStepIndex = existingThinking.findIndex((s: any) => s.id === 'streaming-think')
              let newThinking
              
              if (thinkStepIndex >= 0) {
                newThinking = [...existingThinking]
                newThinking[thinkStepIndex] = {
                  ...newThinking[thinkStepIndex],
                  content: newThinking[thinkStepIndex].content + thinking
                }
              } else {
                newThinking = [...existingThinking, {
                  id: 'streaming-think',
                  expertType: 'thinking',
                  expertName: 'æ€è€ƒè¿‡ç¨‹',
                  content: thinking,
                  timestamp: new Date().toISOString(),
                  status: 'running',
                  type: 'default'
                }]
              }
              
              updateMessageMetadata(lastAiMessage.id!, { thinking: newThinking })
            }
          }
        }
      }

      fullContent = await apiResumeChat(
        params,
        streamCallback,
        abortControllerRef.current.signal
      )

      return fullContent

    } catch (error) {
      const isAbortError = 
        (error instanceof Error && error.name === 'AbortError') ||
        (error instanceof Error && error.message?.toLowerCase().includes('abort')) ||
        abortControllerRef.current?.signal.aborted

      if (!isAbortError) {
        errorHandler.handle(error, 'resumeExecution')
        addMessage({
          role: 'assistant',
          content: errorHandler.getUserMessage(error)
        })
      }
      
      throw error
    } finally {
      setGenerating(false)
      abortControllerRef.current = null
    }

    return fullContent
  }, [isGenerating, conversationMode, onExpertEvent, onChunk, setGenerating, addMessage])

  return {
    sendMessage: sendMessageCore,
    stopGeneration,
    resumeExecution,
    conversationMode,
  }
}
