/**
 * èŠå¤©æ ¸å¿ƒé€»è¾‘ Hook
 * è´Ÿè´£æ¶ˆæ¯å‘é€ã€åœæ­¢ç”Ÿæˆã€åŠ è½½çŠ¶æ€ç®¡ç†ç­‰æ ¸å¿ƒåŠŸèƒ½
 * 
 * v3.1.0 æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨ Zustand Selectors é¿å…æµå¼è¾“å‡ºæ—¶çš„æ— æ•ˆé‡è®¡ç®—
 * v3.1.1 çŠ¶æ€æœºè§£æï¼šå®æ—¶åˆ†ç¦» thinking æ ‡ç­¾å’Œæ­£æ–‡å†…å®¹
 */

import { useCallback, useRef, useEffect, useState } from 'react'
import { 
  sendMessage as apiSendMessage, 
  resumeChat as apiResumeChat,
  type ApiMessage, 
  type StreamCallback,
  type ResumeChatParams
} from '@/services/chat'
import { normalizeAgentId, getAgentType } from '@/utils/agentUtils'
import { generateUUID } from '@/utils'
import { useTranslation } from '@/i18n'
import type { ExpertEvent } from '@/types'
import { errorHandler, logger } from '@/utils/logger'
import { isValidApiMessageRole } from '@/types'

// Performance Optimized Selectors (v3.1.0)
import {
  useMessages,
  useInputMessage,
  useSelectedAgentId,
  useCurrentConversationId,
  useIsGenerating,
  useChatActions,
} from '@/hooks/useChatSelectors'
import { useTaskMode, useTaskActions } from '@/hooks/useTaskSelectors'
import { useChatStore } from '@/store/chatStore'

// ============================================================================
// v3.1.1: æµå¼å†…å®¹çŠ¶æ€æœºè§£æå™¨
// ç”¨äºå®æ—¶åˆ†ç¦» <think> æ ‡ç­¾å†…å®¹å’Œæ­£æ–‡å†…å®¹
// ============================================================================
interface StreamingParserState {
  isInThinking: boolean
  thinkingBuffer: string
  contentBuffer: string
}

/**
 * å¤„ç†æµå¼ chunkï¼Œåˆ†ç¦» thinking å’Œæ­£æ–‡å†…å®¹
 * è¿”å› { content: æ­£æ–‡å†…å®¹, thinking: thinkingå†…å®¹, hasUpdate: æ˜¯å¦æœ‰æ›´æ–° }
 * 
 * v3.1.1 ä¿®å¤ï¼š
 * - åªåœ¨æµçš„æœ€å¼€å§‹æ£€æŸ¥ JSON å…ƒæ•°æ®ï¼ˆé¿å…è¯¯æ€ä»£ç ä¸­çš„ JSONï¼‰
 * - æ­£ç¡®å¤„ç† chunk ä¸­çš„æ ‡ç­¾åˆ†å‰²
 */
function processStreamingChunk(
  chunk: string,
  state: StreamingParserState,
  isFirstChunk: boolean = false
): { content: string; thinking: string; hasUpdate: boolean } {
  let outputContent = ''
  let outputThinking = ''
  
  // v3.1.1 ä¿®å¤ï¼šåªåœ¨æµçš„ç¬¬ä¸€ä¸ª chunk æ£€æŸ¥å¹¶è¿‡æ»¤ç³»ç»Ÿå…ƒæ•°æ®
  // é¿å…è¯¯æ€ AI å›å¤ä¸­çš„åˆæ³• JSON ä»£ç ç¤ºä¾‹
  if (isFirstChunk) {
    const trimmedChunk = chunk.trim()
    
    // ğŸ”¥ğŸ”¥ğŸ”¥ é™å™ªï¼šä½¿ç”¨æ­£åˆ™åŒ¹é…å¹¶ç§»é™¤ Router å†³ç­– JSON
    // åŒ¹é… {"decision_type": "complex"} æˆ– {"decision_type":"complex"}xxx
    const decisionJsonRegex = /^\s*\{\s*"decision_type"\s*:\s*"[^"]+"\s*\}/
    if (decisionJsonRegex.test(trimmedChunk)) {
      // ç§»é™¤ JSON éƒ¨åˆ†ï¼Œä¿ç•™åé¢çš„å†…å®¹
      chunk = trimmedChunk.replace(decisionJsonRegex, '').trim()
      // å¦‚æœç§»é™¤åæ²¡æœ‰å†…å®¹äº†ï¼Œç›´æ¥è¿”å›ç©º
      if (!chunk) {
        return { content: '', thinking: '', hasUpdate: false }
      }
    }
    
    // ğŸ”¥ğŸ”¥ğŸ”¥ é™å™ªï¼šè¿‡æ»¤ç³»ç»Ÿæ—¥å¿—å’Œå·¥å…·è°ƒç”¨æç¤º
    // è¿™äº›ä¸åº”è¯¥å±•ç¤ºç»™ç”¨æˆ·ï¼Œåªåº”è¯¥åœ¨ ThinkingProcess ä¸­æ˜¾ç¤º
    const systemLogPatterns = [
      // Expert thinking æ—¥å¿—
      /^(Expert|ä¸“å®¶)\s+\w+\s+(is\s+thinking|thinking|æ€è€ƒä¸­)/i,
      // å·¥å…·è°ƒç”¨æç¤º
      /^(Calling|è°ƒç”¨)\s+(tool|å·¥å…·)\s*:/i,
      // åŸå§‹ Prompt ä¼ é€’æ ‡è®°
      /^---\s*Prompt\s*---/i,
      /^---\s*System\s*Message\s*---/i,
    ]
    
    for (const pattern of systemLogPatterns) {
      if (pattern.test(trimmedChunk)) {
        // è¿™æ˜¯ç³»ç»Ÿæ—¥å¿—ï¼Œè¿‡æ»¤æ‰ä½†æ ‡è®°ä¸º thinking å†…å®¹ä¾› ThinkingProcess ä½¿ç”¨
        return { content: '', thinking: trimmedChunk, hasUpdate: true }
      }
    }
  }
  
  // çŠ¶æ€æœºè§£æ
  let i = 0
  while (i < chunk.length) {
    const remainingChunk = chunk.slice(i)
    
    if (!state.isInThinking) {
      // ä¸åœ¨ thinking æ ‡ç­¾å†…ï¼Œæ£€æŸ¥æ˜¯å¦è¿›å…¥
      const thinkStart = remainingChunk.indexOf('<think>')
      const thoughtStart = remainingChunk.indexOf('<thought>')
      
      const nextTagStart = thinkStart !== -1 ? thinkStart : thoughtStart
      const actualTagStart = thoughtStart !== -1 && (thinkStart === -1 || thoughtStart < thinkStart) 
        ? thoughtStart 
        : nextTagStart
      
      if (actualTagStart !== -1) {
        // æ‰¾åˆ°æ ‡ç­¾å¼€å§‹ï¼Œä¹‹å‰çš„å†…å®¹æ˜¯æ­£æ–‡
        outputContent += remainingChunk.slice(0, actualTagStart)
        state.isInThinking = true
        i += actualTagStart + (actualTagStart === thinkStart ? 7 : 9) // <think> æˆ– <thought> çš„é•¿åº¦
      } else {
        // æ²¡æœ‰æ ‡ç­¾ï¼Œå…¨éƒ¨ä½œä¸ºæ­£æ–‡
        outputContent += remainingChunk
        break
      }
    } else {
      // åœ¨ thinking æ ‡ç­¾å†…ï¼Œæ£€æŸ¥æ˜¯å¦é€€å‡º
      const thinkEnd = remainingChunk.indexOf('</think>')
      const thoughtEnd = remainingChunk.indexOf('</thought>')
      
      const nextTagEnd = thinkEnd !== -1 ? thinkEnd : thoughtEnd
      const actualTagEnd = thoughtEnd !== -1 && (thinkEnd === -1 || thoughtEnd < thinkEnd) 
        ? thoughtEnd 
        : nextTagEnd
      
      if (actualTagEnd !== -1) {
        // æ‰¾åˆ°æ ‡ç­¾ç»“æŸï¼Œä¹‹å‰çš„å†…å®¹æ˜¯ thinking
        outputThinking += remainingChunk.slice(0, actualTagEnd)
        state.isInThinking = false
        i += actualTagEnd + (actualTagEnd === thinkEnd ? 8 : 10) // </think> æˆ– </thought> çš„é•¿åº¦
      } else {
        // æ²¡æœ‰ç»“æŸæ ‡ç­¾ï¼Œå…¨éƒ¨ä½œä¸º thinking
        outputThinking += remainingChunk
        break
      }
    }
  }
  
  // æ›´æ–°çŠ¶æ€ç¼“å†²
  state.contentBuffer += outputContent
  state.thinkingBuffer += outputThinking
  
  return {
    content: outputContent,
    thinking: outputThinking,
    hasUpdate: outputContent.length > 0 || outputThinking.length > 0
  }
}

/**
 * é‡ç½®è§£æå™¨çŠ¶æ€
 */
function resetStreamingParser(state: StreamingParserState): void {
  state.isInThinking = false
  state.thinkingBuffer = ''
  state.contentBuffer = ''
}

// Dev environment check
const DEBUG = import.meta.env.VITE_DEBUG_MODE === 'true'

// Unified debug log function
const debug = DEBUG
  ? (...args: unknown[]) => logger.debug('[useChatCore]', ...args)
  : () => {}

/**
 * ApiMessage type guard function
 */
function isApiMessage(obj: any): obj is ApiMessage {
  return (
    obj &&
    typeof obj === 'object' &&
    'role' in obj &&
    'content' in obj &&
    isValidApiMessageRole(obj.role) &&
    typeof obj.content === 'string'
  )
}

interface UseChatCoreOptions {
  /** Handle expert event callback */
  onExpertEvent?: (event: ExpertEvent, conversationMode: 'simple' | 'complex') => Promise<void> | void
  /** Handle streaming content callback */
  onChunk?: (chunk: string) => void
  /** New conversation created callback */
  onNewConversation?: (conversationId: string, agentId: string) => void
}

/**
 * Chat core logic Hook
 */
export function useChatCore(options: UseChatCoreOptions = {}) {
  const { t } = useTranslation()
  const { onExpertEvent, onChunk, onNewConversation } = options

  // Refactored: Hook only manages AbortController
  const abortControllerRef = useRef<AbortController | null>(null)
  
  // Performance Optimized Selectors (v3.1.0)
  const conversationMode = useTaskMode() || 'simple'
  
  // Chat store selectors
  const messages = useMessages()
  const inputMessage = useInputMessage()
  const selectedAgentId = useSelectedAgentId()
  const currentConversationId = useCurrentConversationId()
  const isGenerating = useIsGenerating()
  
  // Actions
  const { 
    setInputMessage, 
    setCurrentConversationId, 
    addMessage, 
    updateMessage,
    updateMessageMetadata,
    setMessages, 
    setGenerating 
  } = useChatActions()
  
  const { setMode } = useTaskActions()

  /**
   * Stop generation
   */
  const stopGeneration = useCallback(() => {
    if (abortControllerRef.current) {
      debug('Stop generation')
      abortControllerRef.current.abort()
    }
  }, [])

  /**
   * Send message core logic
   */
  const sendMessageCore = useCallback(async (
    content?: string,
    overrideAgentId?: string
  ) => {
    // Deduplication: prevent duplicate submissions
    if (isGenerating) {
      debug('Request in progress, ignoring duplicate submission')
      return
    }

    const userContent = (content || inputMessage || '').trim()
    if (!userContent) {
      debug('Message content is empty, skipping send')
      return
    }

    setGenerating(true)
    
    // Reset taskStore mode, wait for backend Router decision
    setMode('simple')
    
    // v3.1.1: åˆå§‹åŒ–æµå¼è§£æå™¨çŠ¶æ€
    const streamingParserState: StreamingParserState = {
      isInThinking: false,
      thinkingBuffer: '',
      contentBuffer: ''
    }

    const agentId = overrideAgentId || selectedAgentId
    if (!agentId) {
      logger.error('[useChatCore] No agent selected')
      setGenerating(false)
      return
    }
    const normalizedAgentId = normalizeAgentId(agentId)

    abortControllerRef.current = new AbortController()

    let assistantMessageId: string | undefined

    try {
      const storeState = useChatStore.getState()
      const validHistoryMessages = storeState.messages
        .filter((m): m is Message & { content: string } => 
          !!m && typeof m.content === 'string' && m.content.length > 0
        )
        .map((m): ApiMessage => ({
          role: m.role as 'user' | 'assistant',
          content: m.content
        }))
      
      const chatMessages: ApiMessage[] = [
        ...validHistoryMessages,
        { role: 'user', content: userContent }
      ]

      debug('Preparing to send message, history count:', storeState.messages.length, 'Current input:', userContent)

      assistantMessageId = generateUUID()
      debug('Preparing to add message, AI ID:', assistantMessageId, 'Type:', typeof assistantMessageId)

      const agentType = getAgentType(normalizedAgentId)
      debug('Agent type:', agentType, 'Agent ID:', normalizedAgentId)

      // ğŸ”¥ åªæœ‰ç³»ç»Ÿæ™ºèƒ½ä½“æ‰åˆ›å»º router thinking æ­¥éª¤
      // è‡ªå®šä¹‰æ™ºèƒ½ä½“ç›´æ¥èµ° LLMï¼Œä¸ç»è¿‡ LangGraph Router
      const initialThinking = agentType === 'system' ? [{
        id: generateUUID(),
        expertType: 'router',
        expertName: 'æ™ºèƒ½è·¯ç”±',
        content: 'æ­£åœ¨åˆ†ææ„å›¾ï¼Œé€‰æ‹©æ‰§è¡Œæ¨¡å¼...',
        timestamp: new Date().toISOString(),
        status: 'running' as const,
        type: 'analysis' as const
      }] : []

      setMessages([...storeState.messages,
        { role: 'user', content: userContent },
        {
          id: assistantMessageId,
          role: 'assistant',
          content: '',
          timestamp: Date.now(),
          metadata: {
            thinking: initialThinking
          }
        }
      ])

      setInputMessage('')

      let finalResponseContent = ''
      const storeState2 = useChatStore.getState()
      let actualConversationId = storeState2.currentConversationId || currentConversationId

      debug('Preparing to call sendMessage')

      let hasProcessedComplexMode = false
      let isFirstChunk = true  // v3.1.1: æ ‡è®°æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ª chunk

      const streamCallback: StreamCallback = async (
        chunk: string | undefined,
        conversationId?: string,
        expertEvent?: ExpertEvent
      ) => {
        if (conversationId && conversationId !== actualConversationId) {
          actualConversationId = conversationId
          setCurrentConversationId(conversationId)
        }

        if (expertEvent) {
          onExpertEvent?.(expertEvent as any, conversationMode)
        }

        if (chunk) {
          // v3.1.1: ä½¿ç”¨çŠ¶æ€æœºè§£æå™¨åˆ†ç¦» thinking å’Œæ­£æ–‡å†…å®¹
          const { content, thinking } = processStreamingChunk(chunk, streamingParserState, isFirstChunk)
          
          // æ ‡è®°ç¬¬ä¸€ä¸ª chunk å·²å¤„ç†
          if (isFirstChunk) {
            isFirstChunk = false
          }
          
          // ç´¯ç§¯å®Œæ•´å“åº”ï¼ˆåŒ…æ‹¬ thinkingï¼Œç”¨äºæœ€ç»ˆä¿å­˜ï¼‰
          finalResponseContent += chunk
          
          if (DEBUG) {
            logger.debug('[useChatCore] Received chunk, raw:', chunk.length, 'content:', content.length, 'thinking:', thinking.length, 'Message ID:', assistantMessageId)
          }
          
          // åªå°†æ­£æ–‡å†…å®¹ä¼ é€’ç»™ UI æ˜¾ç¤º
          if (content) {
            onChunk?.(content)
            
            // ğŸ”¥ å…³é”®ä¿®å¤ï¼šå®æ—¶æ›´æ–°æ¶ˆæ¯å†…å®¹åˆ° chatStore
            if (assistantMessageId) {
              updateMessage(assistantMessageId, content, true)
            }
          }
          
          // å¦‚æœæœ‰ thinking å†…å®¹ï¼Œå®æ—¶æ›´æ–°åˆ°æ¶ˆæ¯ metadata
          // ğŸ”¥ ä½¿ç”¨ç´¯ç§¯çš„ thinkingBuffer è€Œä¸æ˜¯æœ¬æ¬¡ chunk çš„ thinking
          if (streamingParserState.thinkingBuffer && assistantMessageId) {
            updateMessageMetadata(assistantMessageId, {
              thinking: [{
                id: 'streaming-think',
                expertType: 'thinking',
                expertName: 'æ€è€ƒè¿‡ç¨‹',
                content: streamingParserState.thinkingBuffer,
                timestamp: new Date().toISOString(),
                status: 'running',
                type: 'default'
              }]
            })
          }
        }
      }

      finalResponseContent = await apiSendMessage(
        chatMessages,
        normalizedAgentId,
        streamCallback,
        actualConversationId,
        abortControllerRef.current.signal,
        assistantMessageId
      )

      const storeState3 = useChatStore.getState()
      const initialConversationId = storeState3.currentConversationId
      if (actualConversationId !== initialConversationId) {
        onNewConversation?.(actualConversationId, selectedAgentId)
      }

      debug(`Task completed, final content length: ${finalResponseContent?.length || 0}`)

      return finalResponseContent

    } catch (error) {
      const isAbortError = 
        (error instanceof Error && error.name === 'AbortError') ||
        (error instanceof Error && error.message?.toLowerCase().includes('abort')) ||
        (error instanceof Error && error.message?.toLowerCase().includes('cancel')) ||
        abortControllerRef.current?.signal.aborted
      
      if (isAbortError) {
        debug('Request cancelled (user initiated)')
        if (assistantMessageId) {
          updateMessage(assistantMessageId, '', false)
        }
      } else {
        errorHandler.handle(error, 'sendMessageCore')

        const userMessage = errorHandler.getUserMessage(error)
        addMessage({
          role: 'assistant',
          content: userMessage
        })
      }
    } finally {
      setGenerating(false)
      abortControllerRef.current = null

      if (conversationMode === 'complex' && assistantMessageId) {
        const currentMessages = useChatStore.getState().messages
        const assistantMsg = currentMessages.find(m => m.id === assistantMessageId)
        if (assistantMsg && !assistantMsg.content?.trim()) {
          debug('Complex mode: keep empty AI message waiting for aggregator summary', assistantMessageId)
        }
      }
    }
  }, [
    isGenerating,
    inputMessage,
    selectedAgentId,
    currentConversationId,
    conversationMode,
    onExpertEvent,
    onChunk,
    onNewConversation,
    setGenerating,
    setMode,
    setMessages,
    setInputMessage,
    setCurrentConversationId,
    addMessage,
    updateMessage,
    t
  ])

  // Page visibility and lifecycle management
  const isPageHiddenRef = useRef(false)
  
  useEffect(() => {
    const handleVisibilityChange = () => {
      if (document.hidden) {
        isPageHiddenRef.current = true
        debug('Page hidden, keeping SSE connection')
      } else {
        isPageHiddenRef.current = false
        debug('Page visible, resuming UI updates')
      }
    }

    document.addEventListener('visibilitychange', handleVisibilityChange)
    
    return () => {
      document.removeEventListener('visibilitychange', handleVisibilityChange)
      if (abortControllerRef.current) {
        debug('Component unmounting, aborting ongoing request')
        abortControllerRef.current.abort()
        abortControllerRef.current = null
      }
    }
  }, [])

  /**
   * v3.1.0 HITL: Resume interrupted execution flow
   */
  const resumeExecution = useCallback(async (
    params: ResumeChatParams
  ): Promise<string> => {
    if (isGenerating) {
      debug('Request in progress, ignoring duplicate resume request')
      return ''
    }

    setGenerating(true)
    abortControllerRef.current = new AbortController()

    let fullContent = ''
    
    // v3.1.1: åˆå§‹åŒ–æµå¼è§£æå™¨çŠ¶æ€
    const streamingParserState: StreamingParserState = {
      isInThinking: false,
      thinkingBuffer: '',
      contentBuffer: ''
    }
    let isFirstChunk = true  // v3.1.1: æ ‡è®°æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ª chunk

    try {
      const streamCallback: StreamCallback = async (
        chunk: string | undefined,
        conversationId?: string,
        expertEvent?: ExpertEvent
      ) => {
        if (expertEvent) {
          onExpertEvent?.(expertEvent as any, conversationMode)
        }

        if (chunk) {
          // v3.1.1: ä½¿ç”¨çŠ¶æ€æœºè§£æå™¨åˆ†ç¦» thinking å’Œæ­£æ–‡å†…å®¹
          const { content, thinking } = processStreamingChunk(chunk, streamingParserState, isFirstChunk)
          
          // æ ‡è®°ç¬¬ä¸€ä¸ª chunk å·²å¤„ç†
          if (isFirstChunk) {
            isFirstChunk = false
          }
          
          // ç´¯ç§¯å®Œæ•´å“åº”
          fullContent += chunk
          
          // åªå°†æ­£æ–‡å†…å®¹ä¼ é€’ç»™ UI æ˜¾ç¤º
          if (content) {
            onChunk?.(content)
          }
          
          // ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç´¯ç§¯çš„ thinkingBuffer æ›´æ–°åˆ°æ¶ˆæ¯ metadata
          // resumeExecution ä¸ç›´æ¥æ›´æ–°æ¶ˆæ¯å†…å®¹ï¼Œç”±ä¸Šå±‚é€šè¿‡ onChunk å¤„ç†
        }
      }

      fullContent = await apiResumeChat(
        params,
        streamCallback,
        abortControllerRef.current.signal
      )

      return fullContent

    } catch (error) {
      const isAbortError = 
        (error instanceof Error && error.name === 'AbortError') ||
        (error instanceof Error && error.message?.toLowerCase().includes('abort')) ||
        abortControllerRef.current?.signal.aborted

      if (!isAbortError) {
        errorHandler.handle(error, 'resumeExecution')
        addMessage({
          role: 'assistant',
          content: errorHandler.getUserMessage(error)
        })
      }
      
      throw error
    } finally {
      setGenerating(false)
      abortControllerRef.current = null
    }
  }, [isGenerating, conversationMode, onExpertEvent, onChunk, setGenerating, addMessage])

  return {
    sendMessage: sendMessageCore,
    stopGeneration,
    resumeExecution,
    conversationMode,
  }
}
