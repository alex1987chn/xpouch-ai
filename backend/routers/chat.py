"""
èŠå¤©è·¯ç”±æ¨¡å— - åŒ…å«ä¸»è¦èŠå¤©ç«¯ç‚¹å’Œçº¿ç¨‹ç®¡ç†
v3.0: å¤æ‚æ¨¡å¼ä½¿ç”¨æ–°çš„äº‹ä»¶åè®®ï¼ˆplan.created, task.started, task.completed, artifact.generated, message.deltaï¼‰
"""
import os
import json
import re
import asyncio  # æ–°å¢ï¼šç”¨äºå¿ƒè·³ä¿æ´»
from datetime import datetime
from typing import List, Optional, AsyncGenerator, Dict, Any
from uuid import uuid4

from fastapi import APIRouter, Depends, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sqlmodel import Session, select
from sqlalchemy.orm import selectinload
from langchain_core.messages import HumanMessage, AIMessage

from database import get_session, engine
from dependencies import get_current_user, get_current_user_with_auth
from utils.thinking_parser import parse_thinking
from models import (
    User, Thread, Message, CustomAgent, TaskSession, SubTask
)
from crud.task_session import (
    create_task_session,
    get_task_session_by_thread,
    update_task_session_status,
    create_subtask,
    get_subtasks_by_session,
    update_subtask_status,
    create_artifacts_batch
)
from constants import (
    normalize_agent_id,
    SYSTEM_AGENT_ORCHESTRATOR,
    SYSTEM_AGENT_DEFAULT_CHAT
)
from utils.llm_factory import get_llm_instance
from agents.graph import commander_graph, create_smart_router_workflow  # ğŸ”¥ æ–°å¢ï¼šå¯¼å…¥ create_smart_router_workflow
from utils.exceptions import AppError, NotFoundError, AuthorizationError

# ğŸ”¥ HITL (Human-in-the-Loop) æ”¯æŒ
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from utils.db import get_db_connection  # ğŸ”¥ æ–°å¢ï¼šLangGraph æ•°æ®åº“è¿æ¥


router = APIRouter(prefix="/api", tags=["chat"])

DEBUG = os.getenv("DEBUG", "false").lower() == "true"


# ============================================================================
# è¯·æ±‚æ¨¡å‹
# ============================================================================

class ChatMessageDTO(BaseModel):
    role: str
    content: str
    id: Optional[str] = None
    timestamp: Optional[str] = None


class ChatRequest(BaseModel):
    message: str
    history: List[ChatMessageDTO]
    conversationId: Optional[str] = None
    agentId: Optional[str] = "assistant"
    stream: Optional[bool] = True
    message_id: Optional[str] = None  # v3.0: å‰ç«¯ä¼ é€’çš„åŠ©æ‰‹æ¶ˆæ¯ ID


# ============================================================================
# HITL (Human-in-the-Loop) è¯·æ±‚æ¨¡å‹
# ============================================================================

class ResumeRequest(BaseModel):
    """æ¢å¤è¢«ä¸­æ–­çš„ HITL æµç¨‹è¯·æ±‚"""
    thread_id: str
    updated_plan: Optional[List[Dict[str, Any]]] = None
    approved: bool = True


# ============================================================================
# çº¿ç¨‹ç®¡ç† API
# ============================================================================

@router.get("/threads", response_model=List[dict])
async def get_threads(
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """è·å–å½“å‰ç”¨æˆ·çš„æ‰€æœ‰çº¿ç¨‹åˆ—è¡¨"""
    statement = (
        select(Thread)
        .where(Thread.user_id == current_user.id)
        .options(selectinload(Thread.messages))
        .order_by(Thread.updated_at.desc())
    )
    threads = session.exec(statement).all()
    
    result = []
    for thread in threads:
        result.append({
            "id": thread.id,
            "title": thread.title,
            "agent_id": thread.agent_id,
            "agent_type": thread.agent_type,
            "thread_mode": thread.thread_mode,
            "user_id": thread.user_id,
            "task_session_id": thread.task_session_id,
            "created_at": thread.created_at.isoformat() if thread.created_at else None,
            "updated_at": thread.updated_at.isoformat() if thread.updated_at else None,
            "messages": [
                {
                    "id": msg.id,
                    "role": msg.role,
                    "content": msg.content,
                    "timestamp": msg.timestamp.isoformat() if msg.timestamp else None
                }
                for msg in thread.messages
            ]
        })
    return result


@router.get("/threads/{thread_id}")
async def get_thread(
    thread_id: str,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """è·å–å•ä¸ªçº¿ç¨‹è¯¦æƒ…"""
    statement = (
        select(Thread)
        .where(Thread.id == thread_id)
        .options(selectinload(Thread.messages))
    )
    thread = session.exec(statement).first()

    if not thread or thread.user_id != current_user.id:
        raise NotFoundError(resource="ä¼šè¯")

    # å¦‚æœæ˜¯AIåŠ©æ‰‹çº¿ç¨‹ï¼ˆå¤æ‚æ¨¡å¼ï¼‰ï¼ŒåŠ è½½TaskSessionå’ŒSubTask
    print(f"[GET_THREAD] thread_id={thread_id}, agent_type={thread.agent_type}, task_session_id={thread.task_session_id}, messages_count={len(thread.messages)}")
    # æ‰“å°æ‰€æœ‰æ¶ˆæ¯çš„è§’è‰²ï¼Œå¸®åŠ©è°ƒè¯•
    for msg in thread.messages:
        print(f"[GET_THREAD]   - msg_id={msg.id}, role={msg.role}, content_preview={msg.content[:30] if msg.content else 'N/A'}...")
    if thread.agent_type == "ai" and thread.task_session_id:
        task_session = session.get(TaskSession, thread.task_session_id)
        if task_session:
            # v3.0: ä½¿ç”¨ selectinload é¢„åŠ è½½ artifacts å…³ç³»ï¼Œé¿å… N+1 æŸ¥è¯¢
            statement = (
                select(SubTask)
                .where(SubTask.task_session_id == task_session.session_id)
                .options(selectinload(SubTask.artifacts))
                .order_by(SubTask.sort_order)
            )
            sub_tasks = session.exec(statement).all()

            return {
                "id": thread.id,
                "title": thread.title,
                "agent_id": thread.agent_id,
                "agent_type": thread.agent_type,
                "user_id": thread.user_id,
                "task_session_id": thread.task_session_id,
                "created_at": thread.created_at.isoformat() if thread.created_at else None,
                "updated_at": thread.updated_at.isoformat() if thread.updated_at else None,
                "messages": [
                    {
                        "id": msg.id,
                        "role": msg.role,
                        "content": msg.content,
                        "timestamp": msg.timestamp.isoformat() if msg.timestamp else None,
                        "extra_data": msg.extra_data
                    }
                    for msg in thread.messages
                ],
                "task_session": {
                    "id": task_session.session_id,  # âœ… å‰ç«¯å…¼å®¹æ€§ï¼šåŒæ—¶æä¾› id å’Œ session_id
                    "session_id": task_session.session_id,
                    "user_query": task_session.user_query,
                    "final_response": task_session.final_response,
                    "status": task_session.status,
                    "sub_tasks": [
                        {
                            "id": st.id,
                            "task_session_id": st.task_session_id,
                            "expert_type": st.expert_type,
                            "task_description": st.task_description,
                            "status": st.status,
                            "output_result": st.output_result,
                            "error_message": st.error_message,
                            "artifacts": [
                                {
                                    "id": art.id,
                                    "type": art.type,
                                    "title": art.title,
                                    "content": art.content,
                                    "language": art.language,
                                    "sort_order": art.sort_order,
                                    "created_at": art.created_at.isoformat() if art.created_at else None
                                }
                                for art in (st.artifacts or [])
                            ],
                            "duration_ms": st.duration_ms,
                            "created_at": st.created_at.isoformat() if st.created_at else None
                        }
                        for st in sub_tasks
                    ]
                }
            }

    return {
        "id": thread.id,
        "title": thread.title,
        "agent_id": thread.agent_id,
        "agent_type": thread.agent_type,
        "user_id": thread.user_id,
        "task_session_id": thread.task_session_id,
        "created_at": thread.created_at.isoformat() if thread.created_at else None,
        "updated_at": thread.updated_at.isoformat() if thread.updated_at else None,
        "messages": [
            {
                "id": msg.id,
                "role": msg.role,
                "content": msg.content,
                "timestamp": msg.timestamp.isoformat() if msg.timestamp else None,
                "extra_data": msg.extra_data
            }
            for msg in thread.messages
        ]
    }


@router.delete("/threads/{thread_id}")
async def delete_thread(
    thread_id: str,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """åˆ é™¤çº¿ç¨‹"""
    thread = session.get(Thread, thread_id)
    if not thread or thread.user_id != current_user.id:
        raise NotFoundError(resource="ä¼šè¯")
    session.delete(thread)
    session.commit()
    return {"ok": True}


# ============================================================================
# ä¸»è¦èŠå¤©ç«¯ç‚¹
# ============================================================================

@router.post("/chat")
async def chat_endpoint(
    request: ChatRequest,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """
    ç»Ÿä¸€èŠå¤©ç«¯ç‚¹ï¼ˆç®€å•æ¨¡å¼ + å¤æ‚æ¨¡å¼ï¼‰
    v3.0: å¤æ‚æ¨¡å¼ä½¿ç”¨æ–°çš„äº‹ä»¶åè®®
    """
    # 1. ç¡®å®š Thread ID
    thread_id = request.conversationId
    thread = None

    if thread_id:
        thread = session.get(Thread, thread_id)
        if thread and thread.user_id != current_user.id:
            raise AuthorizationError("æ²¡æœ‰æƒé™è®¿é—®æ­¤ä¼šè¯")

    if not thread:
        if not thread_id:
            thread_id = str(uuid4())

        # å…œåº•é€»è¾‘ï¼šå¦‚æœ agentId ä¸º Noneã€null æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œå¼ºåˆ¶èµ‹å€¼ä¸ºç³»ç»Ÿé»˜è®¤åŠ©æ‰‹
        if not request.agentId or request.agentId.strip() == "":
            frontend_agent_id = SYSTEM_AGENT_DEFAULT_CHAT
        else:
            frontend_agent_id = normalize_agent_id(request.agentId)

        # sys-task-orchestrator æ˜¯å†…éƒ¨å®ç°ï¼Œä¸åº”åœ¨ URL ä¸­æš´éœ²
        if frontend_agent_id == SYSTEM_AGENT_ORCHESTRATOR:
            frontend_agent_id = SYSTEM_AGENT_DEFAULT_CHAT

        # æ ¹æ® agentId ç¡®å®š agent_type
        custom_agent_check = session.get(CustomAgent, frontend_agent_id)
        if custom_agent_check and custom_agent_check.user_id == current_user.id:
            agent_type = "custom"
            final_agent_id = frontend_agent_id
        else:
            agent_type = "default"
            final_agent_id = SYSTEM_AGENT_DEFAULT_CHAT

        # åˆå§‹ thread_mode ä¸º simpleï¼ŒRouter ä¼šåœ¨å¤„ç†æ—¶æ›´æ–°å®ƒ
        thread = Thread(
            id=thread_id,
            title=request.message[:30] + "..." if len(request.message) > 30 else request.message,
            agent_id=final_agent_id,
            agent_type=agent_type,
            thread_mode="simple",
            user_id=current_user.id,
            created_at=datetime.now(),
            updated_at=datetime.now()
        )
        session.add(thread)
        session.commit()
        session.refresh(thread)

    # 2. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°æ•°æ®åº“
    user_msg_db = Message(
        thread_id=thread_id,
        role="user",
        content=request.message,
        timestamp=datetime.now()
    )
    session.add(user_msg_db)
    session.commit()
    print(f"[CHAT] âœ… ç”¨æˆ·æ¶ˆæ¯å·²ä¿å­˜åˆ°æ•°æ®åº“: thread_id={thread_id}, msg_id={user_msg_db.id}")

    # 3. å‡†å¤‡ LangGraph ä¸Šä¸‹æ–‡
    statement = select(Message).where(Message.thread_id == thread_id).order_by(Message.timestamp)
    db_messages = session.exec(statement).all()

    langchain_messages = []
    for msg in db_messages:
        if msg.role == "user":
            langchain_messages.append(HumanMessage(content=msg.content))
        elif msg.role == "assistant":
            langchain_messages.append(AIMessage(content=msg.content))

    # æ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå®šä¹‰æ™ºèƒ½ä½“
    custom_agent = None
    normalized_agent_id = normalize_agent_id(request.agentId)

    if normalized_agent_id == SYSTEM_AGENT_ORCHESTRATOR:
        normalized_agent_id = SYSTEM_AGENT_DEFAULT_CHAT

    is_system_default = False

    if normalized_agent_id == SYSTEM_AGENT_DEFAULT_CHAT:
        is_system_default = True
        custom_agent = None
        print(f"[CHAT] ç³»ç»Ÿé»˜è®¤åŠ©æ‰‹æ¨¡å¼ - å°†ä½¿ç”¨ LangGraph Router å†³å®šæ‰§è¡Œè·¯å¾„")
    else:
        custom_agent = session.get(CustomAgent, normalized_agent_id)

        if custom_agent and custom_agent.user_id == current_user.id:
            custom_agent.conversation_count += 1
            session.add(custom_agent)
            session.commit()
            print(f"[CHAT] è‡ªå®šä¹‰æ™ºèƒ½ä½“æ¨¡å¼ - ç›´æ¥è°ƒç”¨ LLM: {custom_agent.name}")
        else:
            is_system_default = True
            custom_agent = None
            print(f"[CHAT] æœªæ‰¾åˆ°è‡ªå®šä¹‰æ™ºèƒ½ä½“ï¼Œå›é€€åˆ°ç³»ç»Ÿé»˜è®¤åŠ©æ‰‹æ¨¡å¼")

    # å¦‚æœæ˜¯è‡ªå®šä¹‰æ™ºèƒ½ä½“ï¼Œä½¿ç”¨ç›´æ¥ LLM è°ƒç”¨æ¨¡å¼
    if custom_agent:
        if request.stream:
            print(f"[CHAT] {datetime.now().isoformat()} - âœ… ä½¿ç”¨è‡ªå®šä¹‰æ™ºèƒ½ä½“æµå¼æ¨¡å¼")
            return await _handle_custom_agent_stream(
                custom_agent, langchain_messages, thread_id, thread, request.message_id
            )
        else:
            print(f"[CHAT] {datetime.now().isoformat()} - âŒ ä½¿ç”¨è‡ªå®šä¹‰æ™ºèƒ½ä½“éæµå¼æ¨¡å¼ï¼ˆå‡æµå¼ï¼ï¼‰")
            return await _handle_custom_agent_sync(
                custom_agent, langchain_messages, thread_id, thread, session
            )

    # ç³»ç»Ÿé»˜è®¤åŠ©æ‰‹æ¨¡å¼ï¼šé€šè¿‡ LangGraph å¤„ç†
    print(f"[CHAT] {datetime.now().isoformat()} - è¿›å…¥ç³»ç»Ÿé»˜è®¤åŠ©æ‰‹æ¨¡å¼ï¼Œä½¿ç”¨ LangGraph å¤„ç†")

    initial_state = {
        "messages": langchain_messages,
        "current_agent": "router",
        "task_list": [],
        "current_task_index": 0,
        "strategy": "",
        "expert_results": [],
        "final_response": "",
        "context": {},
        "router_decision": "",
        "thread_id": thread_id,
        "user_id": thread.user_id  # ğŸ”¥ ä¼ å…¥ user_id ç”¨äºè®°å¿†åŠŸèƒ½
    }

    if request.stream:
        print(f"[CHAT] {datetime.now().isoformat()} - âœ… ä½¿ç”¨ LangGraph æµå¼æ¨¡å¼ï¼ˆå¤æ‚æ¨¡å¼ï¼‰")
        return await _handle_langgraph_stream(
            initial_state, thread_id, thread, request.message, session, request.message_id
        )
    else:
        print(f"[CHAT] {datetime.now().isoformat()} - âŒ ä½¿ç”¨ LangGraph éæµå¼æ¨¡å¼ï¼ˆå‡æµå¼ï¼ï¼‰")
        return await _handle_langgraph_sync(
            initial_state, thread_id, thread, request.message, session
        )


# ============================================================================
# è‡ªå®šä¹‰æ™ºèƒ½ä½“å¤„ç†å‡½æ•°
# ============================================================================

async def _handle_custom_agent_stream(
    custom_agent: CustomAgent,
    langchain_messages: list,
    thread_id: str,
    thread: Thread,
    message_id: Optional[str] = None  # v3.0: å‰ç«¯ä¼ é€’çš„åŠ©æ‰‹æ¶ˆæ¯ ID
) -> StreamingResponse:
    """å¤„ç†è‡ªå®šä¹‰æ™ºèƒ½ä½“æµå¼å“åº” (v3.0 æ–°åè®®)
    
    æ–°å¢ï¼šæ·»åŠ å¿ƒè·³ä¿æ´»æœºåˆ¶é˜²æ­¢ Cloudflare/CDN è¶…æ—¶æ–­å¼€è¿æ¥
    """
    async def event_generator():
        full_response = ""
        # v3.0: ç¡®ä¿ä½¿ç”¨ä¸€è‡´çš„ message_id
        actual_message_id = message_id or str(uuid4())

        # ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢ï¼šå¿ƒè·³é—´éš”ï¼ˆ15ç§’ï¼‰è¿œå°äº Cloudflare çš„ 100ç§’è¶…æ—¶ ğŸ”¥ğŸ”¥ğŸ”¥
        HEARTBEAT_INTERVAL = 15.0

        print(f"[CUSTOM AGENT STREAM] {datetime.now().isoformat()} - å¼€å§‹æµå¼å¤„ç†ï¼Œå¿ƒè·³é—´éš”={HEARTBEAT_INTERVAL}ç§’ï¼Œå¼ºåˆ¶å¿ƒè·³é—´éš”=30.0ç§’")

        # ğŸ”¥ å¼ºåˆ¶å¿ƒè·³è®¡æ—¶å™¨ï¼ˆæ¯ 30 ç§’å¼ºåˆ¶å‘é€ä¸€æ¬¡å¿ƒè·³ï¼Œä¸ç®¡æœ‰æ²¡æœ‰äº‹ä»¶ï¼‰
        FORCE_HEARTBEAT_INTERVAL = 30.0
        last_heartbeat_time = datetime.now()

        try:
            # ä½¿ç”¨æ–°çš„é…ç½®ç³»ç»Ÿè·å–æ¨¡å‹
            from providers_config import get_model_config, get_provider_config, get_provider_api_key
            
            model_id = custom_agent.model_id or "deepseek-chat"
            model_config = get_model_config(model_id)
            
            if model_config:
                # ä»é…ç½®æ–‡ä»¶è·å–æä¾›å•†å’Œå®é™…æ¨¡å‹å
                provider = model_config.get('provider')
                actual_model = model_config.get('model', model_id)
                provider_config = get_provider_config(provider)
                
                if not provider_config:
                    raise ValueError(f"æä¾›å•† {provider} æœªé…ç½®")
                
                if not get_provider_api_key(provider):
                    raise ValueError(f"æä¾›å•† {provider} çš„ API Key æœªè®¾ç½®ï¼Œè¯·åœ¨ .env ä¸­é…ç½® {provider_config.get('env_key')}")

                # ä»æ¨¡å‹é…ç½®è¯»å– temperatureï¼ˆå…è®¸æ¨¡å‹çº§åˆ«è¦†ç›–ï¼‰
                model_config = get_model_config(model_id)
                temperature = model_config.get('temperature', 0.7) if model_config else 0.7

                print(f"[CUSTOM AGENT] ä½¿ç”¨æ¨¡å‹: {model_id} ({actual_model} via {provider}), temperature={temperature}ï¼Œæ¶ˆæ¯ID: {actual_message_id}")

                # ä½¿ç”¨æ–°çš„ llm_factoryï¼ˆä¼šè‡ªåŠ¨ä»é…ç½®æ–‡ä»¶è¯»å– base_urlï¼‰
                llm = get_llm_instance(
                    provider=provider,
                    model=actual_model,
                    streaming=True,
                    temperature=temperature
                )
            else:
                # Fallback: æ—§ç‰ˆå…¼å®¹ï¼ˆç›´æ¥ä¼ é€’æ¨¡å‹åï¼‰
                # å°è¯•ä»æ¨¡å‹é…ç½®è¯»å– temperature
                model_config = get_model_config(model_id)
                temperature = model_config.get('temperature', 0.7) if model_config else 0.7

                print(f"[CUSTOM AGENT] æœªæ‰¾åˆ°æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨ fallback: {model_id}, temperature={temperature}")
                llm = get_llm_instance(streaming=True, model=model_id, temperature=temperature)

            # ğŸ”¥ æ£€ç´¢é•¿æœŸè®°å¿†
            from services.memory_manager import memory_manager
            user_query = langchain_messages[-1].content if langchain_messages else ""
            relevant_memories = await memory_manager.search_relevant_memories(thread.user_id, user_query, limit=5)
            
            # æ„å»º System Promptï¼ˆæ³¨å…¥è®°å¿†ï¼‰
            system_prompt = custom_agent.system_prompt
            if relevant_memories:
                print(f"[CUSTOM AGENT] æ¿€æ´»è®°å¿†: {relevant_memories[:100]}...")
                system_prompt += f"\n\nã€å…³äºç”¨æˆ·çš„å·²çŸ¥ä¿¡æ¯ã€‘:\n{relevant_memories}\n(è¯·åœ¨å›ç­”æ—¶è‡ªç„¶åœ°åˆ©ç”¨è¿™äº›ä¿¡æ¯)"
            
            messages_with_system = [("system", system_prompt)]
            messages_with_system.extend(langchain_messages)

            # è·å–æµè¿­ä»£å™¨
            iterator = llm.astream(messages_with_system)

            # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨åœ°è·å–ä¸‹ä¸€ä¸ª chunk
            async def get_next_chunk():
                try:
                    return await asyncio.wait_for(
                        iterator.__anext__(),
                        timeout=HEARTBEAT_INTERVAL
                    )
                except StopAsyncIteration:
                    return None

            while True:
                try:
                    # ç­‰å¾…ä¸‹ä¸€ä¸ª chunkï¼Œè¶…è¿‡ 15 ç§’åˆ™å‘é€å¿ƒè·³
                    chunk = await get_next_chunk()

                    if chunk is None:  # æµç»“æŸ
                        break

                    content = chunk.content
                    if content:
                        full_response += content
                        # v3.0: ä½¿ç”¨ message.delta äº‹ä»¶ï¼ˆæ–°åè®®ï¼‰
                        from event_types.events import EventType, MessageDeltaData, build_sse_event
                        delta_event = build_sse_event(
                            EventType.MESSAGE_DELTA,
                            MessageDeltaData(
                                message_id=actual_message_id,
                                content=content
                            ),
                            str(uuid4())
                        )
                        from utils.event_generator import sse_event_to_string
                        yield sse_event_to_string(delta_event)

                except asyncio.TimeoutError:
                    # ğŸ”¥ğŸ”¥ğŸ”¥ å¿ƒè·³ä¿æ´»ï¼šLLM æ­£åœ¨æ€è€ƒï¼Œä½†è¶…è¿‡ 15 ç§’æœªäº§ç”Ÿæ•°æ® ğŸ”¥ğŸ”¥ğŸ”¥
                    # å‘é€ SSE æ³¨é‡Šï¼ˆå†’å·å¼€å¤´ï¼‰ï¼Œæµè§ˆå™¨ä¼šå¿½ç•¥ï¼Œä½† Cloudflare è®¤ä¸ºæœ‰æ•°æ®ä¼ è¾“
                    print(f"[HEARTBEAT-CUSTOM-TIMEOUT] {datetime.now().isoformat()} - å‘é€å¿ƒè·³ä¿æ´»ï¼ˆå·²ç­‰å¾… {HEARTBEAT_INTERVAL} ç§’æ— æ•°æ®ï¼‰")
                    yield ": keep-alive\n\n"
                    last_heartbeat_time = datetime.now()
                    continue

                # ğŸ”¥ å¼ºåˆ¶å¿ƒè·³ï¼šå³ä½¿æœ‰äº‹ä»¶ï¼Œæ¯ 30 ç§’ä¹Ÿå¼ºåˆ¶å‘é€ä¸€æ¬¡å¿ƒè·³
                current_time = datetime.now()
                time_since_last_heartbeat = (current_time - last_heartbeat_time).total_seconds()
                if time_since_last_heartbeat >= FORCE_HEARTBEAT_INTERVAL:
                    print(f"[HEARTBEAT-CUSTOM-FORCE] {datetime.now().isoformat()} - å¼ºåˆ¶å‘é€å¿ƒè·³ä¿æ´»ï¼ˆè·ç¦»ä¸Šæ¬¡å¿ƒè·³ {time_since_last_heartbeat:.1f} ç§’ï¼‰")
                    yield ": keep-alive\n\n"
                    last_heartbeat_time = current_time

        except Exception as e:
            import traceback
            traceback.print_exc()
            # v3.0: å‘é€ error äº‹ä»¶
            from event_types.events import EventType, ErrorData, build_sse_event
            from utils.event_generator import sse_event_to_string
            error_event = build_sse_event(
                EventType.ERROR,
                ErrorData(code="STREAM_ERROR", message=str(e)),
                str(uuid4())
            )
            yield sse_event_to_string(error_event)

        # è§£æ thinking æ ‡ç­¾ï¼ˆç±»ä¼¼ DeepSeek Chat çš„æ€è€ƒè¿‡ç¨‹ï¼‰
        clean_content, thinking_data = parse_thinking(full_response)

        # v3.0: å‘é€ message.done äº‹ä»¶ï¼ˆæ–°åè®®ï¼‰
        # ä½¿ç”¨ä¸ delta äº‹ä»¶ç›¸åŒçš„ actual_message_id
        from event_types.events import EventType, MessageDoneData, build_sse_event
        done_event = build_sse_event(
            EventType.MESSAGE_DONE,
            MessageDoneData(
                message_id=actual_message_id,
                full_content=clean_content,  # ä½¿ç”¨æ¸…ç†åçš„å†…å®¹
                thinking=thinking_data  # åŒ…å« thinking æ•°æ®
            ),
            str(uuid4())
        )
        from utils.event_generator import sse_event_to_string
        yield sse_event_to_string(done_event)

        yield "data: [DONE]\n\n"
        print(f"[CUSTOM AGENT] æµå¼å“åº”å®Œæˆï¼Œæ¶ˆæ¯ID: {actual_message_id}")

        # ä¿å­˜ AI å›å¤åˆ°æ•°æ®åº“
        if full_response:
            ai_msg_db = Message(
                thread_id=thread_id,
                role="assistant",
                content=clean_content,  # ä¿å­˜æ¸…ç†åçš„å†…å®¹ï¼ˆç§»é™¤ thought æ ‡ç­¾ï¼‰
                extra_data={'thinking': thinking_data} if thinking_data else None,
                timestamp=datetime.now()
            )
            with Session(engine) as inner_session:
                inner_session.add(ai_msg_db)
                thread_obj = inner_session.get(Thread, thread_id)
                if thread_obj:
                    thread_obj.updated_at = datetime.now()
                    inner_session.add(thread_obj)
                inner_session.commit()
        
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


async def _handle_custom_agent_sync(
    custom_agent: CustomAgent,
    langchain_messages: list,
    thread_id: str,
    thread: Thread,
    session: Session
) -> dict:
    """å¤„ç†è‡ªå®šä¹‰æ™ºèƒ½ä½“éæµå¼å“åº”"""
    # ä½¿ç”¨æ–°çš„é…ç½®ç³»ç»Ÿè·å–æ¨¡å‹
    from providers_config import get_model_config, get_provider_config, get_provider_api_key
    
    model_id = custom_agent.model_id or "deepseek-chat"
    model_config = get_model_config(model_id)
    
    if model_config:
        provider = model_config.get('provider')
        actual_model = model_config.get('model', model_id)
        provider_config = get_provider_config(provider)
        
        if not provider_config:
            raise ValueError(f"æä¾›å•† {provider} æœªé…ç½®")
        
        if not get_provider_api_key(provider):
            raise ValueError(f"æä¾›å•† {provider} çš„ API Key æœªè®¾ç½®")
        
        print(f"[CUSTOM AGENT] ä½¿ç”¨æ¨¡å‹: {model_id} ({actual_model} via {provider})")
        
        llm = get_llm_instance(
            provider=provider,
            model=actual_model,
            streaming=False,
            temperature=0.7
        )
    else:
        print(f"[CUSTOM AGENT] æœªæ‰¾åˆ°æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨ fallback: {model_id}")
        llm = get_llm_instance(streaming=False, model=model_id, temperature=0.7)
    
    # ğŸ”¥ æ£€ç´¢é•¿æœŸè®°å¿†
    from services.memory_manager import memory_manager
    user_query = langchain_messages[-1].content if langchain_messages else ""
    relevant_memories = await memory_manager.search_relevant_memories(thread.user_id, user_query, limit=5)
    
    # æ„å»º System Promptï¼ˆæ³¨å…¥è®°å¿†ï¼‰
    system_prompt = custom_agent.system_prompt
    if relevant_memories:
        print(f"[CUSTOM AGENT] æ¿€æ´»è®°å¿†: {relevant_memories[:100]}...")
        system_prompt += f"\n\nã€å…³äºç”¨æˆ·çš„å·²çŸ¥ä¿¡æ¯ã€‘:\n{relevant_memories}\n(è¯·åœ¨å›ç­”æ—¶è‡ªç„¶åœ°åˆ©ç”¨è¿™äº›ä¿¡æ¯)"
    
    messages_with_system = [("system", system_prompt)]
    messages_with_system.extend(langchain_messages)
    
    result = await llm.ainvoke(messages_with_system)
    full_response = result.content

    # ä¿å­˜ AI å›å¤
    # è§£æ thinking æ ‡ç­¾
    clean_content, thinking_data = parse_thinking(full_response)
    ai_msg_db = Message(
        thread_id=thread_id,
        role="assistant",
        content=clean_content,  # ä¿å­˜æ¸…ç†åçš„å†…å®¹
        extra_data={'thinking': thinking_data} if thinking_data else None,
        timestamp=datetime.now()
    )
    session.add(ai_msg_db)
    thread.updated_at = datetime.now()
    session.add(thread)
    session.commit()

    return {
        "role": "assistant",
        "content": full_response,
        "conversationId": thread_id
    }


# ============================================================================
# LangGraph å¤„ç†å‡½æ•° - v3.0 æ–°åè®®
# ============================================================================

async def _handle_langgraph_stream(
    initial_state: dict,
    thread_id: str,
    thread: Thread,
    user_message: str,
    session: Session,
    message_id: Optional[str] = None  # v3.0: å‰ç«¯ä¼ é€’çš„åŠ©æ‰‹æ¶ˆæ¯ ID
) -> StreamingResponse:
    """
    å¤„ç† LangGraph æµå¼å“åº” (v3.0)
    åªå‘é€æ–°åè®®äº‹ä»¶ï¼šplan.created, task.started, task.completed, artifact.generated, message.delta, message.done
    
    æ–°å¢ï¼šæ·»åŠ å¿ƒè·³ä¿æ´»æœºåˆ¶é˜²æ­¢ Cloudflare/CDN è¶…æ—¶æ–­å¼€è¿æ¥
    v3.5 æ›´æ–°ï¼šä½¿ç”¨ AsyncPostgresSaver å®ç° HITL (Human-in-the-Loop) æŒä¹…åŒ–
    """
    async def event_generator():
        full_response = ""
        event_count = 0
        router_mode = ""
        task_session_id = None  # v3.0: è·Ÿè¸ª TaskSession ID

        # v3.0: æ”¶é›†ä»»åŠ¡åˆ—è¡¨å’Œäº§ç‰©ï¼ˆç”¨äºæœ€ç»ˆä¿å­˜ï¼‰
        collected_task_list = []
        expert_artifacts = {}

        # v3.0: åœ¨ initial_state ä¸­æ³¨å…¥ event_queue å’Œ message_id
        # ğŸ”¥ æ³¨æ„ï¼šä¸è¦æ”¾å…¥ db_sessionï¼Œå› ä¸º MemorySaver æ— æ³•åºåˆ—åŒ– SQLAlchemy Session
        # thread_id å’Œ user_id å·²åœ¨åˆ›å»º initial_state æ—¶æ³¨å…¥
        initial_state["event_queue"] = []
        initial_state["message_id"] = message_id  # v3.0: æ³¨å…¥å‰ç«¯ä¼ é€’çš„åŠ©æ‰‹æ¶ˆæ¯ ID

        # ğŸ”¥ğŸ”¥ğŸ”¥ v3.4: Shared Queue æ¨¡å¼ - åˆ›å»ºå…±äº«é˜Ÿåˆ—ç”¨äº Commander å®æ—¶æµå¼è¾“å‡º
        stream_queue = asyncio.Queue()
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢ï¼šå¿ƒè·³é—´éš”ï¼ˆ15ç§’ï¼‰è¿œå°äº Cloudflare çš„ 100ç§’è¶…æ—¶ ğŸ”¥ğŸ”¥ğŸ”¥
        HEARTBEAT_INTERVAL = 15.0

        print(f"[LANGGRAPH STREAM] {datetime.now().isoformat()} - å¼€å§‹æµå¼å¤„ç†ï¼Œå¿ƒè·³é—´éš”={HEARTBEAT_INTERVAL}ç§’ï¼Œå¼ºåˆ¶å¿ƒè·³é—´éš”=30.0ç§’")
        print(f"[LANGGRAPH STREAM] v3.5 HITL æ¨¡å¼å·²å¯ç”¨ (AsyncPostgresSaver)")

        # ğŸ”¥ å¼ºåˆ¶å¿ƒè·³è®¡æ—¶å™¨ï¼ˆæ¯ 30 ç§’å¼ºåˆ¶å‘é€ä¸€æ¬¡å¿ƒè·³ï¼Œä¸ç®¡æœ‰æ²¡æœ‰äº‹ä»¶ï¼‰
        FORCE_HEARTBEAT_INTERVAL = 30.0
        last_heartbeat_time = datetime.now()

        # ğŸ”¥ğŸ”¥ğŸ”¥ v3.5: HITL (Human-in-the-Loop) æ”¯æŒ
        # ä½¿ç”¨ AsyncPostgresSaver å®ç°çŠ¶æ€æŒä¹…åŒ–
        
        # 1. å®šä¹‰ç”Ÿäº§è€…ä»»åŠ¡ (Producer) - åœ¨åå°è¿è¡Œ Graph
        async def producer():
            """ç”Ÿäº§è€…ï¼šè¿è¡Œ LangGraphï¼Œå°†äº‹ä»¶æ”¾å…¥é˜Ÿåˆ—"""
            graph = None
            config = None
            try:
                # ğŸ”¥ğŸ”¥ğŸ”¥ v3.5: åˆ›å»º AsyncPostgresSaver å®ç°æŒä¹…åŒ–
                async with get_db_connection() as conn:
                    checkpointer = AsyncPostgresSaver(conn)
                    
                    # ğŸ”¥ ä½¿ç”¨æŒä¹…åŒ–çš„ checkpointer åˆ›å»º graph
                    graph = create_smart_router_workflow(checkpointer=checkpointer)
                    print(f"[PRODUCER] Graph compiled with AsyncPostgresSaver for HITL")
                    
                    config = {
                        "recursion_limit": 100,
                        "configurable": {
                            "thread_id": thread_id,
                            "stream_queue": stream_queue  # ğŸ”¥ æ³¨å…¥å…±äº«é˜Ÿåˆ—
                        }
                    }
                    
                    # è·å–å›¾çš„æµè¿­ä»£å™¨ï¼Œæ³¨å…¥ stream_queue
                    iterator = graph.astream_events(
                        initial_state,
                        config=config,
                        version="v2"
                    )
                    
                    # æ¶ˆè´¹ Graph äº‹ä»¶
                    async for event in iterator:
                        # å°†äº‹ä»¶æ”¾å…¥é˜Ÿåˆ—ï¼Œè®©ä¸»å¾ªç¯å¤„ç†
                        await stream_queue.put({"type": "graph_event", "event": event})
                    
                    # ğŸ”¥ğŸ”¥ğŸ”¥ v3.5 HITL: æ£€æŸ¥æ˜¯å¦å› ä¸­æ–­è€Œåœæ­¢
                    # ä½¿ç”¨ç›¸åŒçš„ config è·å– state snapshot
                    snapshot = await graph.aget_state(config)
                    if snapshot.next:  # å¦‚æœ next ä¸ä¸ºç©ºï¼Œè¯´æ˜ä»»åŠ¡æœªå®Œæˆä½†åœæ­¢äº† -> å¤„äº Pause çŠ¶æ€
                        current_plan = snapshot.values.get("task_list", [])
                        print(f"[PRODUCER] ğŸ”´ HITL ä¸­æ–­è§¦å‘ï¼è®¡åˆ’ä»»åŠ¡æ•°: {len(current_plan)}")
                        await stream_queue.put({
                            "type": "hitl_interrupt",
                            "data": {
                                "type": "plan_review",
                                "current_plan": current_plan
                            }
                        })
                    else:
                        print(f"[PRODUCER] âœ… Graph æ­£å¸¸å®Œæˆï¼Œæ— ä¸­æ–­")
                    
            except Exception as e:
                print(f"[PRODUCER] é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                await stream_queue.put({"type": "graph_error", "error": str(e)})
            finally:
                # ğŸ”¥ å“¨å…µä¿¡å·ï¼šé€šçŸ¥æ¶ˆè´¹è€…ç»“æŸ
                await stream_queue.put(None)
        
        # 2. å¯åŠ¨åå°ç”Ÿäº§è€…ä»»åŠ¡
        producer_task = asyncio.create_task(producer())
        
        # 3. æ¶ˆè´¹è€…å¾ªç¯ (Consumer) - ä¸»çº¿ç¨‹æ¶ˆè´¹é˜Ÿåˆ—å¹¶ yield SSE
        try:
            while True:
                # ç­‰å¾…é˜Ÿåˆ—æ¶ˆæ¯ï¼ˆå¸¦è¶…æ—¶é˜²æ­¢æ­»é”ï¼‰
                try:
                    token = await asyncio.wait_for(stream_queue.get(), timeout=1.0)
                except asyncio.TimeoutError:
                    # è¶…æ—¶æ£€æŸ¥ç”Ÿäº§è€…æ˜¯å¦å·²ç»“æŸ
                    if producer_task.done():
                        # å†æ¬¡å°è¯•è¯»ç©ºé˜Ÿåˆ—ï¼Œé˜²æ­¢ä¸¢å¤±æœ€åçš„æ¶ˆæ¯
                        while not stream_queue.empty():
                            item = stream_queue.get_nowait()
                            if item is None:
                                break
                            yield item
                        break
                    # ç”Ÿäº§è€…è¿˜åœ¨è¿è¡Œï¼Œå‘é€å¿ƒè·³ä¿æ´»
                    current_time = datetime.now()
                    time_since_last_heartbeat = (current_time - last_heartbeat_time).total_seconds()
                    if time_since_last_heartbeat >= FORCE_HEARTBEAT_INTERVAL:
                        yield ": keep-alive\n\n"
                        last_heartbeat_time = current_time
                    continue
                
                # ğŸ”¥ æ”¶åˆ°å“¨å…µä¿¡å·ï¼Œç»“æŸæ¶ˆè´¹
                if token is None:
                    print(f"[CONSUMER] æ”¶åˆ°å“¨å…µä¿¡å·ï¼Œç»“æŸæ¶ˆè´¹")
                    break
                
                # ğŸ”¥ å¼ºåˆ¶å¿ƒè·³æ£€æŸ¥ï¼ˆæ¯ 30 ç§’ï¼‰
                current_time = datetime.now()
                time_since_last_heartbeat = (current_time - last_heartbeat_time).total_seconds()
                if time_since_last_heartbeat >= FORCE_HEARTBEAT_INTERVAL:
                    yield ": keep-alive\n\n"
                    last_heartbeat_time = current_time
                
                # å¤„ç†é˜Ÿåˆ—ä¸­çš„äº‹ä»¶
                if token.get("type") == "graph_error":
                    # Graph æ‰§è¡Œå‡ºé”™
                    error_msg = token.get("error", "æœªçŸ¥é”™è¯¯")
                    print(f"[CONSUMER] Graph æ‰§è¡Œé”™è¯¯: {error_msg}")
                    break
                
                elif token.get("type") == "graph_event":
                    # å¤„ç† Graph æ ‡å‡†äº‹ä»¶ï¼ˆplan.created, task.started ç­‰ï¼‰
                    event = token["event"]
                    event_count += 1
                    kind = event["event"]
                    name = event.get("name", "")
                    
                    if event_count % 100 == 0:
                        print(f"[CONSUMER] å·²å¤„ç† {event_count} ä¸ªäº‹ä»¶")
                
                elif token.get("type") == "sse":
                    # ğŸ”¥ğŸ”¥ğŸ”¥ v3.4: Commander ç›´æ¥é€šè¿‡ queue å‘é€çš„ SSE äº‹ä»¶
                    # è¿™æ˜¯å®æ—¶æµå¼æ€è€ƒå†…å®¹ (plan.thinking)
                    print(f"[CONSUMER] ğŸ“¤ yield SSE äº‹ä»¶: {token['event'][:100]}...")
                    yield token["event"]
                    continue
                
                elif token.get("type") == "hitl_interrupt":
                    # ğŸ”¥ğŸ”¥ğŸ”¥ v3.5 HITL: äººç±»å®¡æ ¸ä¸­æ–­äº‹ä»¶
                    interrupt_data = token.get("data", {})
                    print(f"[CONSUMER] ğŸ”´ HITL ä¸­æ–­äº‹ä»¶: {interrupt_data.get('type')}")
                    
                    # æ„é€  human.interrupt SSE äº‹ä»¶ï¼ˆç›´æ¥å‘é€æ•°æ®ï¼Œä¸åµŒå¥—ï¼‰
                    event_str = f"event: human.interrupt\ndata: {json.dumps(interrupt_data)}\n\n"
                    yield event_str
                    continue
                
                # å¤„ç† Graph æ ‡å‡†äº‹ä»¶ï¼ˆä» graph_event ç±»å‹ä¸­æå–ï¼‰
                if token.get("type") == "graph_event":
                    event = token["event"]
                    kind = event["event"]
                    name = event.get("name", "")
                    
                    if event_count % 100 == 0:
                        print(f"[CONSUMER] å·²å¤„ç† {event_count} ä¸ªäº‹ä»¶")

                    # v3.0: å¤„ç†èŠ‚ç‚¹è¿”å›çš„ event_queueï¼ˆæ–°åè®®äº‹ä»¶ï¼‰
                    if kind == "on_chain_end":
                        raw_output = event["data"].get("output", {})
                        # ç¡®ä¿ output_data æ˜¯å­—å…¸ç±»å‹ï¼ˆLangGraph æœ‰æ—¶ä¼šè¿”å›å­—ç¬¦ä¸²ï¼‰
                        output_data = raw_output if isinstance(raw_output, dict) else {}
                        
                        if isinstance(output_data, dict):
                            event_queue = output_data.get("event_queue", [])

                            # æ•è· commander èŠ‚ç‚¹è¿”å›çš„ task_session_id
                            if name == "commander":
                                session_id = output_data.get("task_session_id")
                                if session_id:
                                    task_session_id = session_id
                                    print(f"[CONSUMER] æ•è·åˆ° TaskSession ID: {task_session_id}")
                                    # ç«‹å³æ›´æ–° thread çš„ task_session_id
                                    thread_obj = session.get(Thread, thread_id)
                                    if thread_obj and thread_obj.task_session_id != task_session_id:
                                        thread_obj.task_session_id = task_session_id
                                        session.add(thread_obj)
                                        session.commit()
                                        print(f"[CONSUMER] âœ… å·²è®¾ç½® thread.task_session_id = {task_session_id}")

                            # æ”¶é›†ä»»åŠ¡åˆ—è¡¨
                            if output_data.get("task_list"):
                                collected_task_list = output_data["task_list"]
                                
                            # æ”¶é›†äº§ç‰©
                            if output_data.get("__expert_info"):
                                expert_info = output_data["__expert_info"]
                                task_id = expert_info.get("task_id")
                                artifact_data = output_data.get("artifact")
                                if task_id and artifact_data:
                                    if task_id not in expert_artifacts:
                                        expert_artifacts[task_id] = []
                                    expert_artifacts[task_id].append(artifact_data)
                        else:
                            event_queue = []
                        
                        # å‘é€ event_queue ä¸­çš„æ‰€æœ‰äº‹ä»¶
                        for queued_event in event_queue:
                            if queued_event.get("type") == "sse":
                                yield queued_event["event"]

                    # v3.0: æ•è· Router èŠ‚ç‚¹æ‰§è¡Œç»“æŸ
                    if kind == "on_chain_end" and name == "router":
                        output_data = event["data"]["output"]
                        router_decision = output_data.get("router_decision", "")

                        if router_decision:
                            print(f"[CONSUMER] Router å†³ç­–: {router_decision}")
                            router_mode = router_decision
                            
                            if router_decision == "complex":
                                thread_obj = session.get(Thread, thread_id)
                                if thread_obj:
                                    if thread_obj.agent_type != "ai":
                                        thread_obj.agent_type = "ai"
                                    thread_obj.thread_mode = "complex"
                                    session.add(thread_obj)
                                    session.commit()
                                    print(f"[CONSUMER] å·²æ›´æ–° thread ä¸º complex æ¨¡å¼")
                                
                                # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ï¼šé¢„ç”Ÿæˆ session_id å¹¶ç«‹å³å‘é€ plan.started
                                preview_session_id = str(uuid4())
                                from utils.event_generator import event_plan_started, sse_event_to_string
                                plan_started_event = event_plan_started(
                                    session_id=preview_session_id,
                                    title="ä»»åŠ¡è§„åˆ’",
                                    content="æ­£åœ¨åˆ†æéœ€æ±‚...",
                                    status="running"
                                )
                                yield sse_event_to_string(plan_started_event)
                                print(f"[CONSUMER] ğŸš€ ç«‹å³å‘é€ plan.started: {preview_session_id}")
                                
                                # å°† preview_session_id å­˜å…¥ initial_state
                                initial_state["preview_session_id"] = preview_session_id
                            
                            # å‘é€ router.decision äº‹ä»¶
                            from event_types.events import EventType, RouterDecisionData, build_sse_event
                            router_event = build_sse_event(
                                EventType.ROUTER_DECISION,
                                RouterDecisionData(decision=router_decision),
                                str(uuid4())
                            )
                            from utils.event_generator import sse_event_to_string
                            yield sse_event_to_string(router_event)

                    # æ•è· direct_reply èŠ‚ç‚¹æ‰§è¡Œç»“æŸï¼ˆSimple æ¨¡å¼ï¼‰
                    if kind == "on_chain_end" and name == "direct_reply":
                        from event_types.events import EventType, MessageDoneData, build_sse_event
                        done_event = build_sse_event(
                            EventType.MESSAGE_DONE,
                            MessageDoneData(message_id=message_id or str(uuid4()), full_content=full_response),
                            str(uuid4())
                        )
                        from utils.event_generator import sse_event_to_string
                        yield sse_event_to_string(done_event)
                        print(f"[CONSUMER] Direct Reply èŠ‚ç‚¹å®Œæˆ")

                    # æ•è· LLM æµå¼è¾“å‡ºï¼ˆSimple æ¨¡å¼ï¼‰
                    if kind == "on_chat_model_stream" and router_mode == "simple":
                        content = event["data"]["chunk"].content
                        if content:
                            full_response += content
                            from event_types.events import EventType, MessageDeltaData, build_sse_event
                            delta_event = build_sse_event(
                                EventType.MESSAGE_DELTA,
                                MessageDeltaData(message_id=message_id or str(uuid4()), content=content),
                                str(uuid4())
                            )
                            from utils.event_generator import sse_event_to_string
                            yield sse_event_to_string(delta_event)

            print(f"[CONSUMER] æµå¼å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {event_count} ä¸ªäº‹ä»¶")
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ v3.4: ç¡®ä¿ç”Ÿäº§è€…ä»»åŠ¡å®Œæˆ
            if producer_task and not producer_task.done():
                try:
                    await asyncio.wait_for(producer_task, timeout=5.0)
                except asyncio.TimeoutError:
                    print(f"[CONSUMER] ç”Ÿäº§è€…ä»»åŠ¡ç­‰å¾…è¶…æ—¶ï¼Œå¼ºåˆ¶å–æ¶ˆ")
                    producer_task.cancel()

        except Exception as e:
            print(f"[STREAM] é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            # v3.0: å‘é€ error äº‹ä»¶
            from event_types.events import EventType, ErrorData, build_sse_event
            from utils.event_generator import sse_event_to_string
            error_event = build_sse_event(
                EventType.ERROR,
                ErrorData(code="STREAM_ERROR", message=str(e)),
                str(uuid4())
            )
            yield sse_event_to_string(error_event)

        # ä¿å­˜ AI å›å¤å’Œ Artifacts åˆ°æ•°æ®åº“
        if full_response:
            # âœ… å…³é”®ä¿®å¤ï¼šä½¿ç”¨ Router ä¼ å…¥çš„ sessionï¼ˆå³ db_sessionï¼‰ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°çš„ session
            # è¿™æ ·æ‰èƒ½çœ‹åˆ° Commander åœ¨ db_session ä¸­ commit çš„ SubTasks
            save_session = session

            # è§£æ thinking æ ‡ç­¾
            clean_content, thinking_data = parse_thinking(full_response)
            ai_msg_db = Message(
                thread_id=thread_id,
                role="assistant",
                content=clean_content,  # ä¿å­˜æ¸…ç†åçš„å†…å®¹
                extra_data={'thinking': thinking_data} if thinking_data else None,
                timestamp=datetime.now()
            )
            save_session.add(ai_msg_db)

            thread_obj = save_session.get(Thread, thread_id)
            if thread_obj:
                thread_obj.updated_at = datetime.now()

                if router_mode:
                    thread_obj.thread_mode = router_mode

                # å¤æ‚æ¨¡å¼ï¼šæ›´æ–° TaskSession å’Œä¿å­˜ SubTask
                if router_mode == "complex" and task_session_id:
                    print(f"[STREAM] æ›´æ–°å¤æ‚æ¨¡å¼æ•°æ®: {len(collected_task_list)} ä¸ªä»»åŠ¡, session={task_session_id}")
                    # æ›´æ–° thread çš„ task_session_id
                    thread_obj.task_session_id = task_session_id
                    print(f"[STREAM] âœ… å·²è®¾ç½® thread.task_session_id = {task_session_id}")

                    # æ›´æ–° TaskSession çŠ¶æ€ä¸ºå®Œæˆ
                    update_task_session_status(
                        save_session,
                        task_session_id,
                        "completed",
                        final_response=full_response
                    )

                    # è·å–å·²å­˜åœ¨çš„ SubTasksï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
                    existing_subtasks = get_subtasks_by_session(save_session, task_session_id)
                    existing_subtask_ids = {st.id for st in existing_subtasks}

                    # ä¿å­˜/æ›´æ–° SubTasks
                    for idx, task in enumerate(collected_task_list):
                        task_id = task.get("id")
                        expert_type = task.get("expert_type", "")
                        # ä½¿ç”¨ task_id è·å– artifactsï¼ˆä¸æ”¶é›†æ—¶ä¸€è‡´ï¼‰
                        artifacts_for_task = expert_artifacts.get(task_id, [])

                        if task_id and task_id in existing_subtask_ids:
                            # æ›´æ–°ç°æœ‰ SubTask
                            # output_result å·²ç»æ˜¯ {"content": "..."} æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                            output_value = task.get("output_result", {"content": ""})
                            # å…¼å®¹å¤„ç†ï¼šå¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™åŒ…è£…
                            if isinstance(output_value, dict):
                                output_result = output_value
                            else:
                                output_result = {"content": str(output_value)}

                            update_subtask_status(
                                save_session,
                                task_id,
                                status=task.get("status", "completed"),
                                output_result=output_result,
                                duration_ms=task.get("duration_ms")
                            )
                            print(f"[STREAM] âœ… SubTask çŠ¶æ€å·²æ›´æ–°: {expert_type}")

                            # ä¿å­˜ artifacts
                            if artifacts_for_task:
                                try:
                                    created = create_artifacts_batch(save_session, task_id, artifacts_for_task)
                                    print(f"[STREAM] âœ… æˆåŠŸä¿å­˜ {len(created)} ä¸ª artifacts åˆ° SubTask: {task_id}")
                                except Exception as art_err:
                                    print(f"[STREAM] âŒ ä¿å­˜ artifacts å¤±è´¥: {art_err}")
                                    import traceback
                                    traceback.print_exc()
                        else:
                            # åˆ›å»ºæ–° SubTask
                            create_subtask(
                                save_session,
                                task_session_id=task_session_id,
                                expert_type=expert_type,
                                task_description=task.get("description", ""),
                                sort_order=task.get("sort_order", 0),
                                input_data=task.get("input_data", {})
                            )

                save_session.add(thread_obj)
                save_session.commit()

        yield "data: [DONE]\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


async def _handle_langgraph_sync(
    initial_state: dict,
    thread_id: str,
    thread: Thread,
    user_message: str,
    session: Session
) -> dict:
    """å¤„ç† LangGraph éæµå¼å“åº” (v3.5 HITL æ”¯æŒ)"""
    # ğŸ”¥ğŸ”¥ğŸ”¥ v3.5: ä½¿ç”¨ AsyncPostgresSaver å®ç°æŒä¹…åŒ–
    async with get_db_connection() as conn:
        checkpointer = AsyncPostgresSaver(conn)
        graph = create_smart_router_workflow(checkpointer=checkpointer)
        print(f"[SYNC MODE] Graph compiled with AsyncPostgresSaver for HITL")
        
        # ğŸ”¥ æ·»åŠ  config ä¼ é€’ thread_id ç»™ checkpointerï¼Œå¹¶è®¾ç½®é€’å½’é™åˆ¶
        # æ³¨æ„ï¼šrecursion_limit å¿…é¡»åœ¨ config é¡¶å±‚ï¼Œä¸èƒ½åœ¨ configurable ä¸­
        result = await graph.ainvoke(
            initial_state,
            config={
                "recursion_limit": 100,  # ğŸ”¥ è®¾ç½®é€’å½’é™åˆ¶ï¼ˆæ”¾åœ¨é¡¶å±‚ï¼ï¼‰
                "configurable": {
                    "thread_id": thread_id
                }
            }
        )
    last_message = result["messages"][-1]

    # è·å– Router å†³ç­–å¹¶æ›´æ–° thread_mode
    router_decision = result.get("router_decision", "simple")
    thread.thread_mode = router_decision
    
    # v3.0: å°½æ—©è®¾ç½® agent_typeï¼Œè¿™æ ·å³ä½¿ä»»åŠ¡è¿›è¡Œä¸­åˆ·æ–°ä¹Ÿèƒ½æ­£ç¡®æ¢å¤çŠ¶æ€
    if router_decision == "complex":
        thread.agent_type = "ai"
        session.add(thread)
        session.flush()

        # åˆ›å»º TaskSession
        task_session = TaskSession(
            session_id=str(uuid4()),
            thread_id=thread_id,
            user_query=user_message,
            status="completed",
            final_response=last_message.content,
            created_at=datetime.now(),
            updated_at=datetime.now(),
            completed_at=datetime.now()
        )
        session.add(task_session)
        session.flush()

        # æ›´æ–° thread çš„ task_session_id
        thread.task_session_id = task_session.session_id

        # ä¿å­˜ SubTaskï¼ˆåŒ…æ‹¬ artifactsï¼‰
        for subtask in result["task_list"]:
            artifacts = subtask.get("artifact")
            if artifacts:
                artifacts = [artifacts] if isinstance(artifacts, dict) else artifacts

            db_subtask = SubTask(
                id=subtask["id"],
                expert_type=subtask["expert_type"],
                task_description=subtask["description"],
                input_data=subtask["input_data"],
                status=subtask["status"],
                output_result=subtask["output_result"],
                artifacts=artifacts,
                started_at=subtask.get("started_at"),
                completed_at=subtask.get("completed_at"),
                created_at=subtask.get("created_at"),
                updated_at=subtask.get("updated_at"),
                task_session_id=task_session.session_id
            )
            session.add(db_subtask)

    # ä¿å­˜ AI å›å¤
    # è§£æ thinking æ ‡ç­¾
    clean_content, thinking_data = parse_thinking(last_message.content)
    ai_msg_db = Message(
        thread_id=thread_id,
        role="assistant",
        content=clean_content,  # ä¿å­˜æ¸…ç†åçš„å†…å®¹
        extra_data={'thinking': thinking_data} if thinking_data else None,
        timestamp=datetime.now()
    )
    session.add(ai_msg_db)
    thread.updated_at = datetime.now()
    session.add(thread)
    session.commit()

    return {
        "role": "assistant",
        "content": last_message.content,
        "conversationId": thread_id,
        "threadMode": router_decision
    }


# ============================================================================
# HITL (Human-in-the-Loop) - æµå¼æ¢å¤æ¥å£
# ============================================================================

@router.post("/chat/resume")
async def resume_chat(
    request: ResumeRequest,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """
    æ¢å¤è¢«ä¸­æ–­çš„ HITL æµç¨‹ï¼ˆæµå¼å“åº”ï¼‰
    
    å½“ç”¨æˆ·åœ¨å‰ç«¯å®¡æ ¸è®¡åˆ’åï¼Œè°ƒç”¨æ­¤æ¥å£ç»§ç»­æ‰§è¡Œã€‚
    è¿”å› SSE æµï¼ŒåŒ…å«åç»­æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œäº‹ä»¶ã€‚
    """
    print(f"[HITL RESUME] thread_id={request.thread_id}, approved={request.approved}")
    
    # éªŒè¯ thread å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
    thread = session.get(Thread, request.thread_id)
    if not thread:
        raise NotFoundError(f"Thread not found: {request.thread_id}")
    if thread.user_id != current_user.id:
        raise AuthorizationError("æ— æƒè®¿é—®æ­¤çº¿ç¨‹")
    
    # å¦‚æœç”¨æˆ·æ‹’ç»ï¼Œæ¸…ç†çŠ¶æ€å¹¶ç»“æŸæµç¨‹
    if not request.approved:
        print(f"[HITL RESUME] ç”¨æˆ·æ‹’ç»äº†è®¡åˆ’ï¼Œæ¸…ç†çŠ¶æ€")
        
        # ğŸ”¥ æ¸…ç† LangGraph checkpointï¼ˆé¿å…åƒµå°¸çŠ¶æ€ï¼‰
        try:
            # Windows å…¼å®¹ï¼šä½¿ç”¨åŒæ­¥è¿æ¥æ¸…ç†
            import psycopg
            db_url = os.getenv("DATABASE_URL", "")
            db_url = db_url.replace("postgresql+asyncpg", "postgresql").replace("postgresql+psycopg", "postgresql")
            
            with psycopg.connect(db_url) as conn:
                with conn.cursor() as cur:
                    # å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                    cur.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_name = 'checkpoints'
                        )
                    """)
                    if cur.fetchone()[0]:
                        cur.execute(
                            "DELETE FROM checkpoints WHERE thread_id = %s",
                            (request.thread_id,)
                        )
                        deleted = cur.rowcount
                        print(f"[HITL RESUME] æ¸…ç†äº† {deleted} ä¸ª checkpoint(s)")
                    else:
                        print("[HITL RESUME] checkpoints è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†")
                conn.commit()
        except Exception as e:
            # å¦‚æœè¡¨ä¸å­˜åœ¨æˆ–å…¶ä»–é”™è¯¯ï¼Œè®°å½•ä½†ä¸é˜»æ–­æµç¨‹
            print(f"[HITL RESUME WARN] æ¸…ç† checkpoint å¤±è´¥: {e}")
        
        # ğŸ”¥ æ›´æ–° task_session çŠ¶æ€ä¸º cancelledï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            task_session = session.exec(
                select(TaskSession).where(TaskSession.thread_id == request.thread_id)
            ).first()
            if task_session:
                task_session.status = "cancelled"
                task_session.final_response = "è®¡åˆ’è¢«ç”¨æˆ·å–æ¶ˆ"
                task_session.updated_at = datetime.now()
                session.add(task_session)
                session.commit()
                print(f"[HITL RESUME] TaskSession {task_session.session_id} å·²æ ‡è®°ä¸º cancelled")
        except Exception as e:
            print(f"[HITL RESUME WARN] æ›´æ–° task_session å¤±è´¥: {e}")
        
        return {"status": "cancelled", "message": "è®¡åˆ’å·²è¢«ç”¨æˆ·æ‹’ç»"}
    
    # ğŸ”¥ æµå¼æ¢å¤æ‰§è¡Œ
    async def resume_stream_generator():
        """æµå¼æ¢å¤ç”Ÿæˆå™¨"""
        async with get_db_connection() as conn:
            checkpointer = AsyncPostgresSaver(conn)
            graph = create_smart_router_workflow(checkpointer=checkpointer)
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ åˆ›å»ºå…±äº«é˜Ÿåˆ—ç”¨äºå®æ—¶æµå¼æ¨é€
            stream_queue = asyncio.Queue()
            
            config = {
                "recursion_limit": 100,
                "configurable": {
                    "thread_id": request.thread_id,
                    "stream_queue": stream_queue  # ğŸ”¥ ä¼ é€’ç»™ aggregator ç”¨äºå®æ—¶æ¨é€
                }
            }
            
            # 1. å¦‚æœç”¨æˆ·ä¿®æ”¹äº†è®¡åˆ’ï¼Œæ›´æ–°çŠ¶æ€
            if request.updated_plan:
                print(f"[HITL RESUME] æ›´æ–°è®¡åˆ’ï¼Œä»»åŠ¡æ•°: {len(request.updated_plan)}")
                
                # ğŸ”¥ğŸ”¥ğŸ”¥ æ¸…ç†å·²åˆ é™¤ä»»åŠ¡çš„ä¾èµ–å…³ç³»
                # è·å–å½“å‰ä¿ç•™çš„ä»»åŠ¡IDé›†åˆ
                kept_task_ids = {task.get("id") for task in request.updated_plan}
                print(f"[HITL RESUME] ä¿ç•™çš„ä»»åŠ¡ID: {kept_task_ids}")
                
                # æ¸…ç†æ¯ä¸ªä»»åŠ¡çš„ depends_on ä¸­æŒ‡å‘å·²åˆ é™¤ä»»åŠ¡çš„ä¾èµ–
                cleaned_plan = []
                for task in request.updated_plan:
                    cleaned_task = dict(task)
                    if cleaned_task.get("depends_on"):
                        original_deps = cleaned_task["depends_on"]
                        # åªä¿ç•™æŒ‡å‘ä»ç„¶å­˜åœ¨ä»»åŠ¡çš„ä¾èµ–
                        cleaned_deps = [dep for dep in original_deps if dep in kept_task_ids]
                        if len(cleaned_deps) != len(original_deps):
                            print(f"[HITL RESUME] ä»»åŠ¡ {cleaned_task.get('id')} çš„ä¾èµ–å·²æ¸…ç†: {original_deps} -> {cleaned_deps}")
                        cleaned_task["depends_on"] = cleaned_deps if cleaned_deps else None
                    cleaned_plan.append(cleaned_task)
                
                await graph.aupdate_state(config, {"task_list": cleaned_plan})
            
            # 2. ğŸ”¥ğŸ”¥ğŸ”¥ æµå¼æ¢å¤æ‰§è¡Œï¼ˆå¿…é¡»ä½¿ç”¨ astream_events ä¿æŒ SSEï¼‰
            # ä¼ å…¥ None ä½œä¸º inputï¼ŒLangGraph è‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­
            
            # ğŸ”¥ åˆ›å»ºä¸¤ä¸ªé˜Ÿåˆ—ï¼š
            # - realtime_queue: ç»™ aggregator ç”¨äºå®æ—¶æ¨é€ message.delta
            # - sse_queue: ç”¨äºæ”¶é›†æ‰€æœ‰ SSE äº‹ä»¶å‘é€ç»™å‰ç«¯
            realtime_queue = asyncio.Queue()  # aggregator å®æ—¶æ¨é€ç”¨
            sse_queue = asyncio.Queue()       # SSE è¾“å‡ºç»™å‰ç«¯ç”¨
            
            config = {
                "recursion_limit": 100,
                "configurable": {
                    "thread_id": request.thread_id,
                    "stream_queue": realtime_queue  # ğŸ”¥ ä¼ é€’ç»™ aggregator
                }
            }
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ é¦–å…ˆå‘é€ plan.created äº‹ä»¶ï¼Œåˆå§‹åŒ–å‰ç«¯ thinking æ­¥éª¤
            plan_tasks = request.updated_plan if request.updated_plan else []
            if plan_tasks:
                from event_types.events import EventType, PlanCreatedData, build_sse_event
                from utils.event_generator import sse_event_to_string
                
                plan_event = build_sse_event(
                    EventType.PLAN_CREATED,
                    PlanCreatedData(
                        session_id=request.thread_id,
                        tasks=[
                            {
                                "id": task.get("id", f"task-{i}"),
                                "expert_type": task.get("expert_type", "unknown"),
                                "description": task.get("description", ""),
                                "sort_order": task.get("sort_order", i),
                                "status": task.get("status", "pending")
                            }
                            for i, task in enumerate(plan_tasks)
                        ],
                        estimated_steps=len(plan_tasks),
                        execution_mode="sequential",
                        summary=f"æ¢å¤æ‰§è¡Œ {len(plan_tasks)} ä¸ªä»»åŠ¡"
                    ),
                    str(uuid4())
                )
                await sse_queue.put({
                    "type": "sse",
                    "event": sse_event_to_string(plan_event)
                })
                print(f"[HITL RESUME] å·²å‘é€ plan.created äº‹ä»¶ï¼Œä»»åŠ¡æ•°: {len(plan_tasks)}")
            
            async def producer():
                """ç”Ÿäº§è€…ï¼šè¿è¡Œ LangGraphï¼Œå°†äº‹ä»¶è½¬æ¢ä¸º SSE æ”¾å…¥é˜Ÿåˆ—"""
                try:
                    print(f"[RESUME PRODUCER] å¼€å§‹æµå¼æ¢å¤æ‰§è¡Œ...")
                    event_count = 0
                    loop_count = 0
                    realtime_event_count = 0
                    
                    # ğŸ”¥ğŸ”¥ğŸ”¥ åˆ›å»ºç‹¬ç«‹ä»»åŠ¡æŒç»­æ”¶é›† realtime_queue äº‹ä»¶
                    # é¿å…åœ¨ graph.astream_events å¾ªç¯å†…éƒ¨æ”¶é›†å¯¼è‡´çš„æ—¶åºé—®é¢˜
                    realtime_events_collector = []
                    collector_task = None
                    
                    async def collect_realtime_events():
                        """ç‹¬ç«‹åç¨‹ï¼šæŒç»­æ”¶é›† realtime_queue äº‹ä»¶"""
                        nonlocal realtime_event_count
                        while True:
                            try:
                                realtime_event = await asyncio.wait_for(realtime_queue.get(), timeout=0.5)
                                if realtime_event and realtime_event.get("type") == "sse":
                                    realtime_events_collector.append(realtime_event)
                                    realtime_event_count += 1
                                    if realtime_event_count % 10 == 0:
                                        print(f"[RESUME PRODUCER] å·²æ”¶é›† {realtime_event_count} ä¸ª realtime äº‹ä»¶")
                            except asyncio.TimeoutError:
                                continue
                            except Exception as e:
                                print(f"[RESUME PRODUCER] æ”¶é›† realtime äº‹ä»¶é”™è¯¯: {e}")
                                break
                    
                    # å¯åŠ¨æ”¶é›†å™¨åç¨‹
                    collector_task = asyncio.create_task(collect_realtime_events())
                    
                    # ğŸ”¥ğŸ”¥ğŸ”¥ å¾ªç¯æ‰§è¡Œç›´åˆ°æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆå¤„ç†å¤šè½®ä¸­æ–­ï¼‰
                    while True:
                        loop_count += 1
                        print(f"[RESUME PRODUCER] ç¬¬ {loop_count} è½®æ‰§è¡Œ...")
                        
                        async for event in graph.astream_events(
                            None,  # ä» checkpoint ç»§ç»­
                            config=config,
                            version="v2"
                        ):
                            kind = event.get("event", "")
                            name = event.get("name", "")
                            data = event.get("data", {})
                            
                            # ğŸ”¥ğŸ”¥ğŸ”¥ å¤„ç†èŠ‚ç‚¹è¿”å›çš„ event_queue äº‹ä»¶
                            if kind == "on_chain_end":
                                output_data = data.get("output", {})
                                
                                if isinstance(output_data, dict):
                                    event_queue = output_data.get("event_queue", [])
                                    for queued_event in event_queue:
                                        if queued_event.get("type") == "sse":
                                            await sse_queue.put({
                                                "type": "sse",
                                                "event": queued_event["event"]
                                            })
                                            event_count += 1
                                            
                                    if event_queue:
                                        print(f"[RESUME PRODUCER] èŠ‚ç‚¹ '{name}' è¿”å› {len(event_queue)} ä¸ªäº‹ä»¶")
                            
                            # å¤„ç† aggregator å®Œæˆ
                            if kind == "on_chain_end" and name == "aggregator":
                                print(f"[RESUME PRODUCER] Aggregator å®Œæˆ")
                        
                        # ğŸ”¥ğŸ”¥ğŸ”¥ å°†æ”¶é›†åˆ°çš„ realtime äº‹ä»¶å‘é€åˆ° sse_queue
                        if realtime_events_collector:
                            flush_count = len(realtime_events_collector)
                            for evt in realtime_events_collector:
                                await sse_queue.put(evt)
                                event_count += 1
                            realtime_events_collector.clear()
                            print(f"[RESUME PRODUCER] æœ¬è½®åˆ·æ–° {flush_count} ä¸ª realtime äº‹ä»¶")
                        
                        # æ£€æŸ¥æ˜¯å¦å®Œæˆæˆ–å†æ¬¡ä¸­æ–­
                        snapshot = await graph.aget_state(config)
                        if not snapshot.next:
                            # ç­‰å¾…æ”¶é›†å™¨å®Œæˆ
                            if collector_task and not collector_task.done():
                                collector_task.cancel()
                                try:
                                    await collector_task
                                except asyncio.CancelledError:
                                    pass
                            # æœ€ååˆ·æ–°ä¸€æ¬¡ realtime äº‹ä»¶
                            if realtime_events_collector:
                                for evt in realtime_events_collector:
                                    await sse_queue.put(evt)
                                    event_count += 1
                            print(f"[RESUME PRODUCER] æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå…± {event_count} ä¸ªäº‹ä»¶ (realtime: {realtime_event_count})")
                            break
                        
                        print(f"[RESUME PRODUCER] æ£€æµ‹åˆ°ä¸­æ–­ï¼Œè‡ªåŠ¨ç»§ç»­æ‰§è¡Œ...")
                    
                    # å‘é€ message.done
                    await sse_queue.put({
                        "type": "sse",
                        "event": f"event: message.done\ndata: {json.dumps({'type': 'message.done'})}\n\n"
                    })
                        
                except Exception as e:
                    print(f"[RESUME PRODUCER] é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
                    await sse_queue.put({"type": "graph_error", "error": str(e)})
                finally:
                    await sse_queue.put(None)
            
            # å¯åŠ¨ç”Ÿäº§è€…
            producer_task = asyncio.create_task(producer())
            
            # æ¶ˆè´¹å¹¶ yield SSE
            try:
                while True:
                    try:
                        token = await asyncio.wait_for(sse_queue.get(), timeout=1.0)
                    except asyncio.TimeoutError:
                        if producer_task.done():
                            while not sse_queue.empty():
                                item = sse_queue.get_nowait()
                                if item is None:
                                    break
                                if item.get("type") == "sse":
                                    yield item["event"]
                                elif item.get("type") == "hitl_interrupt":
                                    yield f"event: human.interrupt\ndata: {json.dumps(item['data'])}\n\n"
                                elif item.get("type") == "graph_error":
                                    yield f"event: error\ndata: {json.dumps({'error': item.get('error')})}\n\n"
                            break
                        yield ": keep-alive\n\n"
                        continue
                    
                    if token is None:
                        break
                    
                    if token.get("type") == "sse":
                        yield token["event"]
                    elif token.get("type") == "hitl_interrupt":
                        yield f"event: human.interrupt\ndata: {json.dumps(token['data'])}\n\n"
                    elif token.get("type") == "graph_error":
                        yield f"event: error\ndata: {json.dumps({'error': token.get('error')})}\n\n"
                        
            finally:
                if not producer_task.done():
                    producer_task.cancel()
    
    return StreamingResponse(
        resume_stream_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


def format_resume_event(token: dict) -> Optional[str]:
    """æ ¼å¼åŒ–æ¢å¤æµä¸­çš„äº‹ä»¶ä¸º SSE"""
    if token.get("type") == "graph_error":
        error_msg = token.get("error", "æœªçŸ¥é”™è¯¯")
        event_payload = {"type": "error", "message": error_msg}
        return f"event: error\ndata: {json.dumps(event_payload)}\n\n"
    
    elif token.get("type") == "hitl_interrupt":
        interrupt_data = token.get("data", {})
        event_payload = {"type": "human.interrupt", "data": interrupt_data}
        return f"event: human.interrupt\ndata: {json.dumps(event_payload)}\n\n"
    
    elif token.get("type") == "graph_event":
        event = token["event"]
        kind = event.get("event", "")
        name = event.get("name", "")
        data = event.get("data", {})
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ å¤„ç† task ç›¸å…³äº‹ä»¶ï¼ˆä» event_queue ä¸­æå–ï¼‰
        if kind == "on_chain_end":
            output_data = data.get("output", {})
            if isinstance(output_data, dict):
                event_queue = output_data.get("event_queue", [])
                for queued_event in event_queue:
                    if queued_event.get("type") == "sse":
                        return queued_event["event"]
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ å¤„ç† generic worker èŠ‚ç‚¹ï¼ˆtask æ‰§è¡Œï¼‰
        if kind == "on_chain_start" and name == "generic":
            # ä»»åŠ¡å¼€å§‹
            input_data = data.get("input", {})
            task_list = input_data.get("task_list", [])
            current_index = input_data.get("current_task_index", 0)
            if task_list and current_index < len(task_list):
                task = task_list[current_index]
                event_payload = {
                    "type": "task.started",
                    "data": {
                        "task_id": task.get("id"),
                        "expert_type": task.get("expert_type"),
                        "description": task.get("description"),
                        "started_at": datetime.now().isoformat()
                    }
                }
                return f"event: task.started\ndata: {json.dumps(event_payload)}\n\n"
        
        if kind == "on_chain_end" and name == "generic":
            # ä»»åŠ¡å®Œæˆ
            output_data = data.get("output", {})
            task_result = output_data.get("__task_result", {})
            if task_result:
                event_payload = {
                    "type": "task.completed",
                    "data": {
                        "task_id": task_result.get("task_id"),
                        "expert_type": task_result.get("expert_type"),
                        "status": "completed",
                        "completed_at": datetime.now().isoformat()
                    }
                }
                return f"event: task.completed\ndata: {json.dumps(event_payload)}\n\n"
        
        # ğŸ”¥ å¤„ç† message.done äº‹ä»¶ï¼ˆæµç»“æŸæ ‡å¿—ï¼‰
        if kind == "on_chain_end" and name == "aggregator":
            return f"event: message.done\ndata: {json.dumps({'type': 'message.done'})}\n\n"
    
    return None
