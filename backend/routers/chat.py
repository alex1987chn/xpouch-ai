"""
èŠå¤©è·¯ç”±æ¨¡å— - åŒ…å«ä¸»è¦èŠå¤©ç«¯ç‚¹å’Œçº¿ç¨‹ç®¡ç†
"""
import os
import json
import re
from datetime import datetime
from typing import List, Optional, AsyncGenerator
from uuid import uuid4

from fastapi import APIRouter, Depends, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sqlmodel import Session, select
from sqlalchemy.orm import selectinload
from langchain_core.messages import HumanMessage, AIMessage

from database import get_session, engine
from dependencies import get_current_user, get_current_user_with_auth
from models import (
    User, Thread, Message, CustomAgent, TaskSession, SubTask
)
from constants import (
    normalize_agent_id,
    SYSTEM_AGENT_ORCHESTRATOR,
    SYSTEM_AGENT_DEFAULT_CHAT
)
from utils.artifacts import parse_artifacts_from_response, generate_artifact_event
from utils.llm_factory import get_llm_instance
from agents.graph import commander_graph
from utils.exceptions import AppError, NotFoundError, AuthorizationError


router = APIRouter(prefix="/api", tags=["chat"])


# ============================================================================
# è¯·æ±‚æ¨¡å‹
# ============================================================================

class ChatMessageDTO(BaseModel):
    role: str
    content: str
    id: Optional[str] = None
    timestamp: Optional[str] = None


class ChatRequest(BaseModel):
    message: str
    history: List[ChatMessageDTO]
    conversationId: Optional[str] = None
    agentId: Optional[str] = "assistant"
    stream: Optional[bool] = True


# ============================================================================
# æµå¼è¾“å‡ºè¿‡æ»¤å‡½æ•°
# ============================================================================

def should_stream_event(event_tags: list, router_mode: str, name: str = "") -> tuple[bool, str]:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥å°†å½“å‰äº‹ä»¶æµå¼è¾“å‡ºåˆ°å‰ç«¯
    
    Args:
        event_tags: äº‹ä»¶æ ‡ç­¾åˆ—è¡¨
        router_mode: å½“å‰è·¯ç”±æ¨¡å¼ ("", "simple", "complex")
        name: äº‹ä»¶åç§°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    
    Returns:
        tuple[bool, str]: (æ˜¯å¦åº”è¾“å‡º, è·³è¿‡åŸå› )

    è¿‡æ»¤è§„åˆ™ï¼š
    - Router æ¨¡å¼æœªçŸ¥æ—¶: è·³è¿‡æ‰€æœ‰å†…éƒ¨èŠ‚ç‚¹ (router/planner/expert)
    - Simple æ¨¡å¼: åªå…è®¸ direct_reply èŠ‚ç‚¹
    - Complex æ¨¡å¼: è·³è¿‡å†…éƒ¨èŠ‚ç‚¹ï¼Œä¿ç•™ Aggregator è¾“å‡º
    """
    tags_str = str(event_tags).lower()
    
    # Router å†³ç­–æœªçŸ¥æ—¶ï¼Œè·³è¿‡æ‰€æœ‰å†…éƒ¨èŠ‚ç‚¹
    if router_mode == "":
        if any(tag in tags_str for tag in ["router", "commander", "planner", "expert"]):
            return False, f"Router å†³ç­–æœªçŸ¥ï¼Œè·³è¿‡å†…éƒ¨èŠ‚ç‚¹: {tags_str}"
    
    # Simple æ¨¡å¼ï¼šåªå…è®¸ direct_reply èŠ‚ç‚¹
    elif router_mode == "simple":
        if "direct_reply" not in tags_str:
            return False, f"Simple æ¨¡å¼ï¼šè·³è¿‡é direct_reply: {tags_str}"
    
    # Complex æ¨¡å¼ï¼šè·³è¿‡å†…éƒ¨è§„åˆ’èŠ‚ç‚¹å’Œä¸“å®¶
    else:  # router_mode == "complex"
        if any(tag in tags_str for tag in ["router", "commander", "planner", "expert"]):
            return False, f"Complex æ¨¡å¼ï¼šè·³è¿‡å†…éƒ¨èŠ‚ç‚¹: {tags_str}"
    
    return True, "é€šè¿‡è¿‡æ»¤"


def is_task_plan_content(content: str) -> bool:
    """
    æ£€æŸ¥å†…å®¹æ˜¯å¦æ˜¯ä»»åŠ¡è®¡åˆ’ JSON
    
    ç”¨äºè¿‡æ»¤æ‰ä¸åº”å±•ç¤ºç»™ç”¨æˆ·çš„å†…éƒ¨ä»»åŠ¡è®¡åˆ’æ•°æ®
    """
    if not content:
        return False
    
    content_stripped = content.strip()
    
    # ç§»é™¤ Markdown ä»£ç å—æ ‡è®°
    code_block_match = re.match(r'^```(?:json)?\s*([\s\S]*?)\s*```$', content_stripped)
    if code_block_match:
        content_stripped = code_block_match.group(1).strip()
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ JSON æ ¼å¼çš„ä»»åŠ¡è®¡åˆ’
    if content_stripped.startswith('{'):
        content_lower = content_stripped.lower()
        if (('"tasks"' in content_lower and '"strategy"' in content_lower) or
            ('"tasks"' in content_lower and '"expert_type"' in content_lower) or
            ('"estimated_steps"' in content_lower)):
            return True
    
    return False


# ============================================================================
# çº¿ç¨‹ç®¡ç† API
# ============================================================================

@router.get("/threads", response_model=List[dict])
async def get_threads(
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """è·å–å½“å‰ç”¨æˆ·çš„æ‰€æœ‰çº¿ç¨‹åˆ—è¡¨"""
    statement = (
        select(Thread)
        .where(Thread.user_id == current_user.id)
        .options(selectinload(Thread.messages))
        .order_by(Thread.updated_at.desc())
    )
    threads = session.exec(statement).all()
    
    # æ„å»ºå“åº”
    result = []
    for thread in threads:
        result.append({
            "id": thread.id,
            "title": thread.title,
            "agent_id": thread.agent_id,
            "agent_type": thread.agent_type,
            "thread_mode": thread.thread_mode,
            "user_id": thread.user_id,
            "task_session_id": thread.task_session_id,
            "created_at": thread.created_at.isoformat() if thread.created_at else None,
            "updated_at": thread.updated_at.isoformat() if thread.updated_at else None,
            "messages": [
                {
                    "id": msg.id,
                    "role": msg.role,
                    "content": msg.content,
                    "timestamp": msg.timestamp.isoformat() if msg.timestamp else None
                }
                for msg in thread.messages
            ]
        })
    return result


@router.get("/threads/{thread_id}")
async def get_thread(
    thread_id: str,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """è·å–å•ä¸ªçº¿ç¨‹è¯¦æƒ…"""
    statement = (
        select(Thread)
        .where(Thread.id == thread_id)
        .options(selectinload(Thread.messages))
    )
    thread = session.exec(statement).first()

    if not thread or thread.user_id != current_user.id:
        raise NotFoundError(resource="ä¼šè¯")

    # å¦‚æœæ˜¯AIåŠ©æ‰‹çº¿ç¨‹ï¼ˆå¤æ‚æ¨¡å¼ï¼‰ï¼ŒåŠ è½½TaskSessionå’ŒSubTask
    if thread.agent_type == "ai" and thread.task_session_id:
        task_session = session.get(TaskSession, thread.task_session_id)
        if task_session:
            # åŠ è½½SubTasks
            statement = select(SubTask).where(SubTask.task_session_id == task_session.session_id)
            sub_tasks = session.exec(statement).all()

            return {
                "id": thread.id,
                "title": thread.title,
                "agent_id": thread.agent_id,
                "agent_type": thread.agent_type,
                "user_id": thread.user_id,
                "task_session_id": thread.task_session_id,
                "created_at": thread.created_at.isoformat() if thread.created_at else None,
                "updated_at": thread.updated_at.isoformat() if thread.updated_at else None,
                "messages": [
                    {
                        "id": msg.id,
                        "role": msg.role,
                        "content": msg.content,
                        "timestamp": msg.timestamp.isoformat() if msg.timestamp else None,
                        "extra_data": msg.extra_data  # ğŸ‘ˆ æ–°å¢ï¼šè¿”å› extra_dataï¼ˆåŸ metadataï¼‰
                    }
                    for msg in thread.messages
                ],
                "task_session": {
                    "session_id": task_session.session_id,
                    "user_query": task_session.user_query,
                    "final_response": task_session.final_response,
                    "status": task_session.status,
                    "sub_tasks": [
                        {
                            "id": st.id,
                            "task_session_id": st.task_session_id,
                            "expert_type": st.expert_type,
                            "task_description": st.task_description,
                            "status": st.status,
                            "output": st.output,
                            "error": st.error,
                            "artifacts": st.artifacts,
                            "duration_ms": st.duration_ms,
                            "created_at": st.created_at.isoformat() if st.created_at else None
                        }
                        for st in sub_tasks
                    ]
                }
            }

    # å¯¹äºéAIçº¿ç¨‹ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
    return {
        "id": thread.id,
        "title": thread.title,
        "agent_id": thread.agent_id,
        "agent_type": thread.agent_type,
        "user_id": thread.user_id,
        "task_session_id": thread.task_session_id,
        "created_at": thread.created_at.isoformat() if thread.created_at else None,
        "updated_at": thread.updated_at.isoformat() if thread.updated_at else None,
        "messages": [
            {
                "id": msg.id,
                "role": msg.role,
                "content": msg.content,
                "timestamp": msg.timestamp.isoformat() if msg.timestamp else None,
                "extra_data": msg.extra_data  # ğŸ‘ˆ æ–°å¢ï¼šè¿”å› extra_dataï¼ˆåŸ metadataï¼‰
            }
            for msg in thread.messages
        ]
    }


@router.delete("/threads/{thread_id}")
async def delete_thread(
    thread_id: str,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """åˆ é™¤çº¿ç¨‹"""
    thread = session.get(Thread, thread_id)
    if not thread or thread.user_id != current_user.id:
        raise NotFoundError(resource="ä¼šè¯")
    session.delete(thread)
    session.commit()
    return {"ok": True}


# ============================================================================
# ä¸»è¦èŠå¤©ç«¯ç‚¹
# ============================================================================

@router.post("/chat")
async def chat_endpoint(
    request: ChatRequest,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """
    ç»Ÿä¸€èŠå¤©ç«¯ç‚¹ï¼ˆç®€å•æ¨¡å¼ + å¤æ‚æ¨¡å¼ï¼‰
    
    - è‡ªå®šä¹‰æ™ºèƒ½ä½“ï¼šç›´æ¥è°ƒç”¨ LLMï¼Œä¸ç»è¿‡ LangGraph
    - ç³»ç»Ÿé»˜è®¤åŠ©æ‰‹ï¼šé€šè¿‡ LangGraph (Router -> Planner -> Experts) å¤„ç†
    """
    # 1. ç¡®å®š Thread ID
    thread_id = request.conversationId
    thread = None

    if thread_id:
        thread = session.get(Thread, thread_id)
        if thread and thread.user_id != current_user.id:
            raise AuthorizationError("æ²¡æœ‰æƒé™è®¿é—®æ­¤ä¼šè¯")

    if not thread:
        # å¦‚æœæ²¡æœ‰IDæˆ–æ‰¾ä¸åˆ°ï¼Œåˆ›å»ºæ–°çº¿ç¨‹
        if not thread_id:
            thread_id = str(uuid4())

        # å…œåº•é€»è¾‘ï¼šå¦‚æœ agentId ä¸º Noneã€null æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œå¼ºåˆ¶èµ‹å€¼ä¸ºç³»ç»Ÿé»˜è®¤åŠ©æ‰‹
        if not request.agentId or request.agentId.strip() == "":
            frontend_agent_id = SYSTEM_AGENT_DEFAULT_CHAT
        else:
            frontend_agent_id = normalize_agent_id(request.agentId)

        # sys-task-orchestrator æ˜¯å†…éƒ¨å®ç°ï¼Œä¸åº”åœ¨ URL ä¸­æš´éœ²
        if frontend_agent_id == SYSTEM_AGENT_ORCHESTRATOR:
            frontend_agent_id = SYSTEM_AGENT_DEFAULT_CHAT

        # æ ¹æ® agentId ç¡®å®š agent_type
        custom_agent_check = session.get(CustomAgent, frontend_agent_id)
        if custom_agent_check and custom_agent_check.user_id == current_user.id:
            agent_type = "custom"
            final_agent_id = frontend_agent_id
        else:
            agent_type = "default"
            final_agent_id = SYSTEM_AGENT_DEFAULT_CHAT

        # åˆå§‹ thread_mode ä¸º simpleï¼ŒRouter ä¼šåœ¨å¤„ç†æ—¶æ›´æ–°å®ƒ
        thread = Thread(
            id=thread_id,
            title=request.message[:30] + "..." if len(request.message) > 30 else request.message,
            agent_id=final_agent_id,
            agent_type=agent_type,
            thread_mode="simple",
            user_id=current_user.id,
            created_at=datetime.now(),
            updated_at=datetime.now()
        )
        session.add(thread)
        session.commit()
        session.refresh(thread)

    # 2. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°æ•°æ®åº“
    user_msg_db = Message(
        thread_id=thread_id,
        role="user",
        content=request.message,
        timestamp=datetime.now()
    )
    session.add(user_msg_db)
    session.commit()

    # 3. å‡†å¤‡ LangGraph ä¸Šä¸‹æ–‡
    statement = select(Message).where(Message.thread_id == thread_id).order_by(Message.timestamp)
    db_messages = session.exec(statement).all()

    langchain_messages = []
    for msg in db_messages:
        if msg.role == "user":
            langchain_messages.append(HumanMessage(content=msg.content))
        elif msg.role == "assistant":
            langchain_messages.append(AIMessage(content=msg.content))

    # æ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå®šä¹‰æ™ºèƒ½ä½“
    custom_agent = None
    normalized_agent_id = normalize_agent_id(request.agentId)

    if normalized_agent_id == SYSTEM_AGENT_ORCHESTRATOR:
        normalized_agent_id = SYSTEM_AGENT_DEFAULT_CHAT

    is_system_default = False

    if normalized_agent_id == SYSTEM_AGENT_DEFAULT_CHAT:
        is_system_default = True
        custom_agent = None
        print(f"[CHAT] ç³»ç»Ÿé»˜è®¤åŠ©æ‰‹æ¨¡å¼ - å°†ä½¿ç”¨ LangGraph Router å†³å®šæ‰§è¡Œè·¯å¾„")
    else:
        custom_agent = session.get(CustomAgent, normalized_agent_id)

        if custom_agent and custom_agent.user_id == current_user.id:
            custom_agent.conversation_count += 1
            session.add(custom_agent)
            session.commit()
            print(f"[CHAT] è‡ªå®šä¹‰æ™ºèƒ½ä½“æ¨¡å¼ - ç›´æ¥è°ƒç”¨ LLM: {custom_agent.name}")
        else:
            is_system_default = True
            custom_agent = None
            print(f"[CHAT] æœªæ‰¾åˆ°è‡ªå®šä¹‰æ™ºèƒ½ä½“ï¼Œå›é€€åˆ°ç³»ç»Ÿé»˜è®¤åŠ©æ‰‹æ¨¡å¼")

    # å¦‚æœæ˜¯è‡ªå®šä¹‰æ™ºèƒ½ä½“ï¼Œä½¿ç”¨ç›´æ¥ LLM è°ƒç”¨æ¨¡å¼
    if custom_agent:
        if request.stream:
            return await _handle_custom_agent_stream(
                custom_agent, langchain_messages, thread_id, thread
            )
        else:
            return await _handle_custom_agent_sync(
                custom_agent, langchain_messages, thread_id, thread, session
            )

    # ç³»ç»Ÿé»˜è®¤åŠ©æ‰‹æ¨¡å¼ï¼šé€šè¿‡ LangGraph å¤„ç†
    print(f"[CHAT] è¿›å…¥ç³»ç»Ÿé»˜è®¤åŠ©æ‰‹æ¨¡å¼ï¼Œä½¿ç”¨ LangGraph å¤„ç†")

    initial_state = {
        "messages": langchain_messages,
        "current_agent": "router",
        "task_list": [],
        "current_task_index": 0,
        "strategy": "",
        "expert_results": [],
        "final_response": "",
        "context": {},
        "router_decision": ""
    }

    if request.stream:
        return await _handle_langgraph_stream(
            initial_state, thread_id, thread, request.message, session
        )
    else:
        return await _handle_langgraph_sync(
            initial_state, thread_id, thread, request.message, session
        )


# ============================================================================
# è‡ªå®šä¹‰æ™ºèƒ½ä½“å¤„ç†å‡½æ•°
# ============================================================================

async def _handle_custom_agent_stream(
    custom_agent: CustomAgent,
    langchain_messages: list,
    thread_id: str,
    thread: Thread
) -> StreamingResponse:
    """å¤„ç†è‡ªå®šä¹‰æ™ºèƒ½ä½“æµå¼å“åº”"""
    async def event_generator():
        full_response = ""
        try:
            model_name = custom_agent.model_id or os.getenv("MODEL_NAME", "deepseek-chat")
            base_url = os.getenv("OPENAI_BASE_URL") or os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")
            
            # è‡ªåŠ¨ä¿®æ­£ï¼šå¦‚æœä½¿ç”¨ DeepSeek API ä½† model_id æ˜¯ OpenAI æ¨¡å‹
            if "deepseek.com" in base_url and model_name.startswith("gpt-"):
                print(f"[CUSTOM AGENT] æ£€æµ‹åˆ°ä¸å…¼å®¹æ¨¡å‹ {model_name}ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸º deepseek-chat")
                model_name = "deepseek-chat"

            print(f"[CUSTOM AGENT] ä½¿ç”¨æ¨¡å‹: {model_name}")
            
            llm = get_llm_instance(streaming=True, model=model_name, temperature=0.7)

            messages_with_system = [("system", custom_agent.system_prompt)]
            messages_with_system.extend(langchain_messages)

            async for chunk in llm.astream(messages_with_system):
                content = chunk.content
                if content:
                    full_response += content
                    yield f"data: {json.dumps({'content': content, 'conversationId': thread_id})}\n\n"

        except Exception as e:
            import traceback
            traceback.print_exc()
            error_msg = json.dumps({"error": str(e)})
            yield f"data: {error_msg}\n\n"

        # ä¿å­˜ AI å›å¤åˆ°æ•°æ®åº“
        if full_response:
            ai_msg_db = Message(
                thread_id=thread_id,
                role="assistant",
                content=full_response,
                timestamp=datetime.now()
            )
            with Session(engine) as inner_session:
                inner_session.add(ai_msg_db)
                thread = inner_session.get(Thread, thread_id)
                if thread:
                    thread.updated_at = datetime.now()
                    inner_session.add(thread)
                inner_session.commit()
        
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


async def _handle_custom_agent_sync(
    custom_agent: CustomAgent,
    langchain_messages: list,
    thread_id: str,
    thread: Thread,
    session: Session
) -> dict:
    """å¤„ç†è‡ªå®šä¹‰æ™ºèƒ½ä½“éæµå¼å“åº”"""
    model_name = custom_agent.model_id or os.getenv("MODEL_NAME", "deepseek-chat")
    base_url = os.getenv("OPENAI_BASE_URL") or os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")

    if "deepseek.com" in base_url and model_name.startswith("gpt-"):
        model_name = "deepseek-chat"

    llm = get_llm_instance(streaming=False, model=model_name, temperature=0.7)
    
    messages_with_system = [("system", custom_agent.system_prompt)]
    messages_with_system.extend(langchain_messages)
    
    result = await llm.ainvoke(messages_with_system)
    full_response = result.content

    # ä¿å­˜ AI å›å¤
    ai_msg_db = Message(
        thread_id=thread_id,
        role="assistant",
        content=full_response,
        timestamp=datetime.now()
    )
    session.add(ai_msg_db)
    thread.updated_at = datetime.now()
    session.add(thread)
    session.commit()

    return {
        "role": "assistant",
        "content": full_response,
        "conversationId": thread_id
    }


# ============================================================================
# LangGraph å¤„ç†å‡½æ•°
# ============================================================================

async def _handle_langgraph_stream(
    initial_state: dict,
    thread_id: str,
    thread: Thread,
    user_message: str,
    session: Session
) -> StreamingResponse:
    """å¤„ç† LangGraph æµå¼å“åº”"""
    async def event_generator():
        full_response = ""
        event_count = 0
        expert_artifacts = {}
        collected_task_list = []
        collected_expert_results = []
        router_mode = ""
        thinking_steps = []  # ğŸ‘ˆ æ–°å¢ï¼šæ”¶é›† thinking æ­¥éª¤

        try:
            async for event in commander_graph.astream_events(
                initial_state,
                version="v2"
            ):
                event_count += 1
                kind = event["event"]
                name = event.get("name", "")

                if event_count % 10 == 0:
                    print(f"[STREAM] å·²å¤„ç† {event_count} ä¸ªäº‹ä»¶ï¼Œå½“å‰: {kind} - {name}")

                # æ•è· Router èŠ‚ç‚¹æ‰§è¡Œç»“æŸ
                if kind == "on_chain_end" and name == "router":
                    output_data = event["data"]["output"]
                    router_decision = output_data.get("router_decision", "")

                    if router_decision:
                        print(f"[STREAM] Router å†³ç­–: {router_decision}")
                        router_mode = router_decision
                        # ğŸ‘ˆ åªè®°å½•å†³ç­–ï¼Œä¸ç«‹å³ä¿®æ”¹æ•°æ®åº“ï¼ˆé¿å…sessionç”Ÿå‘½å‘¨æœŸå†²çªï¼‰
                        # threadçŠ¶æ€å°†åœ¨streamingç»“æŸåç»Ÿä¸€æ›´æ–°
                        yield f"data: {json.dumps({'routerDecision': router_decision, 'conversationId': thread_id})}\n\n"

                # æ•è·è§„åˆ’èŠ‚ç‚¹æ‰§è¡Œç»“æŸï¼ˆplanner å¯èƒ½æ²¡æœ‰ on_chain_endï¼Œç”¨æµå¼è¾“å‡ºä¸­çš„æ ‡è®°ï¼‰
                if kind == "on_chain_end" and name == "planner":
                    output_data = event["data"]["output"]
                    print(f"[DEBUG] Planner on_chain_end triggered!")
                    print(f"[DEBUG] Planner output keys: {list(output_data.keys())}")
                    
                    if "task_list" in output_data:
                        collected_task_list = output_data["task_list"]
                        print(f"[DEBUG] Collected task_list: {len(collected_task_list)} tasks")
                        
                        # ğŸ‘ˆ æ–°å¢ï¼šæ”¶é›† planner çš„ thinking
                        thinking_steps.append({
                            "id": str(uuid4()),
                            "expertType": "planner",
                            "expertName": "ä»»åŠ¡è§„åˆ’å™¨",
                            "content": f"åˆ¶å®šæ‰§è¡Œç­–ç•¥ï¼Œæ‹†è§£ä¸º {len(collected_task_list)} ä¸ªå­ä»»åŠ¡",
                            "timestamp": datetime.now().isoformat(),
                            "status": "completed"
                        })
                    else:
                        print(f"[WARN] No task_list in planner output!")
                    
                    if "__task_plan" in output_data:
                        task_plan = output_data["__task_plan"]
                        print(f"[STREAM] å‘é€ taskPlan äº‹ä»¶: {task_plan.get('task_count', 0)} ä¸ªä»»åŠ¡")
                        yield f"data: {json.dumps({'taskPlan': task_plan, 'conversationId': thread_id})}\n\n"
                
                # ğŸ‘ˆ å¤‡é€‰ï¼šä»æµå¼è¾“å‡ºä¸­æ•è· planner çš„ JSON è¾“å‡º
                if kind == "on_chat_model_stream" and router_mode == "complex":
                    event_tags = event.get("tags", [])
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ planner/commander çš„è¾“å‡º
                    if any(tag in str(event_tags) for tag in ["planner", "commander"]):
                        content = event["data"]["chunk"].content
                        # å°è¯•è§£æ JSON æ ¼å¼çš„ task_list
                        if content and '"tasks"' in content and not collected_task_list:
                            try:
                                import json
                                data = json.loads(content)
                                if "tasks" in data and isinstance(data["tasks"], list):
                                    collected_task_list = data["tasks"]
                                    print(f"[DEBUG] ä»æµå¼è¾“å‡ºè§£æ task_list: {len(collected_task_list)} tasks")
                                    
                                    # æ”¶é›† thinking
                                    thinking_steps.append({
                                        "id": str(uuid4()),
                                        "expertType": "planner",
                                        "expertName": "ä»»åŠ¡è§„åˆ’å™¨",
                                        "content": f"åˆ¶å®šæ‰§è¡Œç­–ç•¥ï¼Œæ‹†è§£ä¸º {len(collected_task_list)} ä¸ªå­ä»»åŠ¡",
                                        "timestamp": datetime.now().isoformat(),
                                        "status": "completed"
                                    })
                                    
                                    # å‘é€ taskPlan äº‹ä»¶
                                    task_plan = {
                                        "task_count": len(collected_task_list),
                                        "tasks": collected_task_list
                                    }
                                    yield f"data: {json.dumps({'taskPlan': task_plan, 'conversationId': thread_id})}\n\n"
                            except:
                                pass  # è§£æå¤±è´¥ï¼Œå¿½ç•¥

                # æ•è· direct_reply èŠ‚ç‚¹æ‰§è¡Œç»“æŸ
                if kind == "on_chain_end" and name == "direct_reply":
                    yield f"data: {json.dumps({'content': '', 'conversationId': thread_id, 'isFinal': True})}\n\n"
                    print(f"[STREAM] Direct Reply èŠ‚ç‚¹å®Œæˆï¼ŒSimple æ¨¡å¼æµå¼è¾“å‡ºç»“æŸ")

                # æ•è·èšåˆå™¨èŠ‚ç‚¹æ‰§è¡Œç»“æŸ
                if kind == "on_chain_end" and name == "aggregator":
                    output_data = event["data"]["output"]
                    if "final_response" in output_data:
                        final_response = output_data["final_response"]
                        yield f"data: {json.dumps({'content': final_response, 'conversationId': thread_id, 'isFinal': True})}\n\n"

                # æ•è·ä¸“å®¶åˆ†å‘å™¨èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
                if kind == "on_chain_start" and name == "expert_dispatcher":
                    input_data = event.get("data", {}).get("input", {})
                    task_list = input_data.get("task_list", [])
                    current_task_index = input_data.get("current_task_index", 0)

                    if task_list and current_task_index < len(task_list):
                        current_task = task_list[current_task_index]
                        task_start_info = {
                            "task_index": current_task_index + 1,
                            "total_tasks": len(task_list),
                            "expert_type": current_task.get("expert_type", ""),
                            "description": current_task.get("description", "")
                        }
                        yield f"data: {json.dumps({'taskStart': task_start_info, 'conversationId': thread_id})}\n\n"

                # æ•è·ä¸“å®¶åˆ†å‘å™¨èŠ‚ç‚¹æ‰§è¡Œ
                if kind == "on_chain_end" and name == "expert_dispatcher":
                    output_data = event["data"]["output"]

                    if "__expert_info" in output_data:
                        expert_info = output_data["__expert_info"]
                        expert_name = expert_info.get("expert_type")
                        expert_status = expert_info.get("status", "completed")
                        duration_ms = expert_info.get("duration_ms", 0)
                        output_result = expert_info.get("output", "")
                        expert_error = expert_info.get("error")

                        if expert_name not in expert_artifacts:
                            expert_artifacts[expert_name] = []
                        
                        # ğŸ‘ˆ æ–°å¢ï¼šæ”¶é›† expert çš„ thinking
                        thinking_steps.append({
                            "id": str(uuid4()),
                            "expertType": expert_name,
                            "expertName": expert_name,
                            "content": expert_info.get("description", f"{expert_name} æ‰§è¡Œä»»åŠ¡"),
                            "timestamp": datetime.now().isoformat(),
                            "status": "completed" if expert_status == "completed" else "failed"
                        })

                        yield f"data: {json.dumps({'activeExpert': expert_name, 'conversationId': thread_id})}\n\n"

                        if "artifact" in output_data:
                            artifact = output_data["artifact"]
                            expert_artifacts[expert_name].append(artifact)
                            yield f"data: {json.dumps({'artifact': artifact, 'conversationId': thread_id, 'allArtifacts': expert_artifacts[expert_name], 'activeExpert': expert_name})}\n\n"

                        yield f"data: {json.dumps({
                            'expertCompleted': expert_name,
                            'description': expert_info.get('description', ''),
                            'conversationId': thread_id,
                            'duration_ms': duration_ms,
                            'status': expert_status,
                            'output': output_result,
                            'error': expert_error,
                            'allArtifacts': expert_artifacts.get(expert_name, [])
                        })}\n\n"

                # æ•è· LLM æµå¼è¾“å‡º
                if kind == "on_chat_model_stream":
                    event_tags = event.get("tags", [])
                    content = event["data"]["chunk"].content
                    
                    # ğŸ‘ˆ è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰ on_chat_model_stream äº‹ä»¶çš„æ ‡ç­¾
                    if event_count % 5 == 0:
                        print(f"[STREAM DEBUG] on_chat_model_stream: tags={event_tags}, name={name}, router_mode={router_mode}")

                    should_yield, reason = should_stream_event(event_tags, router_mode, name)
                    if not should_yield:
                        print(f"[STREAM] {reason}")
                        continue

                    if is_task_plan_content(content):
                        print(f"[STREAM] è·³è¿‡ä»»åŠ¡è®¡åˆ’JSONå†…å®¹: {content[:200]}...")
                        continue

                    if content:
                        print(f"[STREAM] é€šè¿‡è¿‡æ»¤çš„æµå¼è¾“å‡º: content[:50]={content[:50]}")
                        full_response += content
                        yield f"data: {json.dumps({'content': content, 'conversationId': thread_id})}\n\n"

            print(f"[STREAM] æµå¼å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {event_count} ä¸ªäº‹ä»¶")

        except Exception as e:
            print(f"[STREAM] é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            error_msg = json.dumps({"error": str(e)})
            yield f"data: {error_msg}\n\n"

        # ä¿å­˜ AI å›å¤å’Œ Artifacts åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨ç‹¬ç«‹çš„çŸ­ç”Ÿå‘½å‘¨æœŸSessionï¼‰
        if full_response:
            with Session(engine) as save_session:
                # 1. ä¿å­˜ AI æ¶ˆæ¯ï¼ˆåŒ…å« thinking æ•°æ®ï¼‰
                extra_data = None
                if thinking_steps:
                    extra_data = {
                        "thinking": thinking_steps,
                        "thinking_count": len(thinking_steps)
                    }
                    print(f"[STREAM] ä¿å­˜ thinking æ•°æ®: {len(thinking_steps)} ä¸ªæ­¥éª¤")
                
                ai_msg_db = Message(
                    thread_id=thread_id,
                    role="assistant",
                    content=full_response,
                    timestamp=datetime.now(),
                    extra_data=extra_data
                )
                save_session.add(ai_msg_db)

                # 2. æ›´æ–° thread çŠ¶æ€ï¼ˆåº”ç”¨ router å†³ç­–ï¼‰
                thread_obj = save_session.get(Thread, thread_id)
                if thread_obj:
                    thread_obj.updated_at = datetime.now()

                    # åº”ç”¨ router å†³ç­–ï¼ˆå¦‚æœåœ¨streamingè¿‡ç¨‹ä¸­æ”¶é›†åˆ°äº†ï¼‰
                    if router_mode:
                        thread_obj.thread_mode = router_mode
                        print(f"[STREAM] æ›´æ–° thread_mode ä¸º: {router_mode}")

                    # 3. å¦‚æœæ˜¯å¤æ‚æ¨¡å¼ï¼Œä¿å­˜ TaskSession å’Œ SubTask
                    print(f"[DEBUG] Checking save condition: router_mode={router_mode}, task_list_len={len(collected_task_list)}")
                    if router_mode == "complex" and collected_task_list:
                        print(f"[STREAM] ä¿å­˜å¤æ‚æ¨¡å¼æ•°æ®: {len(collected_task_list)} ä¸ªä»»åŠ¡")

                        now = datetime.now()
                        task_session = TaskSession(
                            session_id=str(uuid4()),
                            thread_id=thread_id,
                            user_query=user_message,
                            status="completed",
                            final_response=full_response,
                            created_at=now,
                            updated_at=now,
                            completed_at=now
                        )
                        save_session.add(task_session)
                        save_session.flush()

                        thread_obj.task_session_id = task_session.session_id
                        thread_obj.agent_type = "ai"
                        save_session.add(thread_obj)

                        for task in collected_task_list:
                            expert_type = task.get("expert_type", "")
                            artifacts_for_expert = expert_artifacts.get(expert_type, [])

                            subtask = SubTask(
                                id=task.get("id", str(uuid4())),
                                expert_type=expert_type,
                                task_description=task.get("description", ""),
                                input_data=task.get("input_data", {}),
                                status=task.get("status", "completed"),
                                output_result={"content": task.get("output_result", "")},
                                artifacts=artifacts_for_expert,
                                task_session_id=task_session.session_id,
                                started_at=task.get("started_at"),
                                completed_at=task.get("completed_at"),
                                created_at=task.get("created_at"),
                                updated_at=task.get("updated_at"),
                            )
                            save_session.add(subtask)
                            print(f"[STREAM] ä¿å­˜ SubTask: {expert_type}, artifacts: {len(artifacts_for_expert)}")

                    save_session.add(thread_obj)
                save_session.commit()

        yield "data: [DONE]\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


async def _handle_langgraph_sync(
    initial_state: dict,
    thread_id: str,
    thread: Thread,
    user_message: str,
    session: Session
) -> dict:
    """å¤„ç† LangGraph éæµå¼å“åº”"""
    result = await commander_graph.ainvoke(initial_state)
    last_message = result["messages"][-1]

    # è·å– Router å†³ç­–å¹¶æ›´æ–° thread_mode
    router_decision = result.get("router_decision", "simple")
    thread.thread_mode = router_decision

    # å¦‚æœæ˜¯å¤æ‚æ¨¡å¼ï¼Œè®¾ç½® agent_type ä¸º "ai"
    if router_decision == "complex":
        thread.agent_type = "ai"

        # åˆ›å»º TaskSession
        task_session = TaskSession(
            session_id=str(uuid4()),
            thread_id=thread_id,
            user_query=user_message,
            status="completed",
            final_response=last_message.content,
            created_at=datetime.now(),
            updated_at=datetime.now(),
            completed_at=datetime.now()
        )
        session.add(task_session)
        session.flush()

        # æ›´æ–° thread çš„ task_session_id
        thread.task_session_id = task_session.session_id

        # ä¿å­˜ SubTaskï¼ˆåŒ…æ‹¬ artifactsï¼‰
        for subtask in result["task_list"]:
            artifacts = subtask.get("artifact")
            if artifacts:
                artifacts = [artifacts] if isinstance(artifacts, dict) else artifacts

            db_subtask = SubTask(
                id=subtask["id"],
                expert_type=subtask["expert_type"],
                task_description=subtask["description"],
                input_data=subtask["input_data"],
                status=subtask["status"],
                output_result=subtask["output_result"],
                artifacts=artifacts,
                started_at=subtask.get("started_at"),
                completed_at=subtask.get("completed_at"),
                created_at=subtask.get("created_at"),
                updated_at=subtask.get("updated_at"),
                task_session_id=task_session.session_id
            )
            session.add(db_subtask)

    # ä¿å­˜ AI å›å¤
    ai_msg_db = Message(
        thread_id=thread_id,
        role="assistant",
        content=last_message.content,
        timestamp=datetime.now()
    )
    session.add(ai_msg_db)
    thread.updated_at = datetime.now()
    session.add(thread)
    session.commit()

    return {
        "role": "assistant",
        "content": last_message.content,
        "conversationId": thread_id,
        "threadMode": router_decision
    }
