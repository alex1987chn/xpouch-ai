"""
XPouch AI æ™ºèƒ½è·¯ç”±å·¥ä½œæµ (v3.0 æ¶æ„)
é›†æˆæ„å›¾è¯†åˆ« (Router) -> ä»»åŠ¡æŒ‡æŒ¥å®˜ (Commander) -> ä¸“å®¶æ‰§è¡Œ (Experts)
æ”¯æŒäº‹ä»¶æº¯æºæŒä¹…åŒ–å’Œ Server-Driven UI
"""
from typing import Dict, Any
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv
import pathlib

# ğŸ”¥ æ–°å¢ï¼šå¯¼å…¥ MemorySaver æ”¯æŒçŠ¶æ€ç®¡ç†
from langgraph.checkpoint.memory import MemorySaver

# å¯¼å…¥æ•°æ®æ¨¡å‹
import sys
sys.path.append(str(pathlib.Path(__file__).parent.parent))
from config import init_langchain_tracing, get_langsmith_config

# v3.1: ä» nodes æ¨¡å—å¯¼å…¥æ‰€æœ‰èŠ‚ç‚¹å‡½æ•°ï¼ˆé‡æ„åï¼‰
from agents.nodes import (
    router_node,
    direct_reply_node,
    commander_node,
    expert_dispatcher_node,
    generic_worker_node,
    aggregator_node,
)
from agents.state import AgentState

# LangSmith é“¾è·¯è¿½è¸ª
env_path = pathlib.Path(__file__).parent.parent / ".env"
load_dotenv(dotenv_path=env_path)

langsmith_config = get_langsmith_config()
if langsmith_config["enabled"]:
    init_langchain_tracing(langsmith_config)

# v3.0: å»¶è¿Ÿåˆå§‹åŒ– LLM - é¿å…æ¨¡å—åŠ è½½æ—¶å°±åˆ›å»ºå®ä¾‹
_router_llm = None
_commander_llm = None
_simple_llm = None

def get_router_llm_lazy():
    """å»¶è¿Ÿåˆå§‹åŒ– Router LLM"""
    global _router_llm
    if _router_llm is None:
        from utils.llm_factory import get_router_llm
        _router_llm = get_router_llm()
    return _router_llm

def get_commander_llm_lazy():
    """å»¶è¿Ÿåˆå§‹åŒ– Commander LLM"""
    global _commander_llm
    if _commander_llm is None:
        from utils.llm_factory import get_commander_llm
        _commander_llm = get_commander_llm()
    return _commander_llm

def get_simple_llm_lazy():
    """å»¶è¿Ÿåˆå§‹åŒ– Simple æ¨¡å¼ LLM"""
    global _simple_llm
    if _simple_llm is None:
        from providers_config import is_provider_configured
        from utils.llm_factory import get_llm_instance, get_router_llm
        try:
            if is_provider_configured('minimax'):
                _simple_llm = get_llm_instance(provider='minimax', streaming=True, temperature=0.7)
                print("[LLM] Simple æ¨¡å¼ä½¿ç”¨: MiniMax-M2.1")
            else:
                _simple_llm = get_router_llm()
                print("[LLM] Simple æ¨¡å¼å›é€€åˆ° Router LLM")
        except Exception as e:
            print(f"[LLM] Simple æ¨¡å¼åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ° Router: {e}")
            _simple_llm = get_router_llm()
    return _simple_llm


# ============================================================================
# 4. æ¡ä»¶è·¯ç”±é€»è¾‘ (Edges)
# ============================================================================

def route_router(state: AgentState) -> str:
    """Router ä¹‹åçš„å»å‘"""
    decision = state.get("router_decision", "complex")

    print(f"[ROUTE_ROUTER] å†³ç­–: {decision}, å°†è·¯ç”±åˆ°: {'direct_reply' if decision == 'simple' else 'commander'}")

    if decision == "simple":
        # Simple æ¨¡å¼è¿›å…¥ direct_reply èŠ‚ç‚¹
        return "direct_reply"
    else:
        # Complex æ¨¡å¼è¿›å…¥æŒ‡æŒ¥å®˜
        return "commander"

def route_dispatcher(state: AgentState) -> str:
    """
    å†³å®šå¾ªç¯çš„å»å‘ï¼šç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªä»»åŠ¡ æˆ– èšåˆç»“æœ

    æ³¨æ„ï¼šè¿™ä¸ªè·¯ç”±å‡½æ•°åœ¨ Generic Worker æ‰§è¡Œåè¢«è°ƒç”¨
    Generic Worker æ‰§è¡Œå®Œä»»åŠ¡åï¼Œcurrent_index ä¸ä¼šè‡ªåŠ¨å¢åŠ 
    å¢åŠ  current_index çš„é€»è¾‘åº”è¯¥åœ¨ generic_worker_node ä¸­å®ç°
    """
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)

    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä»»åŠ¡
    if current_index >= len(task_list):
        return "aggregator"  # æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå»èšåˆ

    # è¿˜æœ‰ä»»åŠ¡ï¼Œéœ€è¦å›åˆ° Dispatcher è®©å®ƒæ£€æŸ¥å¹¶åˆ†å‘
    # Dispatcher ä¼šæ£€æŸ¥ä»»åŠ¡å¹¶å†³å®šæ˜¯å¦ç»§ç»­
    return "expert_dispatcher"

# ============================================================================
# 5. æ„å»ºå·¥ä½œæµå›¾
# ============================================================================

def create_smart_router_workflow() -> StateGraph:
    workflow = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("router", router_node)
    workflow.add_node("direct_reply", direct_reply_node)  # æ–°å¢ï¼šSimple æ¨¡å¼æµå¼å›å¤
    workflow.add_node("commander", commander_node)
    workflow.add_node("expert_dispatcher", expert_dispatcher_node)
    workflow.add_node("generic", generic_worker_node)  # æ–°å¢ï¼šé€šç”¨ä¸“å®¶æ‰§è¡ŒèŠ‚ç‚¹
    workflow.add_node("aggregator", aggregator_node)

    # è®¾ç½®å…¥å£ï¼šç°åœ¨å…¥å£æ˜¯ Routerï¼
    workflow.set_entry_point("router")

    # æ·»åŠ è¿çº¿

    # 1. Router -> (Direct Reply | Commander)
    workflow.add_conditional_edges(
        "router",
        route_router,
        {
            "direct_reply": "direct_reply",
            "commander": "commander"
        }
    )

    # 2. Direct Reply -> END
    workflow.add_edge("direct_reply", END)

    # 3. Commander -> Dispatcher (æŒ‡æŒ¥å®˜å®Œæˆåæ‰§è¡Œ)
    workflow.add_edge("commander", "expert_dispatcher")

    # 4. Dispatcher -> Generic (ä¸“å®¶æ‰§è¡Œ)
    # Dispatcher æ£€æŸ¥ä¸“å®¶å­˜åœ¨åï¼Œæµè½¬åˆ° Generic æ‰§è¡Œ
    workflow.add_edge("expert_dispatcher", "generic")

    # 5. Generic -> (Dispatcher | Aggregator)
    # Generic æ‰§è¡Œå®Œä»»åŠ¡åï¼Œæ ¹æ®æ˜¯å¦è¿˜æœ‰ä»»åŠ¡å†³å®šå»å‘
    workflow.add_conditional_edges(
        "generic",
        route_dispatcher,
        {
            "expert_dispatcher": "expert_dispatcher",  # è¿˜æœ‰ä»»åŠ¡ï¼Œå›åˆ° Dispatcher æ£€æŸ¥ä¸‹ä¸€ä¸ª
            "aggregator": "aggregator"  # æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå»èšåˆç»“æœ
        }
    )

    # 4. Aggregator -> END
    workflow.add_edge("aggregator", END)

    # ---------------------------------------------------------
    # ğŸ”¥ ä¿®æ”¹å¼€å§‹ï¼šæ·»åŠ  Checkpointer
    # ---------------------------------------------------------
    # åˆå§‹åŒ–å†…å­˜æ£€æŸ¥ç‚¹
    # è¿™ä¼šè®© LangGraph æŠŠçŠ¶æ€ä¿å­˜åœ¨å†…å­˜é‡Œï¼Œä¸ä¼šé˜»å¡æ•°æ®åº“ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´ Cloudflare è¶…æ—¶
    memory = MemorySaver()

    # ç¼–è¯‘æ—¶ä¼ å…¥ checkpointer
    return workflow.compile(checkpointer=memory)
    # ---------------------------------------------------------
    # ğŸ”¥ ä¿®æ”¹ç»“æŸ

# å¯¼å‡ºç¼–è¯‘åçš„å›¾
commander_graph = create_smart_router_workflow()

# ============================================================================
# æµ‹è¯•å°è£…å‡½æ•°
# ============================================================================

async def execute_commander_workflow(user_query: str, thread_id: str = "test_thread") -> dict[str, Any]:
    print(f"--- [START] æŸ¥è¯¢: {user_query} ---")
    initial_state: AgentState = {
        "messages": [HumanMessage(content=user_query)],
        "task_list": [],
        "current_task_index": 0,
        "strategy": "",
        "expert_results": [],
        "final_response": ""
    }
    # ğŸ”¥ æ·»åŠ  config ä¼ é€’ thread_id ç»™ MemorySaver
    final_state = await commander_graph.ainvoke(
        initial_state,
        config={"configurable": {"thread_id": thread_id}}
    )
    print("--- [DONE] ---")
    return final_state

if __name__ == "__main__":
    import asyncio
    async def test():
        # æµ‹è¯• 1: ç®€å•é—²èŠ
        print("\n=== æµ‹è¯• 1: ç®€å•æ¨¡å¼ ===")
        await execute_commander_workflow("ä½ å¥½ï¼Œåœ¨å—ï¼Ÿ")
        
        # æµ‹è¯• 2: å¤æ‚ä»»åŠ¡
        print("\n=== æµ‹è¯• 2: å¤æ‚æ¨¡å¼ ===")
        await execute_commander_workflow("å¸®æˆ‘å†™ä¸€ä¸ª Python è„šæœ¬æ¥æŠ“å–è‚¡ç¥¨ä»·æ ¼ã€‚")
    
    asyncio.run(test())
