"""
XPouch AI æ™ºèƒ½è·¯ç”±å·¥ä½œæµ (v3.0 æ¶æ„)
é›†æˆæ„å›¾è¯†åˆ« (Router) -> ä»»åŠ¡æŒ‡æŒ¥å®˜ (Commander) -> ä¸“å®¶æ‰§è¡Œ (Experts)
æ”¯æŒäº‹ä»¶æº¯æºæŒä¹…åŒ–å’Œ Server-Driven UI
"""
from typing import Dict, Any
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, ToolMessage
from langgraph.prebuilt import ToolNode  # ğŸ”¥ æ–°å¢ï¼šå·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
from dotenv import load_dotenv
import pathlib

# ğŸ”¥ æ–°å¢ï¼šå¯¼å…¥ MemorySaver æ”¯æŒçŠ¶æ€ç®¡ç†
from langgraph.checkpoint.memory import MemorySaver

# å¯¼å…¥æ•°æ®æ¨¡å‹
import sys
sys.path.append(str(pathlib.Path(__file__).parent.parent))
from config import init_langchain_tracing, get_langsmith_config

# v3.1: ä» nodes æ¨¡å—å¯¼å…¥æ‰€æœ‰èŠ‚ç‚¹å‡½æ•°ï¼ˆé‡æ„åï¼‰
from agents.nodes import (
    router_node,
    direct_reply_node,
    commander_node,
    expert_dispatcher_node,
    generic_worker_node,
    aggregator_node,
)
from agents.state import AgentState
from tools import ALL_TOOLS  # ğŸ”¥ æ–°å¢ï¼šå¯¼å…¥å·¥å…·é›†

# LangSmith é“¾è·¯è¿½è¸ª
env_path = pathlib.Path(__file__).parent.parent / ".env"
load_dotenv(dotenv_path=env_path)

langsmith_config = get_langsmith_config()
if langsmith_config["enabled"]:
    init_langchain_tracing(langsmith_config)

# v3.0: å»¶è¿Ÿåˆå§‹åŒ– LLM - é¿å…æ¨¡å—åŠ è½½æ—¶å°±åˆ›å»ºå®ä¾‹
_router_llm = None
_commander_llm = None
_simple_llm = None

def get_router_llm_lazy():
    """å»¶è¿Ÿåˆå§‹åŒ– Router LLM"""
    global _router_llm
    if _router_llm is None:
        from utils.llm_factory import get_router_llm
        _router_llm = get_router_llm()
    return _router_llm

def get_commander_llm_lazy():
    """å»¶è¿Ÿåˆå§‹åŒ– Commander LLM"""
    global _commander_llm
    if _commander_llm is None:
        from utils.llm_factory import get_commander_llm
        _commander_llm = get_commander_llm()
    return _commander_llm

def get_simple_llm_lazy():
    """å»¶è¿Ÿåˆå§‹åŒ– Simple æ¨¡å¼ LLM"""
    global _simple_llm
    if _simple_llm is None:
        from providers_config import is_provider_configured
        from utils.llm_factory import get_llm_instance, get_router_llm
        try:
            if is_provider_configured('minimax'):
                _simple_llm = get_llm_instance(provider='minimax', streaming=True, temperature=0.7)
                print("[LLM] Simple æ¨¡å¼ä½¿ç”¨: MiniMax-M2.1")
            else:
                _simple_llm = get_router_llm()
                print("[LLM] Simple æ¨¡å¼å›é€€åˆ° Router LLM")
        except Exception as e:
            print(f"[LLM] Simple æ¨¡å¼åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ° Router: {e}")
            _simple_llm = get_router_llm()
    return _simple_llm


# ============================================================================
# 4. æ¡ä»¶è·¯ç”±é€»è¾‘ (Edges)
# ============================================================================

def route_router(state: AgentState) -> str:
    """Router ä¹‹åçš„å»å‘"""
    decision = state.get("router_decision", "complex")

    print(f"[ROUTE_ROUTER] å†³ç­–: {decision}, å°†è·¯ç”±åˆ°: {'direct_reply' if decision == 'simple' else 'commander'}")

    if decision == "simple":
        # Simple æ¨¡å¼è¿›å…¥ direct_reply èŠ‚ç‚¹
        return "direct_reply"
    else:
        # Complex æ¨¡å¼è¿›å…¥æŒ‡æŒ¥å®˜
        return "commander"

def route_dispatcher(state: AgentState) -> str:
    """
    å†³å®šå¾ªç¯çš„å»å‘ï¼šç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªä»»åŠ¡ æˆ– èšåˆç»“æœ

    æ³¨æ„ï¼šè¿™ä¸ªè·¯ç”±å‡½æ•°åœ¨ Generic Worker æ‰§è¡Œåè¢«è°ƒç”¨
    Generic Worker æ‰§è¡Œå®Œä»»åŠ¡åï¼Œcurrent_index ä¸ä¼šè‡ªåŠ¨å¢åŠ 
    å¢åŠ  current_index çš„é€»è¾‘åº”è¯¥åœ¨ generic_worker_node ä¸­å®ç°
    """
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)

    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä»»åŠ¡
    if current_index >= len(task_list):
        return "aggregator"  # æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå»èšåˆ

    # è¿˜æœ‰ä»»åŠ¡ï¼Œéœ€è¦å›åˆ° Dispatcher è®©å®ƒæ£€æŸ¥å¹¶åˆ†å‘
    # Dispatcher ä¼šæ£€æŸ¥ä»»åŠ¡å¹¶å†³å®šæ˜¯å¦ç»§ç»­
    return "expert_dispatcher"


def route_generic(state: AgentState) -> str:
    """
    Generic Worker ä¹‹åçš„æ¡ä»¶è·¯ç”±

    1. æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨è¯·æ±‚ï¼Œå¦‚æœæœ‰åˆ™æ‰§è¡Œå·¥å…·
    2. å¦‚æœå·¥å…·æ‰§è¡Œå®Œæˆï¼ˆæœ€åä¸€æ¡æ˜¯ ToolMessageï¼‰ï¼Œå›åˆ° Generic ç»§ç»­å¤„ç†
    3. å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ
    """
    from langchain_core.messages import ToolMessage, AIMessage

    messages = state.get("messages", [])
    current_index = state.get("current_task_index", 0)
    task_list = state.get("task_list", [])

    if not messages:
        return route_dispatcher(state)

    # ğŸ”¥ğŸ”¥ğŸ”¥ ç†”æ–­æœºåˆ¶ (Circuit Breaker) ğŸ”¥ğŸ”¥ğŸ”¥
    # æ£€æŸ¥æœ€è¿‘çš„ ToolMessage æ•°é‡ï¼Œé˜²æ­¢æ— é™å¾ªç¯
    recent_tool_count = sum(1 for msg in messages[-10:] if isinstance(msg, ToolMessage))
    if recent_tool_count >= 5:
        print(f"[ROUTE_GENERIC] ğŸ›‘ ç†”æ–­è§¦å‘ï¼šæœ€è¿‘å·²æ‰§è¡Œ {recent_tool_count} æ¬¡å·¥å…·ï¼Œå¼ºåˆ¶ç»“æŸä»»åŠ¡ï¼")
        return "aggregator"

    # ğŸ”¥ æƒ…å†µ1ï¼šLLM è¿”å›äº† tool_callsï¼Œéœ€è¦æ‰§è¡Œå·¥å…·
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"

    # ğŸ”¥ æƒ…å†µ2ï¼šæœ€åä¸€æ¡æ˜¯ ToolMessageï¼Œè¯´æ˜å·¥å…·åˆšæ‰§è¡Œå®Œ
    # éœ€è¦å›åˆ° Generic è®© LLM å¤„ç†å·¥å…·ç»“æœ
    if isinstance(last_message, ToolMessage):
        return "generic"

    # ğŸ”¥ æƒ…å†µ3ï¼šæ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ
    # å¦‚æœ current_index >= len(task_list)ï¼Œè¯´æ˜æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ
    if current_index >= len(task_list):
        return "aggregator"

    # æƒ…å†µ4ï¼šè¿˜æœ‰ä»»åŠ¡ï¼Œç»§ç»­æ‰§è¡Œ
    return route_dispatcher(state)

# ============================================================================
# 5. æ„å»ºå·¥ä½œæµå›¾
# ============================================================================

def create_smart_router_workflow() -> StateGraph:
    workflow = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("router", router_node)
    workflow.add_node("direct_reply", direct_reply_node)  # æ–°å¢ï¼šSimple æ¨¡å¼æµå¼å›å¤
    workflow.add_node("commander", commander_node)
    workflow.add_node("expert_dispatcher", expert_dispatcher_node)
    workflow.add_node("generic", generic_worker_node)  # æ–°å¢ï¼šé€šç”¨ä¸“å®¶æ‰§è¡ŒèŠ‚ç‚¹
    workflow.add_node("aggregator", aggregator_node)

    # ğŸ”¥ æ–°å¢ï¼šå·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
    tool_node = ToolNode(ALL_TOOLS)
    workflow.add_node("tools", tool_node)
    print(f"[WORKFLOW] âœ… å·²æ³¨å†Œå·¥å…·èŠ‚ç‚¹ï¼ŒåŒ…å« {len(ALL_TOOLS)} ä¸ªå·¥å…·: {[t.name for t in ALL_TOOLS]}")

    # è®¾ç½®å…¥å£ï¼šç°åœ¨å…¥å£æ˜¯ Routerï¼
    workflow.set_entry_point("router")

    # æ·»åŠ è¿çº¿

    # 1. Router -> (Direct Reply | Commander)
    workflow.add_conditional_edges(
        "router",
        route_router,
        {
            "direct_reply": "direct_reply",
            "commander": "commander"
        }
    )

    # 2. Direct Reply -> END
    workflow.add_edge("direct_reply", END)

    # 3. Commander -> Dispatcher (æŒ‡æŒ¥å®˜å®Œæˆåæ‰§è¡Œ)
    workflow.add_edge("commander", "expert_dispatcher")

    # 4. Dispatcher -> Generic (ä¸“å®¶æ‰§è¡Œ)
    # Dispatcher æ£€æŸ¥ä¸“å®¶å­˜åœ¨åï¼Œæµè½¬åˆ° Generic æ‰§è¡Œ
    workflow.add_edge("expert_dispatcher", "generic")

    # 5. Generic -> (Tools | Generic | Dispatcher | Aggregator)
    # Generic æ‰§è¡Œä»»åŠ¡åï¼Œæ ¹æ®æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨è¯·æ±‚æˆ–ä»»åŠ¡çŠ¶æ€å†³å®šå»å‘
    workflow.add_conditional_edges(
        "generic",
        route_generic,
        {
            "tools": "tools",  # æœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·
            "generic": "generic",  # å·¥å…·æ‰§è¡Œå®Œï¼Œå›åˆ° Generic å¤„ç†ç»“æœ
            "expert_dispatcher": "expert_dispatcher",  # ç»§ç»­ä¸‹ä¸€ä¸ªä»»åŠ¡
            "aggregator": "aggregator"  # æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå»èšåˆç»“æœ
        }
    )

    # 6. Tools -> Generic (å·¥å…·æ‰§è¡Œå®Œï¼Œå›åˆ° Generic ç»§ç»­å¤„ç†)
    # å·¥å…·æ‰§è¡Œå®Œåï¼ŒLLM ä¼šç»§ç»­å“åº”ï¼Œå¯èƒ½å†æ¬¡è°ƒç”¨å·¥å…·æˆ–å®Œæˆä»»åŠ¡
    workflow.add_edge("tools", "generic")

    # 7. Aggregator -> END
    workflow.add_edge("aggregator", END)

    # ---------------------------------------------------------
    # ğŸ”¥ ä¿®æ”¹å¼€å§‹ï¼šæ·»åŠ  Checkpointer
    # ---------------------------------------------------------
    # åˆå§‹åŒ–å†…å­˜æ£€æŸ¥ç‚¹
    # è¿™ä¼šè®© LangGraph æŠŠçŠ¶æ€ä¿å­˜åœ¨å†…å­˜é‡Œï¼Œä¸ä¼šé˜»å¡æ•°æ®åº“ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´ Cloudflare è¶…æ—¶
    memory = MemorySaver()

    # ç¼–è¯‘æ—¶ä¼ å…¥ checkpointer
    compiled_workflow = workflow.compile(checkpointer=memory)
    # ---------------------------------------------------------
    # ğŸ”¥ ä¿®æ”¹ç»“æŸ

    return compiled_workflow

# å¯¼å‡ºç¼–è¯‘åçš„å›¾
commander_graph = create_smart_router_workflow()

# ============================================================================
# æµ‹è¯•å°è£…å‡½æ•°
# ============================================================================

async def execute_commander_workflow(user_query: str, thread_id: str = "test_thread") -> dict[str, Any]:
    print(f"--- [START] æŸ¥è¯¢: {user_query} ---")
    initial_state: AgentState = {
        "messages": [HumanMessage(content=user_query)],
        "task_list": [],
        "current_task_index": 0,
        "strategy": "",
        "expert_results": [],
        "final_response": ""
    }
    # ğŸ”¥ æ·»åŠ  config ä¼ é€’ thread_id ç»™ MemorySaverï¼Œå¹¶è®¾ç½®é€’å½’é™åˆ¶
    # æ³¨æ„ï¼šrecursion_limit å¿…é¡»åœ¨ config é¡¶å±‚ï¼Œä¸èƒ½åœ¨ configurable ä¸­
    final_state = await commander_graph.ainvoke(
        initial_state,
        config={
            "recursion_limit": 100,  # ğŸ”¥ è®¾ç½®é€’å½’é™åˆ¶ï¼ˆæ”¾åœ¨é¡¶å±‚ï¼ï¼‰
            "configurable": {
                "thread_id": thread_id
            }
        }
    )
    print("--- [DONE] ---")
    return final_state

if __name__ == "__main__":
    import asyncio
    async def test():
        # æµ‹è¯• 1: ç®€å•é—²èŠ
        print("\n=== æµ‹è¯• 1: ç®€å•æ¨¡å¼ ===")
        await execute_commander_workflow("ä½ å¥½ï¼Œåœ¨å—ï¼Ÿ")
        
        # æµ‹è¯• 2: å¤æ‚ä»»åŠ¡
        print("\n=== æµ‹è¯• 2: å¤æ‚æ¨¡å¼ ===")
        await execute_commander_workflow("å¸®æˆ‘å†™ä¸€ä¸ª Python è„šæœ¬æ¥æŠ“å–è‚¡ç¥¨ä»·æ ¼ã€‚")
    
    asyncio.run(test())
