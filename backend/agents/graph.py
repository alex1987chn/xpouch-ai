"""
XPouch AI æ™ºèƒ½è·¯ç”±å·¥ä½œæµ (v3.0 æ¶æ„)
é›†æˆæ„å›¾è¯†åˆ« (Router) -> ä»»åŠ¡æŒ‡æŒ¥å®˜ (Commander) -> ä¸“å®¶æ‰§è¡Œ (Experts)
æ”¯æŒäº‹ä»¶æº¯æºæŒä¹…åŒ–å’Œ Server-Driven UI
"""
from typing import TypedDict, Annotated, List, Dict, Any, Literal, Optional, AsyncGenerator
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser
import os
from dotenv import load_dotenv
import pathlib
from pydantic import BaseModel, Field, field_validator
from uuid import uuid4
from datetime import datetime

# å¯¼å…¥æ•°æ®æ¨¡å‹
import sys
sys.path.append(str(pathlib.Path(__file__).parent.parent))
from models import ExpertType, TaskStatus, SubTask, TaskSession, Message as MessageModel
from config import init_langchain_tracing, get_langsmith_config
from utils.json_parser import parse_llm_json
from utils.exceptions import AppError
from utils.event_generator import (
    EventGenerator,
    event_plan_created, event_task_started, event_task_completed, event_task_failed,
    event_artifact_generated, event_message_delta, event_message_done,
    sse_event_to_string
)
from crud.task_session import (
    create_task_session_with_subtasks,
    update_subtask_status,
    create_artifacts_batch,
    update_task_session_status,
    get_task_session_by_thread
)
from constants import COMMANDER_SYSTEM_PROMPT, COMMANDER_SYSTEM_PROMPT_TEMPLATE, ROUTER_SYSTEM_PROMPT, DEFAULT_ASSISTANT_PROMPT 
from agents.dynamic_experts import DYNAMIC_EXPERT_FUNCTIONS, initialize_expert_cache, get_all_expert_list, format_expert_list_for_prompt, get_expert_function
from agents.expert_loader import get_expert_config_cached

# ============================================================================
# 0. è®¾ç½®ä¸é…ç½®
# ============================================================================
# ä»å·¥å‚å‡½æ•°å¯¼å…¥ LLM å®ä¾‹åˆ›å»ºå™¨
from utils.llm_factory import get_router_llm, get_commander_llm, get_llm_instance, get_expert_llm, get_aggregator_llm

# LangSmith é“¾è·¯è¿½è¸ª
env_path = pathlib.Path(__file__).parent.parent / ".env"
load_dotenv(dotenv_path=env_path)

langsmith_config = get_langsmith_config()
if langsmith_config["enabled"]:
    init_langchain_tracing(langsmith_config)

# v3.0: å»¶è¿Ÿåˆå§‹åŒ– LLM - é¿å…æ¨¡å—åŠ è½½æ—¶å°±åˆ›å»ºå®ä¾‹
_router_llm = None
_commander_llm = None
_simple_llm = None

def get_router_llm_lazy():
    """å»¶è¿Ÿåˆå§‹åŒ– Router LLM"""
    global _router_llm
    if _router_llm is None:
        _router_llm = get_router_llm()
    return _router_llm

def get_commander_llm_lazy():
    """å»¶è¿Ÿåˆå§‹åŒ– Commander LLM"""
    global _commander_llm
    if _commander_llm is None:
        _commander_llm = get_commander_llm()
    return _commander_llm

def get_simple_llm_lazy():
    """å»¶è¿Ÿåˆå§‹åŒ– Simple æ¨¡å¼ LLM"""
    global _simple_llm
    if _simple_llm is None:
        from providers_config import is_provider_configured
        try:
            if is_provider_configured('minimax'):
                _simple_llm = get_llm_instance(provider='minimax', streaming=True, temperature=0.7)
                print("[LLM] Simple æ¨¡å¼ä½¿ç”¨: MiniMax-M2.1")
            else:
                _simple_llm = get_router_llm_lazy()
                print("[LLM] Simple æ¨¡å¼å›é€€åˆ° Router LLM")
        except Exception as e:
            print(f"[LLM] Simple æ¨¡å¼åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ° Router: {e}")
            _simple_llm = get_router_llm_lazy()
    return _simple_llm

# ä¸ºäº†ä¿æŒå‘åå…¼å®¹ï¼Œä¿ç•™å…¨å±€å˜é‡åä½œä¸ºå‡½æ•°åˆ«å
router_llm = None  # æ ‡è®°ä¸ºåºŸå¼ƒï¼Œä½¿ç”¨ get_router_llm_lazy()
commander_llm = None
simple_llm = None

# å…¨å±€äº‹ä»¶ç”Ÿæˆå™¨ï¼ˆç”¨äºç”Ÿæˆ SSE äº‹ä»¶ï¼‰
event_gen = EventGenerator()

# ============================================================================
# 1. ç»“æ„å®šä¹‰ä¸æç¤ºè¯ (æ–°çš„ Router é€»è¾‘)
# ============================================================================
# ROUTER_SYSTEM_PROMPT å·²ä» constants.py å¯¼å…¥

class RoutingDecision(BaseModel):
    """v2.7 ç½‘å…³å†³ç­–ç»“æ„ï¼ˆRouteråªè´Ÿè´£åˆ†ç±»ï¼‰"""
    decision_type: Literal["simple", "complex"] = Field(description="å†³ç­–ç±»å‹")

# --- ä¿ç•™åŸæœ‰çš„æŒ‡æŒ¥å®˜ç»“æ„ ---

class SubTaskOutput(BaseModel):
    """å•ä¸ªå­ä»»åŠ¡ç»“æ„ (Commander ä½¿ç”¨)
    
    æ”¯æŒæ˜¾å¼ä¾èµ–å…³ç³» (DAG)ï¼Œé€šè¿‡ id å’Œ depends_on å®ç°ç²¾å‡†æ•°æ®ç®¡é“
    """
    id: str = Field(default="", description="ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆçŸ­IDï¼Œå¦‚ task_1, task_2ï¼‰")
    expert_type: str = Field(description="æ‰§è¡Œæ­¤ä»»åŠ¡çš„ä¸“å®¶ç±»å‹ï¼ˆå¯ä»¥æ˜¯ç³»ç»Ÿå†…ç½®ä¸“å®¶æˆ–è‡ªå®šä¹‰ä¸“å®¶ï¼‰")
    description: str = Field(description="ä»»åŠ¡æè¿°")
    input_data: Dict[str, Any] = Field(default={}, description="è¾“å…¥å‚æ•°")
    priority: int = Field(default=0, description="ä¼˜å…ˆçº§ (0=æœ€é«˜)")
    depends_on: List[str] = Field(default=[], description="ä¾èµ–çš„ä»»åŠ¡IDåˆ—è¡¨ã€‚å¦‚æœä»»åŠ¡Béœ€è¦ä»»åŠ¡Açš„è¾“å‡ºï¼Œåˆ™å¡«å…¥ ['task_a']")
    
    @field_validator('depends_on', mode='before')
    @classmethod
    def parse_depends_on(cls, v):
        """
        å…¼å®¹å¤„ç†ï¼šå¦‚æœ LLM è¿”å›äº†æ•´æ•°ä¾èµ–ï¼ˆå¦‚ [0]ï¼‰ï¼Œå¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸² ["0"]
        """
        if v is None:
            return []
        
        # æƒ…å†µ 1: LLM å‘ç–¯è¿”äº†ä¸ªå•ä¸ª int/str (ä¸æ˜¯åˆ—è¡¨)
        if isinstance(v, (int, str)):
            return [str(v)]
            
        # æƒ…å†µ 2: æ­£å¸¸çš„åˆ—è¡¨ï¼Œä½†é‡Œé¢æ··äº† int
        if isinstance(v, list):
            return [str(item) for item in v]
            
        return v

class CommanderOutput(BaseModel):
    """æŒ‡æŒ¥å®˜è¾“å‡º - å­ä»»åŠ¡åˆ—è¡¨"""
    tasks: List[SubTaskOutput] = Field(description="å­ä»»åŠ¡åˆ—è¡¨")
    strategy: str = Field(description="æ‰§è¡Œç­–ç•¥æ¦‚è¿°")
    estimated_steps: int = Field(description="é¢„è®¡æ­¥éª¤æ•°")

# ============================================================================
# 2. çŠ¶æ€å®šä¹‰
# ============================================================================

class AgentState(TypedDict):
    """è¶…æ™ºèƒ½ä½“çš„å…¨å±€çŠ¶æ€"""
    messages: Annotated[List[BaseMessage], add_messages]
    task_list: List[Dict[str, Any]]
    current_task_index: int
    strategy: str
    expert_results: List[Dict[str, Any]]
    final_response: str
    # è®°å½•è·¯ç”±å†³ç­–ä¿¡æ¯
    router_decision: str
    # v3.0 æ–°å¢ï¼šæ•°æ®åº“æŒä¹…åŒ–ç›¸å…³
    thread_id: Optional[str]           # å…³è”çš„å¯¹è¯ID
    task_session_id: Optional[str]     # ä»»åŠ¡ä¼šè¯ID
    db_session: Optional[Any]          # æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äºèŠ‚ç‚¹å†…æŒä¹…åŒ–ï¼‰
    # v3.0 æ–°å¢ï¼šäº‹ä»¶é˜Ÿåˆ—ï¼ˆç”¨äº SSE æ¨é€ï¼‰
    event_queue: List[Dict[str, Any]]  # å¾…å‘é€çš„äº‹ä»¶åˆ—è¡¨ 

# ============================================================================
# 3. èŠ‚ç‚¹å®ç°
# ============================================================================

# --- æ–°å¢ï¼šRouter èŠ‚ç‚¹ (ç½‘å…³) ---
async def router_node(state: AgentState) -> Dict[str, Any]:
    """[ç½‘å…³] åªè´Ÿè´£åˆ†ç±»ï¼Œä¸è´Ÿè´£å›ç­”"""
    messages = state["messages"]

    # æ–­ç‚¹æ¢å¤æ£€æŸ¥
    if state.get("task_list") and len(state.get("task_list", [])) > 0:
        return {"router_decision": "complex"}

    parser = PydanticOutputParser(pydantic_object=RoutingDecision)
    try:
        # ğŸ”¥ å…³é”®ï¼šé™æ€ SystemPrompt + åŠ¨æ€ Messages
        response = await get_router_llm_lazy().ainvoke(
            [
                SystemMessage(content=ROUTER_SYSTEM_PROMPT),
                *messages  # ç”¨æˆ·çš„è¾“å…¥åœ¨è¿™é‡Œ
            ],
            config={"tags": ["router"]}
        )
        decision = parser.parse(response.content)
        return {"router_decision": decision.decision_type}
    except Exception as e:
        print(f"[ROUTER ERROR] {e}")
        return {"router_decision": "complex"}

# --- æ–°å¢ï¼šDirect Reply èŠ‚ç‚¹ (Simple æ¨¡å¼æµå¼å›ç­”) ---
async def direct_reply_node(state: AgentState) -> Dict[str, Any]:
    """[ç›´è¿èŠ‚ç‚¹] è´Ÿè´£ Simple æ¨¡å¼ä¸‹çš„æµå¼å›å¤"""
    print(f"[DIRECT_REPLY] èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    messages = state["messages"]

    # ä½¿ç”¨æµå¼é…ç½®ï¼Œæ·»åŠ  metadata ä¾¿äºè¿½è¸ª
    config = {"tags": ["direct_reply"], "metadata": {"node_type": "direct_reply"}}
    
    # Simple æ¨¡å¼ä½¿ç”¨ MiniMaxï¼ˆå“åº”æœ€å¿«ï¼‰
    response = await get_simple_llm_lazy().ainvoke(
        [
            SystemMessage(content=DEFAULT_ASSISTANT_PROMPT),
            *messages  # ç”¨æˆ·çš„å†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡
        ],
        config=config
    )

    print(f"[DIRECT_REPLY] èŠ‚ç‚¹å®Œæˆï¼Œå›å¤é•¿åº¦: {len(response.content)}")

    # ç›´æ¥è¿”å› response å¯¹è±¡ï¼ˆä¿ç•™å®Œæ•´å…ƒæ•°æ®ï¼‰ï¼Œå¹¶æ·»åŠ  final_response å­—æ®µ
    return {
        "messages": [response],
        "final_response": response.content
    }

# --- æŒ‡æŒ¥å®˜èŠ‚ç‚¹ ---
async def commander_node(state: AgentState) -> Dict[str, Any]:
    """
    [æŒ‡æŒ¥å®˜] å°†å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡ã€‚
    v3.0 æ›´æ–°ï¼šç«‹å³æŒä¹…åŒ–åˆ°æ•°æ®åº“ï¼Œå‘é€ plan.created äº‹ä»¶
    """
    messages = state["messages"]
    last_message = messages[-1]
    user_query = last_message.content if isinstance(last_message, HumanMessage) else str(last_message.content)
    
    # è·å–æ•°æ®åº“ä¼šè¯å’Œ thread_id
    db_session = state.get("db_session")
    thread_id = state.get("thread_id")
    
    # åŠ è½½é…ç½® (æ•°æ®åº“æˆ–å›é€€)
    # v3.0: ä¼˜å…ˆä»æ•°æ®åº“ç›´æ¥è¯»å–ï¼Œç¡®ä¿è·å–æœ€æ–°é…ç½®ï¼ˆåŒ…æ‹¬åŠ¨æ€å ä½ç¬¦ï¼‰
    commander_config = None
    if db_session:
        from agents.expert_loader import get_expert_config
        commander_config = get_expert_config("commander", db_session)
        if commander_config:
            print(f"[COMMANDER] ä»æ•°æ®åº“ç›´æ¥åŠ è½½é…ç½®: model={commander_config['model']}")
    
    # å¦‚æœæ•°æ®åº“è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°ç¼“å­˜
    if not commander_config:
        commander_config = get_expert_config_cached("commander")
    
    if not commander_config:
        # å›é€€ï¼šä½¿ç”¨å¸¸é‡ä¸­çš„ Prompt å’Œç¡¬ç¼–ç çš„æ¨¡å‹
        system_prompt = COMMANDER_SYSTEM_PROMPT
        model = os.getenv("MODEL_NAME", "deepseek-chat")
        temperature = 0.5
        print(f"[COMMANDER] ä½¿ç”¨é»˜è®¤å›é€€é…ç½®: model={model}")
    else:
        # ä½¿ç”¨æ•°æ®åº“é…ç½®
        system_prompt = commander_config["system_prompt"]
        model = commander_config["model"]
        temperature = commander_config["temperature"]
        print(f"[COMMANDER] åŠ è½½é…ç½®: model={model}, temperature={temperature}")
    
    # æ³¨å…¥åŠ¨æ€ä¸“å®¶åˆ—è¡¨åˆ° System Prompt
    try:
        # è·å–æ‰€æœ‰å¯ç”¨ä¸“å®¶ï¼ˆåŒ…æ‹¬åŠ¨æ€åˆ›å»ºçš„ä¸“å®¶ï¼‰
        all_experts = get_all_expert_list(db_session)
        expert_list_str = format_expert_list_for_prompt(all_experts)
        
        # å°è¯•æ³¨å…¥ä¸“å®¶åˆ—è¡¨åˆ° Promptï¼ˆå¦‚æœ Prompt æ”¯æŒåŠ¨æ€å ä½ç¬¦ï¼‰
        if "{dynamic_expert_list}" in system_prompt:
            system_prompt = system_prompt.format(dynamic_expert_list=expert_list_str)
            print(f"[COMMANDER] å·²æ³¨å…¥åŠ¨æ€ä¸“å®¶åˆ—è¡¨ï¼Œå…± {len(all_experts)} ä¸ªä¸“å®¶")
        else:
            # å¦‚æœ Prompt ä¸åŒ…å«å ä½ç¬¦ï¼Œä¿ç•™åŸæœ‰é€»è¾‘ï¼ˆå‘åå…¼å®¹ï¼‰
            print(f"[COMMANDER] Prompt ä¸åŒ…å«åŠ¨æ€å ä½ç¬¦ï¼Œè·³è¿‡ä¸“å®¶åˆ—è¡¨æ³¨å…¥")
    except Exception as e:
        # æ³¨å…¥å¤±è´¥æ—¶ä¸ä¸­æ–­æµç¨‹ï¼Œä¿ç•™åŸå§‹ Prompt
        print(f"[COMMANDER] ä¸“å®¶åˆ—è¡¨æ³¨å…¥å¤±è´¥ï¼ˆå·²å¿½ç•¥ï¼‰: {e}")
    
    # æ‰§è¡Œ LLM è¿›è¡Œè§„åˆ’
    try:
        # ä»æ¨¡å‹åç§°æ¨æ–­ provider
        from providers_config import get_model_config
        from utils.llm_factory import get_llm_instance

        model_config = get_model_config(model)

        if model_config and 'provider' in model_config:
            # ä½¿ç”¨æ¨æ–­å‡ºçš„ provider åˆ›å»º LLM
            provider = model_config['provider']
            # ä¼˜å…ˆä½¿ç”¨æ¨¡å‹é…ç½®ä¸­çš„ temperatureï¼ˆå¦‚æœæœ‰ï¼‰
            final_temperature = model_config.get('temperature', temperature)
            # è·å–å®é™…çš„ API æ¨¡å‹åç§°ï¼ˆproviders.yaml ä¸­å®šä¹‰çš„ model å­—æ®µï¼‰
            actual_model = model_config.get('model', model)
            llm = get_llm_instance(
                provider=provider,
                streaming=True,
                temperature=final_temperature
            )
            print(f"[COMMANDER] æ¨¡å‹ '{model}' -> '{actual_model}' ä½¿ç”¨ provider: {provider}, temperature: {final_temperature}")
            llm_with_config = llm.bind(model=actual_model, temperature=final_temperature)
        else:
            # å›é€€åˆ° commander_llmï¼ˆç¡¬ç¼–ç çš„ provider ä¼˜å…ˆçº§ï¼‰
            print(f"[COMMANDER] æ¨¡å‹ '{model}' æœªæ‰¾åˆ° provider é…ç½®ï¼Œå›é€€åˆ° commander_llm")
            llm_with_config = get_commander_llm_lazy().bind(model=model, temperature=temperature)

        from langchain_core.runnables import RunnableConfig
        response = await llm_with_config.ainvoke(
            [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"ç”¨æˆ·æŸ¥è¯¢: {user_query}\n\nè¯·å°†æ­¤æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡åˆ—è¡¨ã€‚")
            ],
            config=RunnableConfig(
                tags=["commander"],
                metadata={"node_type": "commander"}
            )
        )

        # è§£æ JSON
        commander_response = parse_llm_json(
            response.content,
            CommanderOutput,
            strict=False,
            clean_markdown=True
        )

        # v3.1: å…œåº•å¤„ç† - å¦‚æœ LLM æ²¡æœ‰ç”Ÿæˆ idï¼Œè‡ªåŠ¨ç”Ÿæˆ
        for idx, task in enumerate(commander_response.tasks):
            if not task.id:
                task.id = f"task_{idx}"
                print(f"[COMMANDER] è‡ªåŠ¨ä¸ºä»»åŠ¡ {idx} ç”Ÿæˆ id: {task.id}")
        
        # v3.2: ä¿®å¤ä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥ - å°† depends_on ä¸­çš„ç´¢å¼•æ ¼å¼è½¬æ¢ä¸º ID æ ¼å¼
        # LLM å¯èƒ½ç”Ÿæˆ ["0"]ï¼ˆç´¢å¼•ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸º ["task_0"]ï¼ˆIDï¼‰
        task_id_map = {str(idx): task.id for idx, task in enumerate(commander_response.tasks)}
        for task in commander_response.tasks:
            if task.depends_on:
                new_depends_on = []
                for dep in task.depends_on:
                    # å¦‚æœæ˜¯æ•°å­—ç´¢å¼•ï¼ˆå¦‚ "0"ï¼‰ï¼Œè½¬æ¢ä¸ºå¯¹åº”çš„ IDï¼ˆå¦‚ "task_0"ï¼‰
                    if dep in task_id_map:
                        new_depends_on.append(task_id_map[dep])
                    else:
                        # å¦‚æœå·²ç»æ˜¯æ­£ç¡®çš„ ID æ ¼å¼ï¼ˆå¦‚ "task_0"ï¼‰ï¼Œä¿æŒä¸å˜
                        new_depends_on.append(dep)
                task.depends_on = new_depends_on
                print(f"[COMMANDER] ä»»åŠ¡ {task.id} çš„ä¾èµ–å·²è½¬æ¢: {new_depends_on}")

        # v3.0: å‡†å¤‡å­ä»»åŠ¡æ•°æ®ï¼ˆæ”¯æŒæ˜¾å¼ä¾èµ–å…³ç³» DAGï¼‰
        from models import SubTaskCreate
        subtasks_data = [
            SubTaskCreate(
                expert_type=task.expert_type,
                task_description=task.description,
                input_data=task.input_data,
                sort_order=idx,
                execution_mode="sequential",  # é»˜è®¤ä¸²è¡Œï¼Œå¯æ‰©å±•ä¸ºå¹¶è¡Œ
                depends_on=task.depends_on if task.depends_on else None
            )
            for idx, task in enumerate(commander_response.tasks)
        ]
        
        # å»ºç«‹ task_id -> database_id çš„æ˜ å°„ï¼ˆç”¨äºåç»­ä¾èµ–æ³¨å…¥ï¼‰
        task_id_mapping = {
            task.id: idx for idx, task in enumerate(commander_response.tasks)
        }

        # v3.0: ç«‹å³æŒä¹…åŒ–åˆ°æ•°æ®åº“
        task_session = None
        if db_session and thread_id:
            # v3.1: å…ˆæ£€æŸ¥ Router æ˜¯å¦å·²åˆ›å»º TaskSessionï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
            existing_session = get_task_session_by_thread(db_session, thread_id)
            if existing_session:
                task_session = existing_session
                print(f"[COMMANDER] å¤ç”¨ Router åˆ›å»ºçš„ TaskSession: {task_session.session_id}")
                # æ›´æ–°å·²æœ‰ session çš„ä¿¡æ¯
                task_session.plan_summary = commander_response.strategy
                task_session.estimated_steps = commander_response.estimated_steps
                task_session.execution_mode = "sequential"
                db_session.add(task_session)
                # åˆ›å»º SubTask å¹¶å…³è”åˆ°å·²æœ‰ session
                from crud.task_session import create_subtask
                for subtask_data in subtasks_data:
                    create_subtask(
                        db=db_session,
                        task_session_id=task_session.session_id,
                        expert_type=subtask_data.expert_type,
                        task_description=subtask_data.task_description,
                        sort_order=subtask_data.sort_order,
                        input_data=subtask_data.input_data,
                        execution_mode=subtask_data.execution_mode,
                        depends_on=subtask_data.depends_on
                    )
                db_session.commit()
                db_session.refresh(task_session)
            else:
                # åˆ›å»ºæ–°çš„ TaskSession
                task_session = create_task_session_with_subtasks(
                    db=db_session,
                    thread_id=thread_id,
                    user_query=user_query,
                    plan_summary=commander_response.strategy,
                    estimated_steps=commander_response.estimated_steps,
                    subtasks_data=subtasks_data,
                    execution_mode="sequential"
                )
                print(f"[COMMANDER] ä»»åŠ¡ä¼šè¯å·²åˆ›å»º: {task_session.session_id}")

        # è½¬æ¢ä¸ºå†…éƒ¨å­—å…¸æ ¼å¼ï¼ˆç”¨äº LangGraph çŠ¶æ€æµè½¬ï¼‰
        # v3.1: æ”¯æŒæ˜¾å¼ä¾èµ–å…³ç³»ï¼ŒåŒ…å« task_idï¼ˆCommander ç”Ÿæˆï¼‰å’Œ depends_on
        sub_tasks_list = task_session.sub_tasks if task_session else []
        task_list = []
        for idx, subtask in enumerate(sub_tasks_list):
            commander_task = commander_response.tasks[idx]
            task_list.append({
                "id": subtask.id,  # æ•°æ®åº“ç”Ÿæˆçš„ UUID
                "task_id": commander_task.id,  # Commander ç”Ÿæˆçš„çŸ­IDï¼ˆå¦‚ task_searchï¼‰
                "expert_type": subtask.expert_type,
                "description": subtask.task_description,
                "input_data": subtask.input_data,
                "sort_order": subtask.sort_order,
                "status": subtask.status,
                "depends_on": commander_task.depends_on if commander_task.depends_on else [],
                "output_result": None,
                "started_at": None,
                "completed_at": None
            })

        print(f"[COMMANDER] ç”Ÿæˆäº† {len(task_list)} ä¸ªä»»åŠ¡ã€‚ç­–ç•¥: {commander_response.strategy}")

        # v3.0: æ„å»ºäº‹ä»¶é˜Ÿåˆ—
        event_queue = []
        
        # å‘é€ plan.created äº‹ä»¶
        if task_session:
            plan_event = event_plan_created(
                session_id=task_session.session_id,
                summary=commander_response.strategy,
                estimated_steps=commander_response.estimated_steps,
                execution_mode="sequential",
                tasks=[
                    {
                        "id": t.id,
                        "task_id": commander_response.tasks[idx].id,
                        "expert_type": t.expert_type,
                        "description": t.task_description,
                        "sort_order": t.sort_order,
                        "status": t.status,
                        "depends_on": commander_response.tasks[idx].depends_on if commander_response.tasks[idx].depends_on else []
                    }
                    for idx, t in enumerate(task_session.sub_tasks)
                ]
            )
            event_queue.append({"type": "sse", "event": sse_event_to_string(plan_event)})

        return {
            "task_list": task_list,
            "strategy": commander_response.strategy,
            "current_task_index": 0,
            "expert_results": [],
            "task_session_id": task_session.session_id if task_session else None,
            "event_queue": event_queue,
            # ä¿ç•™å‰ç«¯å…¼å®¹çš„å…ƒæ•°æ®
            "__task_plan": {
                "task_count": len(task_list),
                "strategy": commander_response.strategy,
                "estimated_steps": commander_response.estimated_steps,
                "tasks": task_list
            }
        }

    except Exception as e:
        print(f"[ERROR] Commander è§„åˆ’å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            "task_list": [],
            "strategy": f"Error: {str(e)}",
            "current_task_index": 0,
            "event_queue": []
        }

# --- v3.1: Expert Dispatcher èŠ‚ç‚¹ï¼ˆæ”¯æŒæ˜¾å¼ä¾èµ–æ³¨å…¥å’Œä¸Šä¸‹æ–‡ä¼ é€’ï¼‰---
async def expert_dispatcher_node(state: AgentState) -> Dict[str, Any]:
    """
    ä¸“å®¶åˆ†å‘å™¨èŠ‚ç‚¹
    v3.1 æ›´æ–°ï¼šæ”¯æŒæ˜¾å¼ä¾èµ–å…³ç³»ï¼ˆDAGï¼‰ï¼Œè‡ªåŠ¨æ³¨å…¥å‰ç½®ä»»åŠ¡è¾“å‡ºåˆ°ä¸Šä¸‹æ–‡
    """
    task_list = state["task_list"]
    current_index = state["current_task_index"]
    expert_results = state.get("expert_results", [])
    
    # è·å–æ•°æ®åº“ä¼šè¯
    db_session = state.get("db_session")
    task_session_id = state.get("task_session_id")
    
    # æ”¶é›†äº‹ä»¶é˜Ÿåˆ—
    event_queue = state.get("event_queue", [])

    if current_index >= len(task_list):
        return {"expert_results": expert_results, "event_queue": event_queue}

    current_task = task_list[current_index]
    task_id = current_task["id"]
    task_short_id = current_task.get("task_id", f"task_{current_index}")  # Commander ç”Ÿæˆçš„çŸ­ID
    expert_type = current_task["expert_type"]
    description = current_task["description"]
    depends_on = current_task.get("depends_on", [])

    print(f"[EXEC] æ‰§è¡Œä»»åŠ¡ [{current_index + 1}/{len(task_list)}] - {expert_type}: {description}")
    
    # v3.1: ä¾èµ–æ£€æŸ¥å’Œä¸Šä¸‹æ–‡æ³¨å…¥
    dependency_context = ""
    dependency_outputs = []
    if depends_on:
        # æ„å»º task_short_id -> result çš„æ˜ å°„
        task_result_map = {}
        for result in expert_results:
            short_id = result.get("task_short_id")
            if short_id:
                task_result_map[short_id] = result
        
        # è°ƒè¯•æ—¥å¿—
        print(f"[DEBUG] depends_on: {depends_on}")
        print(f"[DEBUG] task_result_map keys: {list(task_result_map.keys())}")
        print(f"[DEBUG] expert_results count: {len(expert_results)}")
        
        # æ”¶é›†ä¾èµ–ä»»åŠ¡çš„è¾“å‡º
        for dep_task_id in depends_on:
            if dep_task_id in task_result_map:
                dep_result = task_result_map[dep_task_id]
                dependency_outputs.append({
                    "task_id": dep_task_id,
                    "expert_type": dep_result["expert_type"],
                    "description": dep_result["description"],
                    "output": dep_result["output"]
                })
                print(f"[DEBUG] æ‰¾åˆ°ä¾èµ–ä»»åŠ¡ {dep_task_id}: {dep_result['expert_type']}")
            else:
                print(f"[WARN] ä¾èµ–ä»»åŠ¡ {dep_task_id} çš„è¾“å‡ºå°šæœªå°±ç»ª")
        
        if dependency_outputs:
            # æ ¼å¼åŒ–ä¾èµ–ä¸Šä¸‹æ–‡
            dependency_parts = []
            for dep in dependency_outputs:
                output_preview = dep['output'][:500] + "..." if len(dep['output']) > 500 else dep['output']
                dep_str = f"ã€å‰ç½®ä»»åŠ¡: {dep['task_id']} ({dep['expert_type']})ã€‘\næè¿°: {dep['description']}\nè¾“å‡º:\n{output_preview}"
                dependency_parts.append(dep_str)
            
            dependency_context = "\n\n".join(dependency_parts)
            print(f"[DEP] å·²æ³¨å…¥ {len(dependency_outputs)} ä¸ªä¾èµ–ä»»åŠ¡çš„ä¸Šä¸‹æ–‡")
    
    # v3.0: æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸º running
    if db_session:
        update_subtask_status(db_session, task_id, "running")
    
    # v3.0: å‘é€ task.started äº‹ä»¶
    started_event = event_task_started(
        task_id=task_id,
        expert_type=expert_type,
        description=description
    )
    event_queue.append({"type": "sse", "event": sse_event_to_string(started_event)})

    try:
        # ä½¿ç”¨ get_expert_function è·å–ä¸“å®¶æ‰§è¡Œå‡½æ•°
        expert_func = get_expert_function(expert_type)

        # v3.1: å‡†å¤‡å¸¦ä¾èµ–ä¸Šä¸‹æ–‡çš„ state
        # å°†ä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥åˆ° current_task çš„ input_data ä¸­
        enhanced_input_data = current_task.get("input_data", {}).copy()
        if dependency_context:
            enhanced_input_data["__dependency_context"] = dependency_context
            # åŒæ—¶ä¿å­˜ç»“æ„åŒ–çš„ä¾èµ–æ•°æ®ä¾›ä¸“å®¶ä½¿ç”¨
            enhanced_input_data["__dependencies"] = [
                {
                    "task_id": dep["task_id"],
                    "expert_type": dep["expert_type"],
                    "output": dep["output"]
                }
                for dep in dependency_outputs
            ]
        
        # åˆ›å»ºå¢å¼ºçš„ stateï¼Œæ³¨å…¥ä¾èµ–ä¸Šä¸‹æ–‡
        enhanced_task = current_task.copy()
        enhanced_task["input_data"] = enhanced_input_data
        
        # ä¸´æ—¶æ›¿æ¢ state ä¸­çš„ current_task
        original_task_list = task_list.copy()
        task_list[current_index] = enhanced_task
        
        # v3.2: åˆ›å»ºå¢å¼ºçš„ stateï¼Œç¡®ä¿ä¸“å®¶èƒ½è·å–åˆ°ä¾èµ–ä¸Šä¸‹æ–‡
        enhanced_state = state.copy()
        enhanced_state["task_list"] = task_list.copy()

        if expert_type in DYNAMIC_EXPERT_FUNCTIONS:
            # ç³»ç»Ÿå†…ç½®ä¸“å®¶ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘ï¼ˆé¢„å…ˆåˆ›å»º LLMï¼‰
            expert_config = get_expert_config_cached(expert_type)
            if expert_config and 'provider' in expert_config:
                expert_llm = get_expert_llm(provider=expert_config['provider'])
            else:
                expert_llm = get_expert_llm()
            result = await expert_func(enhanced_state, expert_llm)
        else:
            # è‡ªå®šä¹‰ä¸“å®¶ï¼Œä½¿ç”¨é€šç”¨èŠ‚ç‚¹ï¼ˆgeneric_worker_node è‡ªå·±ä¼šåˆ›å»º LLMï¼‰
            result = await expert_func(enhanced_state)
        
        # æ¢å¤åŸ task_listï¼ˆé¿å…æ±¡æŸ“ stateï¼‰
        task_list[current_index] = original_task_list[current_index]

        if "error" in result:
             raise AppError(message=result["error"], code="EXPERT_EXECUTION_ERROR")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        current_task["output_result"] = {"content": result.get("output_result", "")}
        current_task["status"] = result.get("status", "completed")
        current_task["completed_at"] = result.get("completed_at")
        
        # æ·»åŠ åˆ°ç»“æœé›†ï¼ˆv3.1: åŒ…å« task_short_id ç”¨äºä¾èµ–æŸ¥æ‰¾ï¼‰
        updated_results = state["expert_results"] + [{
            "task_id": current_task["id"],  # æ•°æ®åº“ UUID
            "task_short_id": task_short_id,  # Commander ç”Ÿæˆçš„çŸ­ ID (å¦‚ task_search)
            "expert_type": expert_type,
            "description": description,
            "output": result.get("output_result", ""),
            "status": result.get("status", "unknown"),
            "duration_ms": result.get("duration_ms", 0)
        }]

        duration_ms = result.get('duration_ms', 0)
        duration = duration_ms / 1000
        print(f"   [OK] è€—æ—¶ {duration:.2f}s")
        
        # v3.0: å¤„ç†äº§ç‰©ï¼ˆArtifactï¼‰
        artifacts_data = result.get("artifacts", [])
        if not artifacts_data and result.get("artifact"):
            # å…¼å®¹æ—§æ ¼å¼
            artifacts_data = [result.get("artifact")]
        
        # v3.0: ä¿å­˜äº§ç‰©åˆ°æ•°æ®åº“
        artifact_count = 0
        if db_session and artifacts_data:
            from models import ArtifactCreate
            artifact_creates = [
                ArtifactCreate(
                    type=art.get("type", "text"),
                    title=art.get("title"),
                    content=art.get("content", ""),
                    language=art.get("language"),
                    sort_order=idx
                )
                for idx, art in enumerate(artifacts_data)
            ]
            created_artifacts = create_artifacts_batch(db_session, task_id, artifact_creates)
            artifact_count = len(created_artifacts)
            
            # å‘é€ artifact.generated äº‹ä»¶
            for art, created in zip(artifacts_data, created_artifacts):
                artifact_event = event_artifact_generated(
                    task_id=task_id,
                    expert_type=expert_type,
                    artifact_id=created.id,
                    artifact_type=created.type,
                    content=created.content,
                    title=created.title,
                    language=created.language,
                    sort_order=created.sort_order
                )
                event_queue.append({"type": "sse", "event": sse_event_to_string(artifact_event)})
        
        # v3.0: æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸º completed
        if db_session:
            update_subtask_status(
                db_session, 
                task_id, 
                "completed",
                output_result={"content": result.get("output_result", "")},
                duration_ms=duration_ms
            )
        
        # v3.0: å‘é€ task.completed äº‹ä»¶
        completed_event = event_task_completed(
            task_id=task_id,
            expert_type=expert_type,
            description=description,
            output=result.get("output_result", ""),
            duration_ms=duration_ms,
            artifact_count=artifact_count
        )
        event_queue.append({"type": "sse", "event": sse_event_to_string(completed_event)})

        return_dict = {
            "task_list": task_list,
            "expert_results": updated_results,
            "current_task_index": current_index + 1,
            "event_queue": event_queue,
            "__expert_info": { # ä¿ç•™å‰ç«¯å…¼å®¹
                "expert_type": expert_type,
                "description": description,
                "status": "completed",
                "output": result.get("output_result", ""),
                "duration_ms": duration_ms,
            }
        }
        if "artifact" in result:
            return_dict["artifact"] = result["artifact"]

        return return_dict

    except Exception as e:
        print(f"   [ERROR] ä¸“å®¶æ‰§è¡Œå¤±è´¥: {e}")
        current_task["status"] = "failed"
        
        # v3.0: æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸º failed
        if db_session:
            update_subtask_status(
                db_session,
                task_id,
                "failed",
                error_message=str(e)
            )
        
        # v3.0: å‘é€ task.failed äº‹ä»¶
        failed_event = event_task_failed(
            task_id=task_id,
            expert_type=expert_type,
            description=description,
            error=str(e)
        )
        event_queue.append({"type": "sse", "event": sse_event_to_string(failed_event)})
        
        return {
            "task_list": task_list,
            "current_task_index": current_index + 1,
            "event_queue": event_queue,
            "__expert_info": {
                "expert_type": expert_type,
                "description": description,
                "status": "failed",
                "error": str(e),
                "duration_ms": 0,
            }
        }

# --- v3.1: Aggregator èŠ‚ç‚¹ï¼ˆè°ƒç”¨ LLM ç”Ÿæˆè‡ªç„¶è¯­è¨€æ€»ç»“ï¼‰---
async def aggregator_node(state: AgentState) -> Dict[str, Any]:
    """
    èšåˆå™¨èŠ‚ç‚¹
    v3.1 æ›´æ–°ï¼šè°ƒç”¨ LLM ç”Ÿæˆè‡ªç„¶è¯­è¨€æ€»ç»“ï¼Œæ”¯æŒæµå¼è¾“å‡º
    """
    expert_results = state["expert_results"]
    strategy = state["strategy"]

    # è·å–æ•°æ®åº“ä¼šè¯
    db_session = state.get("db_session")
    task_session_id = state.get("task_session_id")
    event_queue = state.get("event_queue", [])
    # v3.0: è·å–å‰ç«¯ä¼ é€’çš„ message_idï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    # æ³¨æ„ï¼šMessage.id åœ¨æ•°æ®åº“ä¸­æ˜¯ INTEGER ç±»å‹ï¼Œä¸èƒ½ç›´æ¥ä½¿ç”¨ UUID
    # æ‰€ä»¥ message_id åªç”¨äº SSE äº‹ä»¶æ ‡è¯†ï¼Œä¸ç”¨äºæ•°æ®åº“å­˜å‚¨
    message_id = state.get("message_id", str(uuid4()))

    if not expert_results:
        return {"final_response": "æœªç”Ÿæˆä»»ä½•æ‰§è¡Œç»“æœã€‚", "event_queue": event_queue}

    print(f"[AGG] æ­£åœ¨èšåˆ {len(expert_results)} ä¸ªç»“æœï¼Œè°ƒç”¨ LLM ç”Ÿæˆæ€»ç»“...")

    # v3.1: æ„å»º Aggregator çš„ Prompt
    aggregator_prompt = _build_aggregator_prompt(expert_results, strategy)
    
    # v3.1: è·å– Aggregator LLMï¼ˆå¸¦å…œåº•é€»è¾‘ï¼‰
    aggregator_llm = get_aggregator_llm()
    
    # v3.1: æµå¼ç”Ÿæˆæ€»ç»“
    final_response_chunks = []
    
    try:
        # ä½¿ç”¨æµå¼è¾“å‡º
        async for chunk in aggregator_llm.astream([
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ¥å‘Šæ’°å†™ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†å¤šä¸ªä¸“å®¶çš„åˆ†æç»“æœæ•´åˆæˆä¸€ä»½è¿è´¯ã€ä¸“ä¸šçš„æœ€ç»ˆæŠ¥å‘Šã€‚ä¸è¦ç®€å•ç½—åˆ—ï¼Œè¦ç”¨è‡ªç„¶æµç•…çš„è¯­è¨€è¿›è¡Œæ€»ç»“ã€‚"),
            HumanMessage(content=aggregator_prompt)
        ]):
            content = chunk.content if hasattr(chunk, 'content') else str(chunk)
            if content:
                final_response_chunks.append(content)
                
                # å‘é€ message.delta äº‹ä»¶ï¼ˆå®æ—¶æµå¼ï¼‰
                delta_event = event_message_delta(
                    message_id=message_id,
                    content=content,
                    is_final=False
                )
                event_queue.append({"type": "sse", "event": sse_event_to_string(delta_event)})
        
        final_response = "".join(final_response_chunks)
        
    except Exception as e:
        print(f"[AGG] LLM æ€»ç»“å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ‹¼æ¥: {e}")
        # å…œåº•ï¼šä½¿ç”¨ç®€å•æ‹¼æ¥
        final_response = _build_markdown_response(expert_results, strategy)
        
        # å‘é€ç®€å•æ‹¼æ¥çš„ç»“æœ
        chunk_size = 100
        for i in range(0, len(final_response), chunk_size):
            chunk = final_response[i:i + chunk_size]
            is_final = (i + chunk_size) >= len(final_response)
            delta_event = event_message_delta(
                message_id=message_id,
                content=chunk,
                is_final=is_final
            )
            event_queue.append({"type": "sse", "event": sse_event_to_string(delta_event)})
    
    # å‘é€ message.done äº‹ä»¶
    done_event = event_message_done(
        message_id=message_id,
        full_content=final_response
    )
    event_queue.append({"type": "sse", "event": sse_event_to_string(done_event)})
    
    # v3.0: æ›´æ–°ä»»åŠ¡ä¼šè¯çŠ¶æ€ä¸º completed
    if db_session and task_session_id:
        update_task_session_status(
            db_session,
            task_session_id,
            "completed",
            final_response=final_response
        )
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šæŒä¹…åŒ–èšåˆæ¶ˆæ¯åˆ°æ•°æ®åº“ ğŸ”¥ğŸ”¥ğŸ”¥
        # åªæœ‰å­˜è¿›å»äº†ï¼Œä¸‹æ¬¡åˆ·æ–° GET /messages æ‰èƒ½çœ‹åˆ°å®ƒ
        conversation_id = state.get("thread_id")  # v3.2: ä½¿ç”¨ thread_id ä½œä¸º conversation_id
        if conversation_id:
            # åˆ›å»ºæ¶ˆæ¯è®°å½•ï¼ˆå…³è” conversation_idï¼‰
            # æ³¨æ„ï¼šä¸æ‰‹åŠ¨æŒ‡å®š idï¼Œè®©æ•°æ®åº“è‡ªåŠ¨ç”Ÿæˆï¼ˆid æ˜¯ INTEGER è‡ªå¢ï¼‰
            # message_id åªç”¨äº SSE äº‹ä»¶æ ‡è¯†
            # æ³¨æ„ï¼šMessage æ¨¡å‹æš‚æ—¶æ²¡æœ‰ task_session_id å­—æ®µï¼Œä»¥åå¯èƒ½éœ€è¦æ·»åŠ 
            message_record = MessageModel(
                thread_id=conversation_id,
                role="assistant",
                content=final_response
            )
            db_session.add(message_record)
            db_session.commit()
            print(f"[AGG] èšåˆæ¶ˆæ¯å·²æŒä¹…åŒ–åˆ°æ•°æ®åº“ï¼Œconversation_id={conversation_id}")
    
    print(f"[AGG] èšåˆå®Œæˆï¼Œå›å¤é•¿åº¦: {len(final_response)}")

    return {
        "final_response": final_response,
        "event_queue": event_queue
    }


def _build_aggregator_prompt(expert_results: List[Dict[str, Any]], strategy: str) -> str:
    """
    æ„å»º Aggregator çš„ Promptï¼Œå°†å¤šä¸ªä¸“å®¶ç»“æœè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æ€»ç»“çš„è¾“å…¥
    
    Args:
        expert_results: ä¸“å®¶æ‰§è¡Œç»“æœåˆ—è¡¨
        strategy: æ‰§è¡Œç­–ç•¥æ¦‚è¿°
        
    Returns:
        str: ä¾› LLM æ€»ç»“çš„ Prompt
    """
    lines = [
        f"æ‰§è¡Œç­–ç•¥: {strategy}",
        "",
        "å„ä¸“å®¶åˆ†æç»“æœå¦‚ä¸‹ï¼š",
        ""
    ]
    
    for i, res in enumerate(expert_results, 1):
        lines.append(f"ã€ä¸“å®¶ {i}: {res['expert_type'].upper()}ã€‘")
        lines.append(f"ä»»åŠ¡æè¿°: {res['description']}")
        lines.append(f"åˆ†æç»“æœ:\n{res['output']}")
        lines.append("")
    
    lines.extend([
        "---",
        "",
        "è¯·åŸºäºä»¥ä¸Šå„ä¸“å®¶çš„åˆ†æç»“æœï¼Œæ’°å†™ä¸€ä»½è¿è´¯ã€ä¸“ä¸šçš„æœ€ç»ˆæ€»ç»“æŠ¥å‘Šã€‚è¦æ±‚ï¼š",
        "1. ç”¨è‡ªç„¶æµç•…çš„è¯­è¨€æ•´åˆæ‰€æœ‰ä¸“å®¶çš„è§‚ç‚¹ï¼Œä¸è¦ç®€å•ç½—åˆ—",
        "2. çªå‡ºå…³é”®å‘ç°å’Œæ ¸å¿ƒç»“è®º",
        "3. ä¿æŒé€»è¾‘æ¸…æ™°ï¼Œç»“æ„å®Œæ•´",
        "4. å¦‚æœä¸“å®¶ç»“æœä¹‹é—´æœ‰ä¾èµ–å…³ç³»ï¼Œè¯·ä½“ç°è¿™ç§å…³è”",
        ""
    ])
    
    return "\n".join(lines)

def _build_markdown_response(expert_results: List[Dict[str, Any]], strategy: str) -> str:
    # ç®€å•çš„ Markdown æ„å»ºé€»è¾‘
    lines = [f"# æ‰§è¡ŒæŠ¥å‘Š\n**ç­–ç•¥**: {strategy}\n---"]
    for i, res in enumerate(expert_results, 1):
        lines.append(f"## {i}. {res['expert_type'].upper()}: {res['description']}")
        lines.append(f"{res['output']}\n")
    return "\n".join(lines)

# ============================================================================
# 4. æ¡ä»¶è·¯ç”±é€»è¾‘ (Edges)
# ============================================================================

def route_router(state: AgentState) -> str:
    """Router ä¹‹åçš„å»å‘"""
    decision = state.get("router_decision", "complex")

    print(f"[ROUTE_ROUTER] å†³ç­–: {decision}, å°†è·¯ç”±åˆ°: {'direct_reply' if decision == 'simple' else 'commander'}")

    if decision == "simple":
        # Simple æ¨¡å¼è¿›å…¥ direct_reply èŠ‚ç‚¹
        return "direct_reply"
    else:
        # Complex æ¨¡å¼è¿›å…¥æŒ‡æŒ¥å®˜
        return "commander"

def route_dispatcher(state: AgentState) -> str:
    """å†³å®š åˆ†å‘å™¨ ä¹‹åçš„å»å‘ï¼ˆå¾ªç¯æˆ–èšåˆï¼‰"""
    if state["current_task_index"] >= len(state["task_list"]):
        return "aggregator"
    return "expert_dispatcher"

# ============================================================================
# 5. æ„å»ºå·¥ä½œæµå›¾
# ============================================================================

def create_smart_router_workflow() -> StateGraph:
    workflow = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("router", router_node)
    workflow.add_node("direct_reply", direct_reply_node)  # æ–°å¢ï¼šSimple æ¨¡å¼æµå¼å›å¤
    workflow.add_node("commander", commander_node)
    workflow.add_node("expert_dispatcher", expert_dispatcher_node)
    workflow.add_node("aggregator", aggregator_node)

    # è®¾ç½®å…¥å£ï¼šç°åœ¨å…¥å£æ˜¯ Routerï¼
    workflow.set_entry_point("router")

    # æ·»åŠ è¿çº¿

    # 1. Router -> (Direct Reply | Commander)
    workflow.add_conditional_edges(
        "router",
        route_router,
        {
            "direct_reply": "direct_reply",
            "commander": "commander"
        }
    )

    # 2. Direct Reply -> END
    workflow.add_edge("direct_reply", END)

    # 3. Commander -> Dispatcher (æŒ‡æŒ¥å®˜å®Œæˆåæ‰§è¡Œ)
    workflow.add_edge("commander", "expert_dispatcher")

    # 3. Dispatcher -> (Loop | Aggregator)
    workflow.add_conditional_edges(
        "expert_dispatcher",
        route_dispatcher,
        {
            "expert_dispatcher": "expert_dispatcher",
            "aggregator": "aggregator"
        }
    )

    # 4. Aggregator -> END
    workflow.add_edge("aggregator", END)

    return workflow.compile()

# å¯¼å‡ºç¼–è¯‘åçš„å›¾
commander_graph = create_smart_router_workflow()

# ============================================================================
# æµ‹è¯•å°è£…å‡½æ•°
# ============================================================================

async def execute_commander_workflow(user_query: str) -> Dict[str, Any]:
    print(f"--- [START] æŸ¥è¯¢: {user_query} ---")
    initial_state: AgentState = {
        "messages": [HumanMessage(content=user_query)],
        "task_list": [],
        "current_task_index": 0,
        "strategy": "",
        "expert_results": [],
        "final_response": ""
    }
    final_state = await commander_graph.ainvoke(initial_state)
    print("--- [DONE] ---")
    return final_state

if __name__ == "__main__":
    import asyncio
    async def test():
        # æµ‹è¯• 1: ç®€å•é—²èŠ
        print("\n=== æµ‹è¯• 1: ç®€å•æ¨¡å¼ ===")
        await execute_commander_workflow("ä½ å¥½ï¼Œåœ¨å—ï¼Ÿ")
        
        # æµ‹è¯• 2: å¤æ‚ä»»åŠ¡
        print("\n=== æµ‹è¯• 2: å¤æ‚æ¨¡å¼ ===")
        await execute_commander_workflow("å¸®æˆ‘å†™ä¸€ä¸ª Python è„šæœ¬æ¥æŠ“å–è‚¡ç¥¨ä»·æ ¼ã€‚")
    
    asyncio.run(test())