"""
XPouch AI æ™ºèƒ½è·¯ç”±å·¥ä½œæµ (v3.0 æ¶æ„)
é›†æˆæ„å›¾è¯†åˆ« (Router) -> ä»»åŠ¡è§„åˆ’ (Planner) -> ä¸“å®¶æ‰§è¡Œ (Experts)
æ”¯æŒäº‹ä»¶æº¯æºæŒä¹…åŒ–å’Œ Server-Driven UI
"""
from typing import TypedDict, Annotated, List, Dict, Any, Literal, Optional, AsyncGenerator
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser
import os
from dotenv import load_dotenv
import pathlib
from pydantic import BaseModel, Field
from uuid import uuid4
from datetime import datetime

# å¯¼å…¥æ•°æ®æ¨¡å‹
import sys
sys.path.append(str(pathlib.Path(__file__).parent.parent))
from models import ExpertType, TaskStatus, SubTask, TaskSession
from config import init_langchain_tracing, get_langsmith_config
from utils.json_parser import parse_llm_json
from utils.exceptions import AppError
from utils.event_generator import (
    EventGenerator,
    event_plan_created, event_task_started, event_task_completed, event_task_failed,
    event_artifact_generated, event_message_delta, event_message_done,
    sse_event_to_string
)
from crud.task_session import (
    create_task_session_with_subtasks,
    update_subtask_status,
    create_artifacts_batch,
    update_task_session_status
)
# å°†åŸæœ‰çš„ COMMANDER_SYSTEM_PROMPT ä½œä¸ºè§„åˆ’å™¨ (Planner) çš„æç¤ºè¯
from constants import COMMANDER_SYSTEM_PROMPT as PLANNER_SYSTEM_PROMPT, ROUTER_SYSTEM_PROMPT, DEFAULT_ASSISTANT_PROMPT 
from agents.dynamic_experts import DYNAMIC_EXPERT_FUNCTIONS, initialize_expert_cache
from agents.expert_loader import get_expert_config_cached

# ============================================================================
# 0. è®¾ç½®ä¸é…ç½®
# ============================================================================
# ä»å·¥å‚å‡½æ•°å¯¼å…¥ LLM å®ä¾‹åˆ›å»ºå™¨
from utils.llm_factory import get_router_llm, get_planner_llm, get_expert_llm

# LangSmith é“¾è·¯è¿½è¸ª
env_path = pathlib.Path(__file__).parent.parent / ".env"
load_dotenv(dotenv_path=env_path)

langsmith_config = get_langsmith_config()
if langsmith_config["enabled"]:
    init_langchain_tracing(langsmith_config)

# åˆå§‹åŒ– LLM - ä½¿ç”¨å·¥å‚å‡½æ•°
# Router ä½¿ç”¨è¾ƒä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
llm = get_router_llm()

# å…¨å±€äº‹ä»¶ç”Ÿæˆå™¨ï¼ˆç”¨äºç”Ÿæˆ SSE äº‹ä»¶ï¼‰
event_gen = EventGenerator()

# ============================================================================
# 1. ç»“æ„å®šä¹‰ä¸æç¤ºè¯ (æ–°çš„ Router é€»è¾‘)
# ============================================================================
# ROUTER_SYSTEM_PROMPT å·²ä» constants.py å¯¼å…¥

class RoutingDecision(BaseModel):
    """v2.7 ç½‘å…³å†³ç­–ç»“æ„ï¼ˆRouteråªè´Ÿè´£åˆ†ç±»ï¼‰"""
    decision_type: Literal["simple", "complex"] = Field(description="å†³ç­–ç±»å‹")

# --- ä¿ç•™åŸæœ‰çš„è§„åˆ’å™¨ç»“æ„ (åŸ CommanderOutput) ---

class SubTaskOutput(BaseModel):
    """å•ä¸ªå­ä»»åŠ¡ç»“æ„ (Planner ä½¿ç”¨)"""
    expert_type: ExpertType = Field(description="æ‰§è¡Œæ­¤ä»»åŠ¡çš„ä¸“å®¶ç±»å‹")
    description: str = Field(description="ä»»åŠ¡æè¿°")
    input_data: Dict[str, Any] = Field(default={}, description="è¾“å…¥å‚æ•°")
    priority: int = Field(default=0, description="ä¼˜å…ˆçº§ (0=æœ€é«˜)")

class PlannerOutput(BaseModel):
    """è§„åˆ’å™¨è¾“å‡º - å­ä»»åŠ¡åˆ—è¡¨ (åŸ CommanderOutput)"""
    tasks: List[SubTaskOutput] = Field(description="å­ä»»åŠ¡åˆ—è¡¨")
    strategy: str = Field(description="æ‰§è¡Œç­–ç•¥æ¦‚è¿°")
    estimated_steps: int = Field(description="é¢„è®¡æ­¥éª¤æ•°")

# ============================================================================
# 2. çŠ¶æ€å®šä¹‰
# ============================================================================

class AgentState(TypedDict):
    """è¶…æ™ºèƒ½ä½“çš„å…¨å±€çŠ¶æ€"""
    messages: Annotated[List[BaseMessage], add_messages]
    task_list: List[Dict[str, Any]]
    current_task_index: int
    strategy: str
    expert_results: List[Dict[str, Any]]
    final_response: str
    # è®°å½•è·¯ç”±å†³ç­–ä¿¡æ¯
    router_decision: str
    # v3.0 æ–°å¢ï¼šæ•°æ®åº“æŒä¹…åŒ–ç›¸å…³
    thread_id: Optional[str]           # å…³è”çš„å¯¹è¯ID
    task_session_id: Optional[str]     # ä»»åŠ¡ä¼šè¯ID
    db_session: Optional[Any]          # æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äºèŠ‚ç‚¹å†…æŒä¹…åŒ–ï¼‰
    # v3.0 æ–°å¢ï¼šäº‹ä»¶é˜Ÿåˆ—ï¼ˆç”¨äº SSE æ¨é€ï¼‰
    event_queue: List[Dict[str, Any]]  # å¾…å‘é€çš„äº‹ä»¶åˆ—è¡¨ 

# ============================================================================
# 3. èŠ‚ç‚¹å®ç°
# ============================================================================

# --- æ–°å¢ï¼šRouter èŠ‚ç‚¹ (ç½‘å…³) ---
async def router_node(state: AgentState) -> Dict[str, Any]:
    """[ç½‘å…³] åªè´Ÿè´£åˆ†ç±»ï¼Œä¸è´Ÿè´£å›ç­”"""
    messages = state["messages"]

    # æ–­ç‚¹æ¢å¤æ£€æŸ¥
    if state.get("task_list") and len(state.get("task_list", [])) > 0:
        return {"router_decision": "complex"}

    parser = PydanticOutputParser(pydantic_object=RoutingDecision)
    try:
        # ğŸ”¥ å…³é”®ï¼šé™æ€ SystemPrompt + åŠ¨æ€ Messages
        response = await llm.ainvoke(
            [
                SystemMessage(content=ROUTER_SYSTEM_PROMPT),
                *messages  # ç”¨æˆ·çš„è¾“å…¥åœ¨è¿™é‡Œ
            ],
            config={"tags": ["router"]}
        )
        decision = parser.parse(response.content)
        return {"router_decision": decision.decision_type}
    except Exception as e:
        print(f"[ROUTER ERROR] {e}")
        return {"router_decision": "complex"}

# --- æ–°å¢ï¼šDirect Reply èŠ‚ç‚¹ (Simple æ¨¡å¼æµå¼å›ç­”) ---
async def direct_reply_node(state: AgentState) -> Dict[str, Any]:
    """[ç›´è¿èŠ‚ç‚¹] è´Ÿè´£ Simple æ¨¡å¼ä¸‹çš„æµå¼å›å¤"""
    print(f"[DIRECT_REPLY] èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    messages = state["messages"]

    # ä½¿ç”¨æµå¼é…ç½®ï¼Œæ·»åŠ  metadata ä¾¿äºè¿½è¸ª
    config = {"tags": ["direct_reply"], "metadata": {"node_type": "direct_reply"}}
    
    # ç›´æ¥è°ƒç”¨ LLM ç”Ÿæˆå›å¤ (è¿™æ‰æ˜¯çœŸæ­£çš„æµå¼)
    response = await llm.ainvoke(
        [
            SystemMessage(content=DEFAULT_ASSISTANT_PROMPT),
            *messages  # ç”¨æˆ·çš„å†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡
        ],
        config=config
    )

    print(f"[DIRECT_REPLY] èŠ‚ç‚¹å®Œæˆï¼Œå›å¤é•¿åº¦: {len(response.content)}")

    # ç›´æ¥è¿”å› response å¯¹è±¡ï¼ˆä¿ç•™å®Œæ•´å…ƒæ•°æ®ï¼‰ï¼Œå¹¶æ·»åŠ  final_response å­—æ®µ
    return {
        "messages": [response],
        "final_response": response.content
    }

# --- ä¿®æ”¹ï¼šPlanner èŠ‚ç‚¹ (åŸ Commander) ---
async def planner_node(state: AgentState) -> Dict[str, Any]:
    """
    [æ¶æ„å¸ˆ] å°†å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡ã€‚
    v3.0 æ›´æ–°ï¼šç«‹å³æŒä¹…åŒ–åˆ°æ•°æ®åº“ï¼Œå‘é€ plan.created äº‹ä»¶
    """
    messages = state["messages"]
    last_message = messages[-1]
    user_query = last_message.content if isinstance(last_message, HumanMessage) else str(last_message.content)
    
    # è·å–æ•°æ®åº“ä¼šè¯å’Œ thread_id
    db_session = state.get("db_session")
    thread_id = state.get("thread_id")
    
    # åŠ è½½é…ç½® (æ•°æ®åº“æˆ–å›é€€)
    commander_config = get_expert_config_cached("commander") 
    
    if not commander_config:
        system_prompt = PLANNER_SYSTEM_PROMPT
        model = os.getenv("MODEL_NAME", "deepseek-chat")
        temperature = 0.5
        print(f"[PLANNER] ä½¿ç”¨é»˜è®¤å›é€€é…ç½®: model={model}")
    else:
        system_prompt = commander_config["system_prompt"]
        model = commander_config["model"]
        temperature = commander_config["temperature"]
        print(f"[PLANNER] åŠ è½½æ•°æ®åº“é…ç½®: model={model}")
    
    # æ‰§è¡Œ LLM è¿›è¡Œè§„åˆ’
    try:
        llm_with_config = llm.bind(model=model, temperature=temperature)
        
        from langchain_core.runnables import RunnableConfig
        response = await llm_with_config.ainvoke(
            [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"ç”¨æˆ·æŸ¥è¯¢: {user_query}\n\nè¯·å°†æ­¤æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡åˆ—è¡¨ã€‚")
            ],
            config=RunnableConfig(
                tags=["commander", "planner"],
                metadata={"node_type": "planner"}
            )
        )

        # è§£æ JSON
        planner_response = parse_llm_json(
            response.content,
            PlannerOutput,
            strict=False,
            clean_markdown=True
        )

        # v3.0: å‡†å¤‡å­ä»»åŠ¡æ•°æ®
        from models import SubTaskCreate
        subtasks_data = [
            SubTaskCreate(
                expert_type=task.expert_type,
                task_description=task.description,
                input_data=task.input_data,
                sort_order=idx,
                execution_mode="sequential"  # é»˜è®¤ä¸²è¡Œï¼Œå¯æ‰©å±•ä¸ºå¹¶è¡Œ
            )
            for idx, task in enumerate(planner_response.tasks)
        ]

        # v3.0: ç«‹å³æŒä¹…åŒ–åˆ°æ•°æ®åº“
        task_session = None
        if db_session and thread_id:
            task_session = create_task_session_with_subtasks(
                db=db_session,
                thread_id=thread_id,
                user_query=user_query,
                plan_summary=planner_response.strategy,
                estimated_steps=planner_response.estimated_steps,
                subtasks_data=subtasks_data,
                execution_mode="sequential"
            )
            print(f"[PLANNER] ä»»åŠ¡ä¼šè¯å·²åˆ›å»º: {task_session.session_id}")

        # è½¬æ¢ä¸ºå†…éƒ¨å­—å…¸æ ¼å¼ï¼ˆç”¨äº LangGraph çŠ¶æ€æµè½¬ï¼‰
        task_list = [
            {
                "id": subtask.id,
                "expert_type": subtask.expert_type,
                "description": subtask.task_description,
                "input_data": subtask.input_data,
                "sort_order": subtask.sort_order,
                "status": subtask.status,
                "output_result": None,
                "started_at": None,
                "completed_at": None
            }
            for subtask in task_session.sub_tasks if task_session else []
        ]

        print(f"[PLANNER] ç”Ÿæˆäº† {len(task_list)} ä¸ªä»»åŠ¡ã€‚ç­–ç•¥: {planner_response.strategy}")

        # v3.0: æ„å»ºäº‹ä»¶é˜Ÿåˆ—
        event_queue = []
        
        # å‘é€ plan.created äº‹ä»¶
        if task_session:
            plan_event = event_plan_created(
                session_id=task_session.session_id,
                summary=planner_response.strategy,
                estimated_steps=planner_response.estimated_steps,
                execution_mode="sequential",
                tasks=[
                    {
                        "id": t.id,
                        "expert_type": t.expert_type,
                        "description": t.task_description,
                        "sort_order": t.sort_order,
                        "status": t.status
                    }
                    for t in task_session.sub_tasks
                ]
            )
            event_queue.append({"type": "sse", "event": sse_event_to_string(plan_event)})

        return {
            "task_list": task_list,
            "strategy": planner_response.strategy,
            "current_task_index": 0,
            "expert_results": [],
            "task_session_id": task_session.session_id if task_session else None,
            "event_queue": event_queue,
            # ä¿ç•™å‰ç«¯å…¼å®¹çš„å…ƒæ•°æ®
            "__task_plan": {
                "task_count": len(task_list),
                "strategy": planner_response.strategy,
                "estimated_steps": planner_response.estimated_steps,
                "tasks": task_list
            }
        }

    except Exception as e:
        print(f"[ERROR] Planner è§„åˆ’å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            "task_list": [],
            "strategy": f"Error: {str(e)}",
            "current_task_index": 0,
            "event_queue": []
        }

# --- v3.0: Expert Dispatcher èŠ‚ç‚¹ï¼ˆæ”¯æŒæŒä¹…åŒ–å’Œäº‹ä»¶å‘é€ï¼‰---
async def expert_dispatcher_node(state: AgentState) -> Dict[str, Any]:
    """
    ä¸“å®¶åˆ†å‘å™¨èŠ‚ç‚¹
    v3.0 æ›´æ–°ï¼šæŒä¹…åŒ–çŠ¶æ€å˜æ›´ï¼Œå‘é€ task.started/completed/failed äº‹ä»¶
    """
    task_list = state["task_list"]
    current_index = state["current_task_index"]
    
    # è·å–æ•°æ®åº“ä¼šè¯
    db_session = state.get("db_session")
    task_session_id = state.get("task_session_id")
    
    # æ”¶é›†äº‹ä»¶é˜Ÿåˆ—
    event_queue = state.get("event_queue", [])

    if current_index >= len(task_list):
        return {"expert_results": state["expert_results"], "event_queue": event_queue}

    current_task = task_list[current_index]
    task_id = current_task["id"]
    expert_type = current_task["expert_type"]
    description = current_task["description"]

    print(f"[EXEC] æ‰§è¡Œä»»åŠ¡ [{current_index + 1}/{len(task_list)}] - {expert_type}: {description}")
    
    # v3.0: æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸º running
    if db_session:
        update_subtask_status(db_session, task_id, "running")
    
    # v3.0: å‘é€ task.started äº‹ä»¶
    started_event = event_task_started(
        task_id=task_id,
        expert_type=expert_type,
        description=description
    )
    event_queue.append({"type": "sse", "event": sse_event_to_string(started_event)})

    try:
        expert_func = DYNAMIC_EXPERT_FUNCTIONS.get(expert_type)
        if not expert_func:
            raise ValueError(f"æœªçŸ¥çš„ä¸“å®¶ç±»å‹: {expert_type}")

        result = await expert_func(state, llm)

        if "error" in result:
             raise AppError(message=result["error"], code="EXPERT_EXECUTION_ERROR")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        current_task["output_result"] = {"content": result.get("output_result", "")}
        current_task["status"] = result.get("status", "completed")
        current_task["completed_at"] = result.get("completed_at")
        
        # æ·»åŠ åˆ°ç»“æœé›†
        updated_results = state["expert_results"] + [{
            "task_id": current_task["id"],
            "expert_type": expert_type,
            "description": description,
            "output": result.get("output_result", ""),
            "status": result.get("status", "unknown"),
            "duration_ms": result.get("duration_ms", 0)
        }]

        duration_ms = result.get('duration_ms', 0)
        duration = duration_ms / 1000
        print(f"   [OK] è€—æ—¶ {duration:.2f}s")
        
        # v3.0: å¤„ç†äº§ç‰©ï¼ˆArtifactï¼‰
        artifacts_data = result.get("artifacts", [])
        if not artifacts_data and result.get("artifact"):
            # å…¼å®¹æ—§æ ¼å¼
            artifacts_data = [result.get("artifact")]
        
        # v3.0: ä¿å­˜äº§ç‰©åˆ°æ•°æ®åº“
        artifact_count = 0
        if db_session and artifacts_data:
            from models import ArtifactCreate
            artifact_creates = [
                ArtifactCreate(
                    type=art.get("type", "text"),
                    title=art.get("title"),
                    content=art.get("content", ""),
                    language=art.get("language"),
                    sort_order=idx
                )
                for idx, art in enumerate(artifacts_data)
            ]
            created_artifacts = create_artifacts_batch(db_session, task_id, artifact_creates)
            artifact_count = len(created_artifacts)
            
            # å‘é€ artifact.generated äº‹ä»¶
            for art, created in zip(artifacts_data, created_artifacts):
                artifact_event = event_artifact_generated(
                    task_id=task_id,
                    expert_type=expert_type,
                    artifact_id=created.id,
                    artifact_type=created.type,
                    content=created.content,
                    title=created.title,
                    language=created.language,
                    sort_order=created.sort_order
                )
                event_queue.append({"type": "sse", "event": sse_event_to_string(artifact_event)})
        
        # v3.0: æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸º completed
        if db_session:
            update_subtask_status(
                db_session, 
                task_id, 
                "completed",
                output_result={"content": result.get("output_result", "")},
                duration_ms=duration_ms
            )
        
        # v3.0: å‘é€ task.completed äº‹ä»¶
        completed_event = event_task_completed(
            task_id=task_id,
            expert_type=expert_type,
            description=description,
            output=result.get("output_result", ""),
            duration_ms=duration_ms,
            artifact_count=artifact_count
        )
        event_queue.append({"type": "sse", "event": sse_event_to_string(completed_event)})

        return_dict = {
            "task_list": task_list,
            "expert_results": updated_results,
            "current_task_index": current_index + 1,
            "event_queue": event_queue,
            "__expert_info": { # ä¿ç•™å‰ç«¯å…¼å®¹
                "expert_type": expert_type,
                "description": description,
                "status": "completed",
                "output": result.get("output_result", ""),
                "duration_ms": duration_ms,
            }
        }
        if "artifact" in result:
            return_dict["artifact"] = result["artifact"]

        return return_dict

    except Exception as e:
        print(f"   [ERROR] ä¸“å®¶æ‰§è¡Œå¤±è´¥: {e}")
        current_task["status"] = "failed"
        
        # v3.0: æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸º failed
        if db_session:
            update_subtask_status(
                db_session,
                task_id,
                "failed",
                error_message=str(e)
            )
        
        # v3.0: å‘é€ task.failed äº‹ä»¶
        failed_event = event_task_failed(
            task_id=task_id,
            expert_type=expert_type,
            description=description,
            error=str(e)
        )
        event_queue.append({"type": "sse", "event": sse_event_to_string(failed_event)})
        
        return {
            "task_list": task_list,
            "current_task_index": current_index + 1,
            "event_queue": event_queue,
            "__expert_info": {
                "expert_type": expert_type,
                "description": description,
                "status": "failed",
                "error": str(e),
                "duration_ms": 0,
            }
        }

# --- v3.0: Aggregator èŠ‚ç‚¹ï¼ˆæ”¯æŒæµå¼è¾“å‡ºå’Œäº‹ä»¶å‘é€ï¼‰---
async def aggregator_node(state: AgentState) -> Dict[str, Any]:
    """
    èšåˆå™¨èŠ‚ç‚¹
    v3.0 æ›´æ–°ï¼šæµå¼è¾“å‡ºæœ€ç»ˆå›å¤ï¼Œå‘é€ message.delta/done äº‹ä»¶
    """
    expert_results = state["expert_results"]
    strategy = state["strategy"]
    
    # è·å–æ•°æ®åº“ä¼šè¯
    db_session = state.get("db_session")
    task_session_id = state.get("task_session_id")
    event_queue = state.get("event_queue", [])

    if not expert_results:
        return {"final_response": "æœªç”Ÿæˆä»»ä½•æ‰§è¡Œç»“æœã€‚", "event_queue": event_queue}

    print(f"[AGG] æ­£åœ¨èšåˆ {len(expert_results)} ä¸ªç»“æœ...")
    
    # æ„å»ºæœ€ç»ˆå›å¤ï¼ˆè¿™é‡Œå¯ä»¥è°ƒç”¨ LLM ç”Ÿæˆæ›´è‡ªç„¶çš„æ€»ç»“ï¼‰
    final_response = _build_markdown_response(expert_results, strategy)
    
    # v3.0: æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ˆå°†æœ€ç»ˆå›å¤åˆ†å—å‘é€ï¼‰
    message_id = str(uuid4())
    chunk_size = 50  # æ¯å—å­—ç¬¦æ•°
    
    for i in range(0, len(final_response), chunk_size):
        chunk = final_response[i:i + chunk_size]
        is_final = (i + chunk_size) >= len(final_response)
        
        # å‘é€ message.delta äº‹ä»¶
        delta_event = event_message_delta(
            message_id=message_id,
            content=chunk,
            is_final=is_final
        )
        event_queue.append({"type": "sse", "event": sse_event_to_string(delta_event)})
    
    # å‘é€ message.done äº‹ä»¶
    done_event = event_message_done(
        message_id=message_id,
        full_content=final_response
    )
    event_queue.append({"type": "sse", "event": sse_event_to_string(done_event)})
    
    # v3.0: æ›´æ–°ä»»åŠ¡ä¼šè¯çŠ¶æ€ä¸º completed
    if db_session and task_session_id:
        update_task_session_status(
            db_session,
            task_session_id,
            "completed",
            final_response=final_response
        )
    
    print(f"[AGG] èšåˆå®Œæˆï¼Œå›å¤é•¿åº¦: {len(final_response)}")

    return {
        "final_response": final_response,
        "event_queue": event_queue
    }

def _build_markdown_response(expert_results: List[Dict[str, Any]], strategy: str) -> str:
    # ç®€å•çš„ Markdown æ„å»ºé€»è¾‘
    lines = [f"# æ‰§è¡ŒæŠ¥å‘Š\n**ç­–ç•¥**: {strategy}\n---"]
    for i, res in enumerate(expert_results, 1):
        lines.append(f"## {i}. {res['expert_type'].upper()}: {res['description']}")
        lines.append(f"{res['output']}\n")
    return "\n".join(lines)

# ============================================================================
# 4. æ¡ä»¶è·¯ç”±é€»è¾‘ (Edges)
# ============================================================================

def route_router(state: AgentState) -> str:
    """Router ä¹‹åçš„å»å‘"""
    decision = state.get("router_decision", "complex")

    print(f"[ROUTE_ROUTER] å†³ç­–: {decision}, å°†è·¯ç”±åˆ°: {'direct_reply' if decision == 'simple' else 'planner'}")

    if decision == "simple":
        # Simple æ¨¡å¼è¿›å…¥ direct_reply èŠ‚ç‚¹
        return "direct_reply"
    else:
        # Complex æ¨¡å¼è¿›å…¥è§„åˆ’å™¨
        return "planner"

def route_dispatcher(state: AgentState) -> str:
    """å†³å®š åˆ†å‘å™¨ ä¹‹åçš„å»å‘ï¼ˆå¾ªç¯æˆ–èšåˆï¼‰"""
    if state["current_task_index"] >= len(state["task_list"]):
        return "aggregator"
    return "expert_dispatcher"

# ============================================================================
# 5. æ„å»ºå·¥ä½œæµå›¾
# ============================================================================

def create_smart_router_workflow() -> StateGraph:
    workflow = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("router", router_node)
    workflow.add_node("direct_reply", direct_reply_node)  # æ–°å¢ï¼šSimple æ¨¡å¼æµå¼å›å¤
    workflow.add_node("planner", planner_node)
    workflow.add_node("expert_dispatcher", expert_dispatcher_node)
    workflow.add_node("aggregator", aggregator_node)

    # è®¾ç½®å…¥å£ï¼šç°åœ¨å…¥å£æ˜¯ Routerï¼
    workflow.set_entry_point("router")

    # æ·»åŠ è¿çº¿

    # 1. Router -> (Direct Reply | Planner)
    workflow.add_conditional_edges(
        "router",
        route_router,
        {
            "direct_reply": "direct_reply",
            "planner": "planner"
        }
    )

    # 2. Direct Reply -> END
    workflow.add_edge("direct_reply", END)

    # 2. Planner -> Dispatcher (è§„åˆ’å®Œå¿…ç„¶æ‰§è¡Œ)
    workflow.add_edge("planner", "expert_dispatcher")

    # 3. Dispatcher -> (Loop | Aggregator)
    workflow.add_conditional_edges(
        "expert_dispatcher",
        route_dispatcher,
        {
            "expert_dispatcher": "expert_dispatcher",
            "aggregator": "aggregator"
        }
    )

    # 4. Aggregator -> END
    workflow.add_edge("aggregator", END)

    return workflow.compile()

# å¯¼å‡ºç¼–è¯‘åçš„å›¾
commander_graph = create_smart_router_workflow()

# ============================================================================
# æµ‹è¯•å°è£…å‡½æ•°
# ============================================================================

async def execute_commander_workflow(user_query: str) -> Dict[str, Any]:
    print(f"--- [START] æŸ¥è¯¢: {user_query} ---")
    initial_state: AgentState = {
        "messages": [HumanMessage(content=user_query)],
        "task_list": [],
        "current_task_index": 0,
        "strategy": "",
        "expert_results": [],
        "final_response": ""
    }
    final_state = await commander_graph.ainvoke(initial_state)
    print("--- [DONE] ---")
    return final_state

if __name__ == "__main__":
    import asyncio
    async def test():
        # æµ‹è¯• 1: ç®€å•é—²èŠ
        print("\n=== æµ‹è¯• 1: ç®€å•æ¨¡å¼ ===")
        await execute_commander_workflow("ä½ å¥½ï¼Œåœ¨å—ï¼Ÿ")
        
        # æµ‹è¯• 2: å¤æ‚ä»»åŠ¡
        print("\n=== æµ‹è¯• 2: å¤æ‚æ¨¡å¼ ===")
        await execute_commander_workflow("å¸®æˆ‘å†™ä¸€ä¸ª Python è„šæœ¬æ¥æŠ“å–è‚¡ç¥¨ä»·æ ¼ã€‚")
    
    asyncio.run(test())