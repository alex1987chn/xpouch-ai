"""
XPouch AI æ™ºèƒ½è·¯ç”±å·¥ä½œæµ (v2.7 æ¶æ„)
é›†æˆæ„å›¾è¯†åˆ« (Router) -> ä»»åŠ¡è§„åˆ’ (Planner) -> ä¸“å®¶æ‰§è¡Œ (Experts)
"""
from typing import TypedDict, Annotated, List, Dict, Any, Literal
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser
import os
from dotenv import load_dotenv
import pathlib
from pydantic import BaseModel, Field
from uuid import uuid4
from datetime import datetime

# å¯¼å…¥æ•°æ®æ¨¡å‹
import sys
sys.path.append(str(pathlib.Path(__file__).parent.parent))
from models import ExpertType, TaskStatus, SubTask
from config import init_langchain_tracing, get_langsmith_config
from utils.json_parser import parse_llm_json
from utils.exceptions import AppError
# å°†åŸæœ‰çš„ COMMANDER_SYSTEM_PROMPT ä½œä¸ºè§„åˆ’å™¨ (Planner) çš„æç¤ºè¯
from constants import COMMANDER_SYSTEM_PROMPT as PLANNER_SYSTEM_PROMPT, ROUTER_SYSTEM_PROMPT, DEFAULT_ASSISTANT_PROMPT 
from agents.dynamic_experts import DYNAMIC_EXPERT_FUNCTIONS, initialize_expert_cache
from agents.expert_loader import get_expert_config_cached

# ============================================================================
# 0. è®¾ç½®ä¸é…ç½®
# ============================================================================
env_path = pathlib.Path(__file__).parent.parent / ".env"
load_dotenv(dotenv_path=env_path)

api_key = os.getenv("OPENAI_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
base_url = os.getenv("OPENAI_BASE_URL") or os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")
model_name = os.getenv("MODEL_NAME", "deepseek-chat")

# LangSmith é“¾è·¯è¿½è¸ª
langsmith_config = get_langsmith_config()
if langsmith_config["enabled"]:
    init_langchain_tracing(langsmith_config)

# åˆå§‹åŒ– LLM
# å»ºè®®ï¼šå¦‚æœå¯èƒ½ï¼ŒRouter å¯ä»¥ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ï¼ˆå¦‚ gpt-4o-miniï¼‰ï¼Œè¿™é‡Œæš‚æ—¶å¤ç”¨ä¸»é…ç½®
llm = ChatOpenAI(
    model=model_name,
    temperature=0.3, # Router éœ€è¦æ›´ç¡®å®šçš„è¾“å‡ºï¼Œç¨å¾®é™ä½æ¸©åº¦
    api_key=api_key,
    base_url=base_url,
    streaming=True
)

# ============================================================================
# 1. ç»“æ„å®šä¹‰ä¸æç¤ºè¯ (æ–°çš„ Router é€»è¾‘)
# ============================================================================
# ROUTER_SYSTEM_PROMPT å·²ä» constants.py å¯¼å…¥

class RoutingDecision(BaseModel):
    """v2.7 ç½‘å…³å†³ç­–ç»“æ„ï¼ˆRouteråªè´Ÿè´£åˆ†ç±»ï¼‰"""
    decision_type: Literal["simple", "complex"] = Field(description="å†³ç­–ç±»å‹")

# --- ä¿ç•™åŸæœ‰çš„è§„åˆ’å™¨ç»“æ„ (åŸ CommanderOutput) ---

class SubTaskOutput(BaseModel):
    """å•ä¸ªå­ä»»åŠ¡ç»“æ„ (Planner ä½¿ç”¨)"""
    expert_type: ExpertType = Field(description="æ‰§è¡Œæ­¤ä»»åŠ¡çš„ä¸“å®¶ç±»å‹")
    description: str = Field(description="ä»»åŠ¡æè¿°")
    input_data: Dict[str, Any] = Field(default={}, description="è¾“å…¥å‚æ•°")
    priority: int = Field(default=0, description="ä¼˜å…ˆçº§ (0=æœ€é«˜)")

class PlannerOutput(BaseModel):
    """è§„åˆ’å™¨è¾“å‡º - å­ä»»åŠ¡åˆ—è¡¨ (åŸ CommanderOutput)"""
    tasks: List[SubTaskOutput] = Field(description="å­ä»»åŠ¡åˆ—è¡¨")
    strategy: str = Field(description="æ‰§è¡Œç­–ç•¥æ¦‚è¿°")
    estimated_steps: int = Field(description="é¢„è®¡æ­¥éª¤æ•°")

# ============================================================================
# 2. çŠ¶æ€å®šä¹‰
# ============================================================================

class AgentState(TypedDict):
    """è¶…æ™ºèƒ½ä½“çš„å…¨å±€çŠ¶æ€"""
    messages: Annotated[List[BaseMessage], add_messages]
    task_list: List[Dict[str, Any]]
    current_task_index: int
    strategy: str
    expert_results: List[Dict[str, Any]]
    final_response: str
    # æ–°å¢ï¼šè®°å½•è·¯ç”±å†³ç­–ä¿¡æ¯
    router_decision: str 

# ============================================================================
# 3. èŠ‚ç‚¹å®ç°
# ============================================================================

# --- æ–°å¢ï¼šRouter èŠ‚ç‚¹ (ç½‘å…³) ---
async def router_node(state: AgentState) -> Dict[str, Any]:
    """[ç½‘å…³] åªè´Ÿè´£åˆ†ç±»ï¼Œä¸è´Ÿè´£å›ç­”"""
    messages = state["messages"]

    # æ–­ç‚¹æ¢å¤æ£€æŸ¥
    if state.get("task_list") and len(state.get("task_list", [])) > 0:
        return {"router_decision": "complex"}

    parser = PydanticOutputParser(pydantic_object=RoutingDecision)
    try:
        # ğŸ”¥ å…³é”®ï¼šé™æ€ SystemPrompt + åŠ¨æ€ Messages
        response = await llm.ainvoke(
            [
                SystemMessage(content=ROUTER_SYSTEM_PROMPT),
                *messages  # ç”¨æˆ·çš„è¾“å…¥åœ¨è¿™é‡Œ
            ],
            config={"tags": ["router"]}
        )
        decision = parser.parse(response.content)
        return {"router_decision": decision.decision_type}
    except Exception as e:
        print(f"[ROUTER ERROR] {e}")
        return {"router_decision": "complex"}

# --- æ–°å¢ï¼šDirect Reply èŠ‚ç‚¹ (Simple æ¨¡å¼æµå¼å›ç­”) ---
async def direct_reply_node(state: AgentState) -> Dict[str, Any]:
    """[ç›´è¿èŠ‚ç‚¹] è´Ÿè´£ Simple æ¨¡å¼ä¸‹çš„æµå¼å›å¤"""
    print(f"[DIRECT_REPLY] èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    messages = state["messages"]

    # ä½¿ç”¨æµå¼é…ç½®ï¼Œæ·»åŠ  metadata ä¾¿äºè¿½è¸ª
    config = {"tags": ["direct_reply"], "metadata": {"node_type": "direct_reply"}}
    
    # ç›´æ¥è°ƒç”¨ LLM ç”Ÿæˆå›å¤ (è¿™æ‰æ˜¯çœŸæ­£çš„æµå¼)
    response = await llm.ainvoke(
        [
            SystemMessage(content=DEFAULT_ASSISTANT_PROMPT),
            *messages  # ç”¨æˆ·çš„å†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡
        ],
        config=config
    )

    print(f"[DIRECT_REPLY] èŠ‚ç‚¹å®Œæˆï¼Œå›å¤é•¿åº¦: {len(response.content)}")

    # ç›´æ¥è¿”å› response å¯¹è±¡ï¼ˆä¿ç•™å®Œæ•´å…ƒæ•°æ®ï¼‰ï¼Œå¹¶æ·»åŠ  final_response å­—æ®µ
    return {
        "messages": [response],
        "final_response": response.content
    }

# --- ä¿®æ”¹ï¼šPlanner èŠ‚ç‚¹ (åŸ Commander) ---
async def planner_node(state: AgentState) -> Dict[str, Any]:
    """
    [æ¶æ„å¸ˆ] å°†å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡ã€‚
    ä»…å½“ Router å†³å®š intent="complex" æ—¶è§¦å‘ã€‚
    """
    messages = state["messages"]
    last_message = messages[-1]
    user_query = last_message.content if isinstance(last_message, HumanMessage) else str(last_message.content)
    
    # åŠ è½½é…ç½® (æ•°æ®åº“æˆ–å›é€€)
    # æ³¨æ„ï¼šä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä»ç„¶è¯»å– key="commander" çš„é…ç½®
    commander_config = get_expert_config_cached("commander") 
    
    if not commander_config:
        system_prompt = PLANNER_SYSTEM_PROMPT
        model = model_name  # ğŸ‘ˆ ä½¿ç”¨ä¸ Router ç›¸åŒçš„æ¨¡å‹ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        temperature = 0.5
        print(f"[PLANNER] ä½¿ç”¨é»˜è®¤å›é€€é…ç½®: model={model}")
    else:
        system_prompt = commander_config["system_prompt"]
        model = commander_config["model"]
        temperature = commander_config["temperature"]
        print(f"[PLANNER] åŠ è½½æ•°æ®åº“é…ç½®: model={model}")
    
    # æ‰§è¡Œ LLM è¿›è¡Œè§„åˆ’
    try:
        llm_with_config = llm.bind(model=model, temperature=temperature)
        
        # ğŸ‘ˆ æ·»åŠ  RunnableConfig æ ‡ç­¾ï¼Œä¾¿äºæµå¼è¾“å‡ºè¿‡æ»¤
        from langchain_core.runnables import RunnableConfig
        response = await llm_with_config.ainvoke(
            [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"ç”¨æˆ·æŸ¥è¯¢: {user_query}\n\nè¯·å°†æ­¤æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡åˆ—è¡¨ã€‚")
            ],
            config=RunnableConfig(
                tags=["commander", "planner"],
                metadata={"node_type": "planner"}
            )
        )

        # è§£æ JSON
        planner_response = parse_llm_json(
            response.content,
            PlannerOutput, # ä½¿ç”¨æ–°çš„ Pydantic æ¨¡å‹å
            strict=False,
            clean_markdown=True
        )

        # è½¬æ¢ä¸ºå†…éƒ¨å­—å…¸æ ¼å¼
        task_list = [
            {
                "id": str(uuid4()),
                "expert_type": task.expert_type,
                "description": task.description,
                "input_data": task.input_data,
                "priority": task.priority,
                "status": "pending",
                "output_result": None,
                "started_at": None,
                "completed_at": None,
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat()
            }
            for task in planner_response.tasks
        ]

        print(f"[PLANNER] ç”Ÿæˆäº† {len(task_list)} ä¸ªä»»åŠ¡ã€‚ç­–ç•¥: {planner_response.strategy}")

        return {
            "task_list": task_list,
            "strategy": planner_response.strategy,
            "current_task_index": 0,
            "expert_results": [],
            # å‰ç«¯ç”¨äº UI æ¸²æŸ“çš„å…ƒæ•°æ®
            "__task_plan": {
                "task_count": len(task_list),
                "strategy": planner_response.strategy,
                "estimated_steps": planner_response.estimated_steps,
                "tasks": task_list
            }
        }

    except Exception as e:
        print(f"[ERROR] Planner è§„åˆ’å¤±è´¥: {e}")
        return {
            "task_list": [],
            "strategy": f"Error: {str(e)}",
            "current_task_index": 0
        }

# --- åŸæœ‰ï¼šExpert Dispatcher èŠ‚ç‚¹ (é€»è¾‘ä¸å˜) ---
async def expert_dispatcher_node(state: AgentState) -> Dict[str, Any]:
    task_list = state["task_list"]
    current_index = state["current_task_index"]

    if current_index >= len(task_list):
        return {"expert_results": state["expert_results"]}

    current_task = task_list[current_index]
    expert_type = current_task["expert_type"]
    description = current_task["description"]

    print(f"[EXEC] æ‰§è¡Œä»»åŠ¡ [{current_index + 1}/{len(task_list)}] - {expert_type}: {description}")

    try:
        expert_func = DYNAMIC_EXPERT_FUNCTIONS.get(expert_type)
        if not expert_func:
            raise ValueError(f"æœªçŸ¥çš„ä¸“å®¶ç±»å‹: {expert_type}")

        result = await expert_func(state, llm)

        if "error" in result:
             raise AppError(message=result["error"], code="EXPERT_EXECUTION_ERROR")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        current_task["output_result"] = {"content": result.get("output_result", "")}
        current_task["status"] = result.get("status", "completed")
        current_task["completed_at"] = result.get("completed_at")
        
        # æ·»åŠ åˆ°ç»“æœé›†
        updated_results = state["expert_results"] + [{
            "task_id": current_task["id"],
            "expert_type": expert_type,
            "description": description,
            "output": result.get("output_result", ""),
            "status": result.get("status", "unknown"),
            "duration_ms": result.get("duration_ms", 0)
        }]

        duration = result.get('duration_ms', 0) / 1000
        print(f"   [OK] è€—æ—¶ {duration:.2f}s")

        duration_ms = result.get('duration_ms', 0)
        return_dict = {
            "task_list": task_list,
            "expert_results": updated_results,
            "current_task_index": current_index + 1,
            "__expert_info": { # ç”¨äºå‰ç«¯ SSE äº‹ä»¶
                "expert_type": expert_type,
                "description": description,
                "status": "completed",
                "output": result.get("output_result", ""),
                "duration_ms": duration_ms,
            }
        }
        if "artifact" in result:
            return_dict["artifact"] = result["artifact"]

        return return_dict

    except Exception as e:
        print(f"   [ERROR] ä¸“å®¶æ‰§è¡Œå¤±è´¥: {e}")
        current_task["status"] = "failed"
        return {
            "task_list": task_list,
            "current_task_index": current_index + 1,
            "__expert_info": {
                "expert_type": expert_type,
                "description": description,
                "status": "failed",
                "error": str(e),
                "duration_ms": 0,
            }
        }

# --- åŸæœ‰ï¼šAggregator èŠ‚ç‚¹ (é€»è¾‘ä¸å˜) ---
async def aggregator_node(state: AgentState) -> Dict[str, Any]:
    expert_results = state["expert_results"]
    strategy = state["strategy"]

    if not expert_results:
        return {"final_response": "æœªç”Ÿæˆä»»ä½•æ‰§è¡Œç»“æœã€‚"}

    print(f"[AGG] æ­£åœ¨èšåˆ {len(expert_results)} ä¸ªç»“æœ...")
    final_response = _build_markdown_response(expert_results, strategy)
    return {"final_response": final_response}

def _build_markdown_response(expert_results: List[Dict[str, Any]], strategy: str) -> str:
    # ç®€å•çš„ Markdown æ„å»ºé€»è¾‘
    lines = [f"# æ‰§è¡ŒæŠ¥å‘Š\n**ç­–ç•¥**: {strategy}\n---"]
    for i, res in enumerate(expert_results, 1):
        lines.append(f"## {i}. {res['expert_type'].upper()}: {res['description']}")
        lines.append(f"{res['output']}\n")
    return "\n".join(lines)

# ============================================================================
# 4. æ¡ä»¶è·¯ç”±é€»è¾‘ (Edges)
# ============================================================================

def route_router(state: AgentState) -> str:
    """Router ä¹‹åçš„å»å‘"""
    decision = state.get("router_decision", "complex")

    print(f"[ROUTE_ROUTER] å†³ç­–: {decision}, å°†è·¯ç”±åˆ°: {'direct_reply' if decision == 'simple' else 'planner'}")

    if decision == "simple":
        # Simple æ¨¡å¼è¿›å…¥ direct_reply èŠ‚ç‚¹
        return "direct_reply"
    else:
        # Complex æ¨¡å¼è¿›å…¥è§„åˆ’å™¨
        return "planner"

def route_dispatcher(state: AgentState) -> str:
    """å†³å®š åˆ†å‘å™¨ ä¹‹åçš„å»å‘ï¼ˆå¾ªç¯æˆ–èšåˆï¼‰"""
    if state["current_task_index"] >= len(state["task_list"]):
        return "aggregator"
    return "expert_dispatcher"

# ============================================================================
# 5. æ„å»ºå·¥ä½œæµå›¾
# ============================================================================

def create_smart_router_workflow() -> StateGraph:
    workflow = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("router", router_node)
    workflow.add_node("direct_reply", direct_reply_node)  # æ–°å¢ï¼šSimple æ¨¡å¼æµå¼å›å¤
    workflow.add_node("planner", planner_node)
    workflow.add_node("expert_dispatcher", expert_dispatcher_node)
    workflow.add_node("aggregator", aggregator_node)

    # è®¾ç½®å…¥å£ï¼šç°åœ¨å…¥å£æ˜¯ Routerï¼
    workflow.set_entry_point("router")

    # æ·»åŠ è¿çº¿

    # 1. Router -> (Direct Reply | Planner)
    workflow.add_conditional_edges(
        "router",
        route_router,
        {
            "direct_reply": "direct_reply",
            "planner": "planner"
        }
    )

    # 2. Direct Reply -> END
    workflow.add_edge("direct_reply", END)

    # 2. Planner -> Dispatcher (è§„åˆ’å®Œå¿…ç„¶æ‰§è¡Œ)
    workflow.add_edge("planner", "expert_dispatcher")

    # 3. Dispatcher -> (Loop | Aggregator)
    workflow.add_conditional_edges(
        "expert_dispatcher",
        route_dispatcher,
        {
            "expert_dispatcher": "expert_dispatcher",
            "aggregator": "aggregator"
        }
    )

    # 4. Aggregator -> END
    workflow.add_edge("aggregator", END)

    return workflow.compile()

# å¯¼å‡ºç¼–è¯‘åçš„å›¾
commander_graph = create_smart_router_workflow()

# ============================================================================
# æµ‹è¯•å°è£…å‡½æ•°
# ============================================================================

async def execute_commander_workflow(user_query: str) -> Dict[str, Any]:
    print(f"--- [START] æŸ¥è¯¢: {user_query} ---")
    initial_state: AgentState = {
        "messages": [HumanMessage(content=user_query)],
        "task_list": [],
        "current_task_index": 0,
        "strategy": "",
        "expert_results": [],
        "final_response": ""
    }
    final_state = await commander_graph.ainvoke(initial_state)
    print("--- [DONE] ---")
    return final_state

if __name__ == "__main__":
    import asyncio
    async def test():
        # æµ‹è¯• 1: ç®€å•é—²èŠ
        print("\n=== æµ‹è¯• 1: ç®€å•æ¨¡å¼ ===")
        await execute_commander_workflow("ä½ å¥½ï¼Œåœ¨å—ï¼Ÿ")
        
        # æµ‹è¯• 2: å¤æ‚ä»»åŠ¡
        print("\n=== æµ‹è¯• 2: å¤æ‚æ¨¡å¼ ===")
        await execute_commander_workflow("å¸®æˆ‘å†™ä¸€ä¸ª Python è„šæœ¬æ¥æŠ“å–è‚¡ç¥¨ä»·æ ¼ã€‚")
    
    asyncio.run(test())