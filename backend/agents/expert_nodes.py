"""
ä¸“å®¶èŠ‚ç‚¹ï¼ˆåŠ¨æ€ç‰ˆæœ¬ï¼‰

ä½¿ç”¨æ•°æ®åº“åŠ è½½çš„ Promptï¼Œä¸å†ä¾èµ–ç¡¬ç¼–ç å¸¸é‡
"""
import os
from typing import Dict, Any
from langchain_core.messages import SystemMessage, HumanMessage
from datetime import datetime

from agents.expert_loader import get_expert_config_cached
from agents.experts import EXPERT_DESCRIPTIONS
from utils.llm_factory import get_effective_model, get_default_model


async def run_expert_node(
    expert_key: str,
    state: Dict[str, Any],
    llm
) -> Dict[str, Any]:
    """
    é€šç”¨ä¸“å®¶èŠ‚ç‚¹ï¼šæ ¹æ® expert_key ä»æ•°æ®åº“åŠ è½½é…ç½®

    Args:
        expert_key: ä¸“å®¶ç±»å‹æ ‡è¯†ï¼ˆå¦‚ 'coder', 'search'ï¼‰
        state: å®Œæ•´çš„ AgentState
        llm: LLM å®ä¾‹

    Returns:
        Dict: æ›´æ–°åçš„ AgentStateï¼ˆåŒ…å« output_resultï¼‰
    """
    # ä»æ•°æ®åº“/ç¼“å­˜åŠ è½½ä¸“å®¶é…ç½®
    expert_config = get_expert_config_cached(expert_key)

    if not expert_config:
        # é™çº§ï¼šä½¿ç”¨ç¡¬ç¼–ç  Promptï¼ˆå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼‰
        from agents.experts import EXPERT_PROMPTS
        # ğŸ‘ˆ ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç çš„ gpt-4o
        default_model = get_default_model()
        expert_config = {
            "expert_key": expert_key,
            "name": EXPERT_DESCRIPTIONS.get(expert_key, expert_key),
            "system_prompt": EXPERT_PROMPTS.get(expert_key, ""),
            "model": default_model,
            "temperature": 0.5
        }
        print(f"[ExpertNode] Using fallback config for '{expert_key}': model={default_model}")

    system_prompt = expert_config["system_prompt"]
    # ğŸ‘ˆ åº”ç”¨æ¨¡å‹å…œåº•æœºåˆ¶
    model = get_effective_model(expert_config.get("model"))
    temperature = expert_config["temperature"]

    print(f"[ExpertNode] Running {expert_key} with model={model}, temp={temperature}")

    # è·å–å½“å‰ä»»åŠ¡
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)

    if current_index >= len(task_list):
        return {"error": "æ²¡æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡"}

    current_task = task_list[current_index]
    description = current_task.get("description", "")
    input_data = current_task.get("input_data", {})

    started_at = datetime.now()

    try:
        # ä½¿ç”¨é…ç½®çš„æ¨¡å‹å’Œæ¸©åº¦å‚æ•°
        llm_with_config = llm.bind(
            model=model,
            temperature=temperature
        )

        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ä»»åŠ¡æè¿°: {description}\n\nè¾“å…¥å‚æ•°:\n{format_input_data(input_data)}")
        ])

        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)

        print(f"[{expert_key.upper()}] ä¸“å®¶å®Œæˆ (è€—æ—¶: {duration_ms/1000:.2f}s)")

        result = {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms
        }

        # æ ¹æ®ä¸“å®¶ç±»å‹å’Œå†…å®¹è‡ªåŠ¨ç¡®å®š artifact ç±»å‹
        artifact_type = _detect_artifact_type(response.content, expert_key)

        # æ·»åŠ  artifact
        result["artifact"] = {
            "type": artifact_type,
            "title": f"{expert_config['name']}ç»“æœ",
            "content": response.content,
            "source": f"{expert_key}_expert"
        }

        return result

    except Exception as e:
        print(f"[{expert_key.upper()}] ä¸“å®¶å¤±è´¥: {e}")
        return {
            "output_result": f"{expert_config['name']}å¤±è´¥: {str(e)}",
            "status": "failed",
            "error": str(e),
            "started_at": started_at.isoformat(),
            "completed_at": datetime.now().isoformat()
        }


def _detect_artifact_type(content: str, expert_key: str) -> str:
    """
    æ ¹æ®å†…å®¹è‡ªåŠ¨æ£€æµ‹ artifact ç±»å‹

    ä¼˜å…ˆæ ¹æ®å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼Œå…¶æ¬¡æ ¹æ®ä¸“å®¶ç±»å‹å…œåº•

    Args:
        content: ä¸“å®¶è¾“å‡ºçš„å†…å®¹
        expert_key: ä¸“å®¶ç±»å‹æ ‡è¯†

    Returns:
        str: artifact ç±»å‹ (code | html | markdown | text | search)
    """
    import re
    content_lower = content.lower().strip()

    # 1. HTML æ£€æµ‹ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    # æ£€æµ‹å®Œæ•´çš„ HTML æ–‡æ¡£æˆ–åŒ…å« <html> æ ‡ç­¾çš„å†…å®¹
    if (content_lower.startswith("<!doctype html") or
        content_lower.startswith("<html") or
        ("<html" in content_lower and "</html>" in content_lower)):
        return "html"

    # 2. æ£€æµ‹æ˜¯å¦åŒ…å« HTML ä»£ç å—ï¼ˆ```htmlï¼‰
    html_code_block = re.search(r'```html\n([\s\S]*?)```', content, re.IGNORECASE)
    if html_code_block:
        html_content = html_code_block.group(1).lower().strip()
        if html_content.startswith("<") and (">" in html_content or "</" in html_content):
            return "html"

    # 3. æ£€æµ‹ Markdown æ ¼å¼
    has_markdown = any(marker in content for marker in ['# ', '## ', '### ', '> ', '- ', '* '])
    has_code_block = '```' in content

    # 4. æ ¹æ®ä¸“å®¶ç±»å‹å…œåº•
    artifact_type_map = {
        "coder": "code",
        "writer": "markdown",
        "search": "search",
        "planner": "markdown",
        "researcher": "markdown",
        "analyzer": "markdown",
        "image_analyzer": "text",
    }
    default_type = artifact_type_map.get(expert_key, "text")

    # 5. ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœ coder ç”Ÿæˆäº† markdownï¼Œè¿”å› markdown
    if expert_key == "coder" and has_markdown and not has_code_block:
        return "markdown"

    return default_type


def format_input_data(input_data: Dict) -> str:
    """æ ¼å¼åŒ–è¾“å…¥æ•°æ®ä¸ºæ–‡æœ¬"""
    if not input_data:
        return "ï¼ˆæ— é¢å¤–å‚æ•°ï¼‰"

    lines = []
    for key, value in input_data.items():
        if isinstance(value, (list, dict)):
            lines.append(f"- {key}: {value}")
        else:
            lines.append(f"- {key}: {value}")

    return "\n".join(lines)
