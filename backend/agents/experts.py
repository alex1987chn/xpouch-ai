"""
XPouch AI åŠ¨æ€ä¸“å®¶æ‰§è¡Œæ¨¡å— (v2.7 æ¶æ„ - æ•°æ®åº“é©±åŠ¨ç‰ˆ)
å®ç°å„ä¸ªå‚ç›´é¢†åŸŸä¸“å®¶çš„å…·ä½“æ‰§è¡Œé€»è¾‘ï¼Œä¼˜å…ˆè¯»å–æ•°æ®åº“é…ç½®ï¼Œæ”¯æŒç¡¬ç¼–ç å›é€€ã€‚
"""
from typing import Dict, Any, Tuple, Optional
from datetime import datetime
import re
from langchain_core.messages import SystemMessage, HumanMessage
from agents.expert_loader import get_expert_config_cached
from constants import EXPERT_DESCRIPTIONS, EXPERT_PROMPTS
from utils.llm_factory import get_effective_model, get_default_model

# ============================================================================
# ğŸ› ï¸ æ ¸å¿ƒè¾…åŠ©å‡½æ•°ï¼šè·å–ä¸“å®¶é…ç½®
# ============================================================================

def _get_expert_settings(expert_key: str) -> Tuple[str, str, float]:
    """
    è·å–ä¸“å®¶çš„è¿è¡Œæ—¶é…ç½® (Prompt, Model, Temperature)
    ä¼˜å…ˆçº§ï¼šæ•°æ®åº“åŠ¨æ€é…ç½® > ç¡¬ç¼–ç é»˜è®¤å€¼
    æ¨¡å‹å…œåº•ï¼šå¦‚æœé…ç½®çš„æ¨¡å‹ä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸ºç¯å¢ƒå˜é‡ä¸­çš„é»˜è®¤æ¨¡å‹
    """
    # 1. å°è¯•è¯»åº“
    db_config = get_expert_config_cached(expert_key)

    if db_config:
        # æ•°æ®åº“ä¸­æœ‰é…ç½®
        prompt = db_config.get("system_prompt") or EXPERT_PROMPTS.get(expert_key, "")
        # åº”ç”¨æ¨¡å‹å…œåº•æœºåˆ¶
        model = get_effective_model(db_config.get("model"))
        temp = db_config.get("temperature", 0.5)
        return prompt, model, temp

    # 2. å›é€€åˆ°ç¡¬ç¼–ç 
    prompt = EXPERT_PROMPTS.get(expert_key, "You are a helpful assistant.")
    return prompt, get_default_model(), 0.5

def format_input_data(data: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–è¾“å…¥æ•°æ®"""
    if not data: return "æ— "
    return "\n".join([f"  - {k}: {v}" for k, v in data.items()])

def extract_code_blocks(content: str) -> Optional[tuple[str, str]]:
    """æå–ä»£ç å— (ç”¨äº Coder ä¸“å®¶)"""
    pattern = r'```(\w+)?\n(.*?)\n```'
    matches = re.findall(pattern, content, re.DOTALL)
    if matches:
        lang, code = matches[0]
        return (lang or 'text', code.strip())
    return None

# v3.1: æ–°å¢è¾…åŠ©å‡½æ•° - æ„å»ºå¸¦ä¾èµ–ä¸Šä¸‹æ–‡çš„ä¸“å®¶ Prompt
def _build_expert_prompt(current_task: Dict[str, Any]) -> str:
    """
    æ„å»ºä¸“å®¶æ‰§è¡Œçš„ HumanMessage å†…å®¹
    æ”¯æŒä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥ï¼ˆé€šè¿‡ __dependency_context å­—æ®µï¼‰
    
    å…³é”®æ”¹è¿›ï¼šæ˜ç¡®æŒ‡ç¤ºä¸“å®¶å¿…é¡»åŸºäºå‰ç½®ä»»åŠ¡è¾“å‡ºæ‰§è¡Œå½“å‰ä»»åŠ¡
    
    Args:
        current_task: å½“å‰ä»»åŠ¡å­—å…¸
        
    Returns:
        str: æ„é€ å¥½çš„ Prompt å†…å®¹
    """
    description = current_task.get("description", "")
    input_data = current_task.get("input_data", {})
    
    # æå–ä¾èµ–ä¸Šä¸‹æ–‡ï¼ˆå†…éƒ¨å­—æ®µï¼Œä¸æš´éœ²ç»™ LLMï¼‰
    dependency_context = input_data.get("__dependency_context", "")
    # æ¸…ç† input_dataï¼Œç§»é™¤å†…éƒ¨å­—æ®µ
    clean_input_data = {k: v for k, v in input_data.items() if not k.startswith("__")}
    
    if dependency_context:
        return f"""ã€é‡è¦ã€‘ä½ å¿…é¡»åŸºäºä»¥ä¸‹å‰ç½®ä»»åŠ¡çš„è¾“å‡ºç»“æœæ¥å®Œæˆå½“å‰ä»»åŠ¡ã€‚ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œå¿…é¡»ä»æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æå–å…³é”®æ•°æ®ã€‚

å‰ç½®ä»»åŠ¡è¾“å‡ºï¼ˆè¿™æ˜¯ä½ å”¯ä¸€çš„ä¿¡æ¯æ¥æºï¼‰ï¼š
{dependency_context}

---

å½“å‰ä»»åŠ¡æŒ‡ä»¤: {description}

é™„åŠ è¾“å…¥å‚æ•°:
{format_input_data(clean_input_data)}

---

âš ï¸ æ‰§è¡Œè¦æ±‚ï¼š
1. ä½ å¿…é¡»å¼•ç”¨å¹¶ä½¿ç”¨å‰ç½®ä»»åŠ¡è¾“å‡ºä¸­çš„å…·ä½“æ•°æ®
2. å¦‚æœå‰ç½®ä»»åŠ¡æä¾›äº†å¤šä¸ªé€‰é¡¹/æ•°æ®ç‚¹ï¼Œè¯·æ˜ç¡®è¯´æ˜ä½ ä½¿ç”¨äº†å“ªä¸€ä¸ª
3. ä¸è¦è¿”å›å ä½ç¬¦ï¼ˆå¦‚"[è¯·åœ¨æ­¤å¤„æ’å…¥...]"ï¼‰ï¼Œå¿…é¡»å¡«å…¥å®é™…ä»å‰ç½®è¾“å‡ºä¸­æå–çš„å†…å®¹"""
    else:
        return f"""ä»»åŠ¡æè¿°: {description}

è¾“å…¥å‚æ•°:
{format_input_data(clean_input_data)}"""

# ============================================================================
# ğŸ” Search Expert
# ============================================================================

async def run_search_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list): return {"error": "æ— ä»»åŠ¡"}
    
    current_task = task_list[current_index]
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("search")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        # v3.1: ä½¿ç”¨æ–°çš„ Prompt æ„å»ºå‡½æ•°ï¼Œæ”¯æŒä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥
        human_prompt = _build_expert_prompt(current_task)
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ])

        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)
        print(f"[SEARCH] å®Œæˆ ({duration_ms}ms)")

        result = {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms,
            "artifact": {
                "type": "text",
                "title": "æœç´¢ç»“æœ",
                "content": response.content,
                "source": "search_expert"
            }
        }
        return result
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ’» Coder Expert
# ============================================================================

async def run_coder_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list): return {"error": "æ— ä»»åŠ¡"}

    current_task = task_list[current_index]
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("coder")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        # v3.1: ä½¿ç”¨æ–°çš„ Prompt æ„å»ºå‡½æ•°ï¼Œæ”¯æŒä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥
        human_prompt = _build_expert_prompt(current_task)
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ])

        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)

        # Artifact å¤„ç†é€»è¾‘ä¿æŒä¸å˜
        artifact_data = None
        code_result = extract_code_blocks(response.content)
        
        if code_result:
            lang, code = code_result
            lang_lower = lang.lower() if lang else ""
            artifact_type = "code"
            
            if lang_lower in ["html", "htm", "svg"]: artifact_type = "html"
            elif lang_lower in ["md", "markdown"]: artifact_type = "markdown"

            artifact_data = {
                "type": artifact_type,
                "language": lang,
                "title": f"{lang} ä»£ç ",
                "content": code,
                "source": "coder_expert"
            }

        result = {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms
        }
        if artifact_data:
            result["artifact"] = artifact_data
            
        return result
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ”¬ Researcher Expert
# ============================================================================

async def run_researcher_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list):
        return {"error": "æ— ä»»åŠ¡", "status": "failed"}
    current_task = task_list[current_index]
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("researcher")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        # v3.1: ä½¿ç”¨æ–°çš„ Prompt æ„å»ºå‡½æ•°ï¼Œæ”¯æŒä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥
        human_prompt = _build_expert_prompt(current_task)
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ])
        
        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)

        return {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms,
            "artifact": {
                "type": "text",
                "title": "ç ”ç©¶æŠ¥å‘Š",
                "content": response.content,
                "source": "researcher_expert"
            }
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ“Š Analyzer Expert
# ============================================================================

async def run_analyzer_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list):
        return {"error": "æ— ä»»åŠ¡", "status": "failed"}
    current_task = task_list[current_index]
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("analyzer")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        # v3.1: ä½¿ç”¨æ–°çš„ Prompt æ„å»ºå‡½æ•°ï¼Œæ”¯æŒä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥
        human_prompt = _build_expert_prompt(current_task)
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ])
        
        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)

        return {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms,
            "artifact": {
                "type": "text",
                "title": "åˆ†æç»“æœ",
                "content": response.content,
                "source": "analyzer_expert"
            }
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ“ Writer Expert
# ============================================================================

async def run_writer_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list):
        return {"error": "æ— ä»»åŠ¡", "status": "failed"}
    current_task = task_list[current_index]
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("writer")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        # v3.1: ä½¿ç”¨æ–°çš„ Prompt æ„å»ºå‡½æ•°ï¼Œæ”¯æŒä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥
        human_prompt = _build_expert_prompt(current_task)
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ])
        
        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)

        return {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms,
            "artifact": {
                "type": "markdown",
                "title": "å†™ä½œå†…å®¹",
                "content": response.content,
                "source": "writer_expert"
            }
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ§  Planner Expert (å­ä»»åŠ¡æ¨¡å¼)
# ============================================================================

async def run_planner_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    # Planner é€šå¸¸ä½œä¸º graph.py çš„ä¸€éƒ¨åˆ†ï¼Œä½†å¦‚æœä½œä¸ºå­ä»»åŠ¡è°ƒç”¨ï¼Œé€»è¾‘å¦‚ä¸‹
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list):
        return {"error": "æ— ä»»åŠ¡", "status": "failed"}
    current_task = task_list[current_index]
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½® (æ³¨æ„ key ä»ç„¶æ˜¯ commander ä»¥ä¿æŒå…¼å®¹)
    system_prompt, model, temp = _get_expert_settings("commander")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        # v3.1: ä½¿ç”¨æ–°çš„ Prompt æ„å»ºå‡½æ•°ï¼Œæ”¯æŒä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥
        human_prompt = _build_expert_prompt(current_task)
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ])
        
        completed_at = datetime.now()
        return {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": int((completed_at - started_at).total_seconds() * 1000),
            "artifact": {
                "type": "text",
                "title": "è§„åˆ’æ–¹æ¡ˆ",
                "content": response.content,
                "source": "planner_expert"
            }
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ–¼ï¸ Image Analyzer Expert
# ============================================================================

async def run_image_analyzer_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    current_task = task_list[current_index]
    input_data = current_task.get("input_data", {})
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("image_analyzer")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        # v3.1: æ„å»ºå¸¦ä¾èµ–ä¸Šä¸‹æ–‡çš„ content
        dependency_context = input_data.get("__dependency_context", "")
        clean_input_data = {k: v for k, v in input_data.items() if not k.startswith("__")}
        
        content_parts = []
        if dependency_context:
            content_parts.append(f"å‚è€ƒä¸Šä¸‹æ–‡ï¼ˆå‰ç½®ä»»åŠ¡è¾“å‡ºï¼‰ï¼š\n{dependency_context}\n\n---\n")
        
        content_parts.append(f"ä»»åŠ¡: {current_task.get('description')}")
        if clean_input_data.get("image_url"): 
            content_parts.append(f"\n\nå›¾ç‰‡URL: {clean_input_data['image_url']}")
        
        content = "\n".join(content_parts)

        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=content)
        ])
        
        completed_at = datetime.now()
        return {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": int((completed_at - started_at).total_seconds() * 1000),
            "artifact": {
                "type": "text",
                "title": "å›¾åƒåˆ†æ",
                "content": response.content,
                "source": "image_analyzer_expert"
            }
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ä¸“å®¶æ˜ å°„
# ============================================================================

EXPERT_FUNCTIONS = {
    "search": run_search_expert,
    "coder": run_coder_expert,
    "researcher": run_researcher_expert,
    "analyzer": run_analyzer_expert,
    "writer": run_writer_expert,
    "planner": run_planner_expert,
    "image_analyzer": run_image_analyzer_expert
}

__all__ = [
    "EXPERT_PROMPTS",
    "EXPERT_FUNCTIONS",
    "run_search_expert", "run_coder_expert", "run_researcher_expert",
    "run_analyzer_expert", "run_writer_expert", "run_planner_expert", 
    "run_image_analyzer_expert"
]