"""
XPouch AI åŠ¨æ€ä¸“å®¶æ‰§è¡Œæ¨¡å— (v2.7 æ¶æ„ - æ•°æ®åº“é©±åŠ¨ç‰ˆ)
å®ç°å„ä¸ªå‚ç›´é¢†åŸŸä¸“å®¶çš„å…·ä½“æ‰§è¡Œé€»è¾‘ï¼Œä¼˜å…ˆè¯»å–æ•°æ®åº“é…ç½®ï¼Œæ”¯æŒç¡¬ç¼–ç å›é€€ã€‚
"""
from typing import Dict, Any, Tuple, Optional
from datetime import datetime
import re
from langchain_core.messages import SystemMessage, HumanMessage
from agents.expert_loader import get_expert_config_cached

# ============================================================================
# 0. é»˜è®¤æç¤ºè¯ (ä½œä¸º Fallback)
# ============================================================================

EXPERT_DESCRIPTIONS = {
    "search": "æœç´¢ä¸“å®¶",
    "coder": "ç¼–ç¨‹ä¸“å®¶",
    "researcher": "ç ”ç©¶ä¸“å®¶",
    "analyzer": "åˆ†æä¸“å®¶",
    "writer": "å†™ä½œä¸“å®¶",
    "planner": "è§„åˆ’ä¸“å®¶",
    "image_analyzer": "å›¾ç‰‡åˆ†æä¸“å®¶"
}

# è¿™äº›æ˜¯ç¡¬ç¼–ç çš„é»˜è®¤æç¤ºè¯ï¼Œä»…å½“æ•°æ®åº“æ— é…ç½®æ—¶ä½¿ç”¨
EXPERT_PROMPTS = {
    "search": """ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æœç´¢ä¸“å®¶ã€‚
èŒè´£ï¼šæ ¹æ®ä»»åŠ¡è¦æ±‚æœç´¢ç›¸å…³ä¿¡æ¯ï¼Œæ•´ç†å½’çº³ã€‚
è¾“å‡ºè¦æ±‚ï¼šæ¸…æ™°çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œå…³é”®è¦ç‚¹æç‚¼ã€‚""",

    "coder": """ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹ä¸“å®¶ã€‚
èŒè´£ï¼šç¼–å†™æ¸…æ™°ã€é«˜æ•ˆä¸”éµå¾ªæœ€ä½³å®è·µçš„ä»£ç ã€‚
è¾“å‡ºè¦æ±‚ï¼šå®Œæ•´å¯è¿è¡Œçš„ä»£ç ï¼ŒåŒ…å«å¿…è¦çš„æ³¨é‡Šã€‚""",

    "researcher": """ä½ æ˜¯ä¸€ä¸ªç ”ç©¶ä¸“å®¶ã€‚
èŒè´£ï¼šè¿›è¡Œæ·±å…¥çš„æ–‡çŒ®å’ŒæŠ€æœ¯è°ƒç ”ã€‚
è¾“å‡ºè¦æ±‚ï¼šç³»ç»ŸåŒ–çš„ç ”ç©¶æŠ¥å‘Šï¼Œæ·±åº¦åˆ†æã€‚""",

    "analyzer": """ä½ æ˜¯ä¸€ä¸ªåˆ†æä¸“å®¶ã€‚
èŒè´£ï¼šè¿›è¡Œé€»è¾‘ä¸¥å¯†çš„åˆ†æå’Œæ¨ç†ã€‚
è¾“å‡ºè¦æ±‚ï¼šç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Šï¼Œæ˜ç¡®ç»“è®ºã€‚""",

    "writer": """ä½ æ˜¯ä¸€ä¸ªå†™ä½œä¸“å®¶ã€‚
èŒè´£ï¼šåˆ›ä½œç”ŸåŠ¨ã€ä¼˜ç¾ä¸”æ˜“è¯»çš„å†…å®¹ã€‚
è¾“å‡ºè¦æ±‚ï¼šæ¸…æ™°çš„ç»“æ„ï¼Œå‡†ç¡®çš„è¡¨è¾¾ã€‚""",

    "planner": """ä½ æ˜¯ä¸€ä¸ªè§„åˆ’ä¸“å®¶ã€‚
èŒè´£ï¼šåˆ¶å®šè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’å’Œæ–¹æ¡ˆã€‚
è¾“å‡ºè¦æ±‚ï¼šåˆ†é˜¶æ®µè®¡åˆ’ï¼Œæ˜ç¡®åˆ†å·¥ã€‚""",

    "image_analyzer": """ä½ æ˜¯ä¸€ä¸ªå›¾ç‰‡åˆ†æä¸“å®¶ã€‚
èŒè´£ï¼šåˆ†æå›¾ç‰‡å†…å®¹ï¼Œè¯†åˆ«ç‰©ä½“å’Œåœºæ™¯ã€‚
è¾“å‡ºè¦æ±‚ï¼šè¯¦ç»†çš„è§†è§‰æè¿°ã€‚"""
}

# ============================================================================
# ğŸ› ï¸ æ ¸å¿ƒè¾…åŠ©å‡½æ•°ï¼šè·å–ä¸“å®¶é…ç½®
# ============================================================================

def _get_expert_settings(expert_key: str) -> Tuple[str, str, float]:
    """
    è·å–ä¸“å®¶çš„è¿è¡Œæ—¶é…ç½® (Prompt, Model, Temperature)
    ä¼˜å…ˆçº§ï¼šæ•°æ®åº“åŠ¨æ€é…ç½® > ç¡¬ç¼–ç é»˜è®¤å€¼
    """
    # 1. å°è¯•è¯»åº“
    db_config = get_expert_config_cached(expert_key)
    
    if db_config:
        # æ•°æ®åº“ä¸­æœ‰é…ç½®
        prompt = db_config.get("system_prompt") or EXPERT_PROMPTS.get(expert_key, "")
        model = db_config.get("model", "gpt-4o") # é»˜è®¤æ¨¡å‹
        temp = db_config.get("temperature", 0.5)
        return prompt, model, temp
    
    # 2. å›é€€åˆ°ç¡¬ç¼–ç 
    prompt = EXPERT_PROMPTS.get(expert_key, "You are a helpful assistant.")
    return prompt, "gpt-4o", 0.5

def format_input_data(data: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–è¾“å…¥æ•°æ®"""
    if not data: return "æ— "
    return "\n".join([f"  - {k}: {v}" for k, v in data.items()])

def extract_code_blocks(content: str) -> Optional[tuple[str, str]]:
    """æå–ä»£ç å— (ç”¨äº Coder ä¸“å®¶)"""
    pattern = r'```(\w+)?\n(.*?)\n```'
    matches = re.findall(pattern, content, re.DOTALL)
    if matches:
        lang, code = matches[0]
        return (lang or 'text', code.strip())
    return None

# ============================================================================
# ğŸ” Search Expert
# ============================================================================

async def run_search_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list): return {"error": "æ— ä»»åŠ¡"}
    
    current_task = task_list[current_index]
    description = current_task.get("description", "")
    input_data = current_task.get("input_data", {})
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("search")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ä»»åŠ¡æè¿°: {description}\n\nè¾“å…¥å‚æ•°:\n{format_input_data(input_data)}")
        ])

        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)
        print(f"[SEARCH] å®Œæˆ ({duration_ms}ms)")

        result = {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms,
            "artifact": {
                "type": "text",
                "title": "æœç´¢ç»“æœ",
                "content": response.content,
                "source": "search_expert"
            }
        }
        return result
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ’» Coder Expert
# ============================================================================

async def run_coder_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list): return {"error": "æ— ä»»åŠ¡"}

    current_task = task_list[current_index]
    description = current_task.get("description", "")
    input_data = current_task.get("input_data", {})
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("coder")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ä»»åŠ¡æè¿°: {description}\n\nè¾“å…¥å‚æ•°:\n{format_input_data(input_data)}")
        ])

        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)

        # Artifact å¤„ç†é€»è¾‘ä¿æŒä¸å˜
        artifact_data = None
        code_result = extract_code_blocks(response.content)
        
        if code_result:
            lang, code = code_result
            lang_lower = lang.lower() if lang else ""
            artifact_type = "code"
            
            if lang_lower in ["html", "htm", "svg"]: artifact_type = "html"
            elif lang_lower in ["md", "markdown"]: artifact_type = "markdown"

            artifact_data = {
                "type": artifact_type,
                "language": lang,
                "title": f"{lang} ä»£ç ",
                "content": code,
                "source": "coder_expert"
            }

        result = {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms
        }
        if artifact_data:
            result["artifact"] = artifact_data
            
        return result
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ”¬ Researcher Expert
# ============================================================================

async def run_researcher_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list):
        return {"error": "æ— ä»»åŠ¡", "status": "failed"}
    current_task = task_list[current_index]
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("researcher")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ä»»åŠ¡æè¿°: {current_task.get('description')}\n\nè¾“å…¥:\n{format_input_data(current_task.get('input_data'))}")
        ])
        
        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)

        return {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms,
            "artifact": {
                "type": "text",
                "title": "ç ”ç©¶æŠ¥å‘Š",
                "content": response.content,
                "source": "researcher_expert"
            }
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ“Š Analyzer Expert
# ============================================================================

async def run_analyzer_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list):
        return {"error": "æ— ä»»åŠ¡", "status": "failed"}
    current_task = task_list[current_index]
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("analyzer")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ä»»åŠ¡: {current_task.get('description')}\n\nè¾“å…¥:\n{format_input_data(current_task.get('input_data'))}")
        ])
        
        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)

        return {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms,
            "artifact": {
                "type": "text",
                "title": "åˆ†æç»“æœ",
                "content": response.content,
                "source": "analyzer_expert"
            }
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ“ Writer Expert
# ============================================================================

async def run_writer_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list):
        return {"error": "æ— ä»»åŠ¡", "status": "failed"}
    current_task = task_list[current_index]
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("writer")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ä»»åŠ¡: {current_task.get('description')}\n\nè¾“å…¥:\n{format_input_data(current_task.get('input_data'))}")
        ])
        
        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)

        return {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms,
            "artifact": {
                "type": "markdown",
                "title": "å†™ä½œå†…å®¹",
                "content": response.content,
                "source": "writer_expert"
            }
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ§  Planner Expert (å­ä»»åŠ¡æ¨¡å¼)
# ============================================================================

async def run_planner_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    # Planner é€šå¸¸ä½œä¸º graph.py çš„ä¸€éƒ¨åˆ†ï¼Œä½†å¦‚æœä½œä¸ºå­ä»»åŠ¡è°ƒç”¨ï¼Œé€»è¾‘å¦‚ä¸‹
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    if current_index >= len(task_list):
        return {"error": "æ— ä»»åŠ¡", "status": "failed"}
    current_task = task_list[current_index]
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½® (æ³¨æ„ key ä»ç„¶æ˜¯ commander ä»¥ä¿æŒå…¼å®¹)
    system_prompt, model, temp = _get_expert_settings("commander")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"è§„åˆ’ä»»åŠ¡: {current_task.get('description')}")
        ])
        
        completed_at = datetime.now()
        return {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": int((completed_at - started_at).total_seconds() * 1000),
            "artifact": {
                "type": "text",
                "title": "è§„åˆ’æ–¹æ¡ˆ",
                "content": response.content,
                "source": "planner_expert"
            }
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ğŸ–¼ï¸ Image Analyzer Expert
# ============================================================================

async def run_image_analyzer_expert(state: Dict[str, Any], llm) -> Dict[str, Any]:
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    current_task = task_list[current_index]
    input_data = current_task.get("input_data", {})
    started_at = datetime.now()

    # [NEW] è¯»å–é…ç½®
    system_prompt, model, temp = _get_expert_settings("image_analyzer")
    llm_with_config = llm.bind(model=model, temperature=temp)

    try:
        content = f"ä»»åŠ¡: {current_task.get('description')}"
        if input_data.get("image_url"): content += f"\n\nå›¾ç‰‡URL: {input_data['image_url']}"

        response = await llm_with_config.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=content)
        ])
        
        completed_at = datetime.now()
        return {
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": int((completed_at - started_at).total_seconds() * 1000),
            "artifact": {
                "type": "text",
                "title": "å›¾åƒåˆ†æ",
                "content": response.content,
                "source": "image_analyzer_expert"
            }
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}

# ============================================================================
# ä¸“å®¶æ˜ å°„
# ============================================================================

EXPERT_FUNCTIONS = {
    "search": run_search_expert,
    "coder": run_coder_expert,
    "researcher": run_researcher_expert,
    "analyzer": run_analyzer_expert,
    "writer": run_writer_expert,
    "planner": run_planner_expert,
    "image_analyzer": run_image_analyzer_expert
}

__all__ = [
    "EXPERT_PROMPTS",
    "EXPERT_FUNCTIONS",
    "run_search_expert", "run_coder_expert", "run_researcher_expert",
    "run_analyzer_expert", "run_writer_expert", "run_planner_expert", 
    "run_image_analyzer_expert"
]