"""
Commander èŠ‚ç‚¹ - ä»»åŠ¡è§„åˆ’

å°†å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡ï¼Œæ”¯æŒæ˜¾å¼ä¾èµ–å…³ç³»ï¼ˆDAGï¼‰
"""
import os
from typing import Dict, Any, List, Optional
from datetime import datetime
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.runnables import RunnableConfig
from pydantic import BaseModel, Field, field_validator
from sqlmodel import Session

from agents.state import AgentState
from utils.json_parser import parse_llm_json
from utils.llm_factory import get_llm_instance
from constants import COMMANDER_SYSTEM_PROMPT
from database import engine


class SubTaskOutput(BaseModel):
    """å•ä¸ªå­ä»»åŠ¡ç»“æ„ (Commander ä½¿ç”¨)
    
    æ”¯æŒæ˜¾å¼ä¾èµ–å…³ç³» (DAG)ï¼Œé€šè¿‡ id å’Œ depends_on å®ç°ç²¾å‡†æ•°æ®ç®¡é“
    """
    id: str = Field(default="", description="ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆçŸ­IDï¼Œå¦‚ task_1, task_2ï¼‰")
    expert_type: str = Field(description="æ‰§è¡Œæ­¤ä»»åŠ¡çš„ä¸“å®¶ç±»å‹ï¼ˆå¯ä»¥æ˜¯ç³»ç»Ÿå†…ç½®ä¸“å®¶æˆ–è‡ªå®šä¹‰ä¸“å®¶ï¼‰")
    description: str = Field(description="ä»»åŠ¡æè¿°")
    input_data: Dict[str, Any] = Field(default={}, description="è¾“å…¥å‚æ•°")
    priority: int = Field(default=0, description="ä¼˜å…ˆçº§ (0=æœ€é«˜)")
    depends_on: List[str] = Field(default=[], description="ä¾èµ–çš„ä»»åŠ¡IDåˆ—è¡¨ã€‚å¦‚æœä»»åŠ¡Béœ€è¦ä»»åŠ¡Açš„è¾“å‡ºï¼Œåˆ™å¡«å…¥ ['task_a']")
    
    @field_validator('depends_on', mode='before')
    @classmethod
    def parse_depends_on(cls, v):
        """å…¼å®¹å¤„ç†ï¼šå¦‚æœ LLM è¿”å›äº†æ•´æ•°ä¾èµ–ï¼ˆå¦‚ [0]ï¼‰ï¼Œå¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸² ["0"]"""
        if v is None:
            return []
        
        # æƒ…å†µ 1: LLM å‘ç–¯è¿”äº†ä¸ªå•ä¸ª int/str (ä¸æ˜¯åˆ—è¡¨)
        if isinstance(v, (int, str)):
            return [str(v)]
            
        # æƒ…å†µ 2: æ­£å¸¸çš„åˆ—è¡¨ï¼Œä½†é‡Œé¢æ··äº† int
        if isinstance(v, list):
            return [str(item) for item in v]
            
        return v


class CommanderOutput(BaseModel):
    """æŒ‡æŒ¥å®˜è¾“å‡º - å­ä»»åŠ¡åˆ—è¡¨"""
    tasks: List[SubTaskOutput] = Field(description="å­ä»»åŠ¡åˆ—è¡¨")
    strategy: str = Field(description="æ‰§è¡Œç­–ç•¥æ¦‚è¿°")
    estimated_steps: int = Field(description="é¢„è®¡æ­¥éª¤æ•°")


async def _preload_expert_configs(task_list: List[Dict], db_session: Any) -> None:
    """
    P1 ä¼˜åŒ–: é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®åˆ°ç¼“å­˜
    
    åœ¨ Commander é˜¶æ®µå°±å¹¶è¡ŒåŠ è½½æ‰€æœ‰éœ€è¦çš„ä¸“å®¶é…ç½®ï¼Œ
    é¿å… GenericWorker æ‰§è¡Œæ—¶å†é€ä¸ªæŸ¥è¯¢æ•°æ®åº“ã€‚
    
    Args:
        task_list: ä»»åŠ¡åˆ—è¡¨
        db_session: æ•°æ®åº“ä¼šè¯
    """
    if not task_list or not db_session:
        return
    
    # æå–æ‰€æœ‰å”¯ä¸€çš„ä¸“å®¶ç±»å‹
    expert_types = list(set(task.get("expert_type") for task in task_list if task.get("expert_type")))
    if not expert_types:
        return
    
    print(f"[COMMANDER] P1ä¼˜åŒ–: é¢„åŠ è½½ {len(expert_types)} ä¸ªä¸“å®¶é…ç½®...")
    
    # å¹¶è¡ŒåŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®
    from agents.services.expert_manager import get_expert_config_cached
    
    loaded_count = 0
    for expert_type in expert_types:
        try:
            # å…ˆä»ç¼“å­˜æ£€æŸ¥
            cached = get_expert_config_cached(expert_type)
            if cached:
                loaded_count += 1
                continue
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®åº“åŠ è½½
            from agents.services.expert_manager import get_expert_config
            config = get_expert_config(expert_type, db_session)
            if config:
                loaded_count += 1
        except Exception as e:
            print(f"[COMMANDER] é¢„åŠ è½½ä¸“å®¶ '{expert_type}' å¤±è´¥: {e}")
    
    print(f"[COMMANDER] P1ä¼˜åŒ–: æˆåŠŸé¢„åŠ è½½ {loaded_count}/{len(expert_types)} ä¸ªä¸“å®¶é…ç½®")


async def commander_node(state: AgentState) -> Dict[str, Any]:
    """
    [æŒ‡æŒ¥å®˜] å°†å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡ã€‚
    v3.0 æ›´æ–°ï¼šç«‹å³æŒä¹…åŒ–åˆ°æ•°æ®åº“ï¼Œå‘é€ plan.created äº‹ä»¶
    v3.1 æ›´æ–°ï¼šä½¿ç”¨ç‹¬ç«‹æ•°æ®åº“ä¼šè¯ï¼Œé¿å… MemorySaver åºåˆ—åŒ–é—®é¢˜
    """
    from agents.services.expert_manager import get_expert_config, get_expert_config_cached
    from agents.services.expert_manager import get_all_expert_list, format_expert_list_for_prompt
    from agents.services.task_manager import get_or_create_task_session
    from models import SubTaskCreate
    from utils.event_generator import event_plan_created, sse_event_to_string
    
    messages = state["messages"]
    last_message = messages[-1]
    user_query = last_message.content if isinstance(last_message, HumanMessage) else str(last_message.content)
    
    # è·å– thread_id
    thread_id = state.get("thread_id")
    
    # ğŸ”¥ ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼ˆé¿å… MemorySaver åºåˆ—åŒ–é—®é¢˜ï¼‰
    with Session(engine) as db_session:
        try:
            # åŠ è½½é…ç½® (æ•°æ®åº“æˆ–å›é€€)
            commander_config = get_expert_config("commander", db_session)
            
            # å¦‚æœæ•°æ®åº“è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°ç¼“å­˜
            if not commander_config:
                commander_config = get_expert_config_cached("commander")
            
            if not commander_config:
                # å›é€€ï¼šä½¿ç”¨å¸¸é‡ä¸­çš„ Prompt å’Œç¡¬ç¼–ç çš„æ¨¡å‹
                system_prompt = COMMANDER_SYSTEM_PROMPT
                model = os.getenv("MODEL_NAME", "deepseek-chat")
                temperature = 0.5
                print(f"[COMMANDER] ä½¿ç”¨é»˜è®¤å›é€€é…ç½®: model={model}")
            else:
                # ä½¿ç”¨æ•°æ®åº“é…ç½®
                system_prompt = commander_config["system_prompt"]
                model = commander_config["model"]
                temperature = commander_config["temperature"]
                print(f"[COMMANDER] åŠ è½½é…ç½®: model={model}, temperature={temperature}")
            
            # æ³¨å…¥åŠ¨æ€ä¸“å®¶åˆ—è¡¨åˆ° System Prompt
            try:
                # è·å–æ‰€æœ‰å¯ç”¨ä¸“å®¶ï¼ˆåŒ…æ‹¬åŠ¨æ€åˆ›å»ºçš„ä¸“å®¶ï¼‰
                all_experts = get_all_expert_list(db_session)
                expert_list_str = format_expert_list_for_prompt(all_experts)
                
                # å°è¯•æ³¨å…¥ä¸“å®¶åˆ—è¡¨åˆ° Promptï¼ˆå¦‚æœ Prompt æ”¯æŒåŠ¨æ€å ä½ç¬¦ï¼‰
                if "{dynamic_expert_list}" in system_prompt:
                    system_prompt = system_prompt.format(dynamic_expert_list=expert_list_str)
                    print(f"[COMMANDER] å·²æ³¨å…¥åŠ¨æ€ä¸“å®¶åˆ—è¡¨ï¼Œå…± {len(all_experts)} ä¸ªä¸“å®¶")
                else:
                    # å¦‚æœ Prompt ä¸åŒ…å«å ä½ç¬¦ï¼Œä¿ç•™åŸæœ‰é€»è¾‘ï¼ˆå‘åå…¼å®¹ï¼‰
                    print(f"[COMMANDER] Prompt ä¸åŒ…å«åŠ¨æ€å ä½ç¬¦ï¼Œè·³è¿‡ä¸“å®¶åˆ—è¡¨æ³¨å…¥")
            except Exception as e:
                # æ³¨å…¥å¤±è´¥æ—¶ä¸ä¸­æ–­æµç¨‹ï¼Œä¿ç•™åŸå§‹ Prompt
                print(f"[COMMANDER] ä¸“å®¶åˆ—è¡¨æ³¨å…¥å¤±è´¥ï¼ˆå·²å¿½ç•¥ï¼‰: {e}")
            
            # æ‰§è¡Œ LLM è¿›è¡Œè§„åˆ’
            # ä»æ¨¡å‹åç§°æ¨æ–­ provider
            from providers_config import get_model_config
            from agents.graph import get_commander_llm_lazy

            model_config = get_model_config(model)

            if model_config and 'provider' in model_config:
                # ä½¿ç”¨æ¨æ–­å‡ºçš„ provider åˆ›å»º LLM
                provider = model_config['provider']
                # ä¼˜å…ˆä½¿ç”¨æ¨¡å‹é…ç½®ä¸­çš„ temperatureï¼ˆå¦‚æœæœ‰ï¼‰
                final_temperature = model_config.get('temperature', temperature)
                # è·å–å®é™…çš„ API æ¨¡å‹åç§°ï¼ˆproviders.yaml ä¸­å®šä¹‰çš„ model å­—æ®µï¼‰
                actual_model = model_config.get('model', model)
                llm = get_llm_instance(
                    provider=provider,
                    streaming=True,
                    temperature=final_temperature
                )
                print(f"[COMMANDER] æ¨¡å‹ '{model}' -> '{actual_model}' ä½¿ç”¨ provider: {provider}, temperature: {final_temperature}")
                llm_with_config = llm.bind(model=actual_model, temperature=final_temperature)
            else:
                # å›é€€åˆ° commander_llmï¼ˆç¡¬ç¼–ç çš„ provider ä¼˜å…ˆçº§ï¼‰
                print(f"[COMMANDER] æ¨¡å‹ '{model}' æœªæ‰¾åˆ° provider é…ç½®ï¼Œå›é€€åˆ° commander_llm")
                llm_with_config = get_commander_llm_lazy().bind(model=model, temperature=temperature)

            response = await llm_with_config.ainvoke(
                [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=f"ç”¨æˆ·æŸ¥è¯¢: {user_query}\n\nè¯·å°†æ­¤æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡åˆ—è¡¨ã€‚")
                ],
                config=RunnableConfig(
                    tags=["commander"],
                    metadata={"node_type": "commander"}
                )
            )

            # è§£æ JSON
            commander_response = parse_llm_json(
                response.content,
                CommanderOutput,
                strict=False,
                clean_markdown=True
            )

            # v3.1: å…œåº•å¤„ç† - å¦‚æœ LLM æ²¡æœ‰ç”Ÿæˆ idï¼Œè‡ªåŠ¨ç”Ÿæˆ
            for idx, task in enumerate(commander_response.tasks):
                if not task.id:
                    task.id = f"task_{idx}"
                    print(f"[COMMANDER] è‡ªåŠ¨ä¸ºä»»åŠ¡ {idx} ç”Ÿæˆ id: {task.id}")
            
            # v3.2: ä¿®å¤ä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥ - å°† depends_on ä¸­çš„ç´¢å¼•æ ¼å¼è½¬æ¢ä¸º ID æ ¼å¼
            task_id_map = {str(idx): task.id for idx, task in enumerate(commander_response.tasks)}
            for task in commander_response.tasks:
                if task.depends_on:
                    new_depends_on = []
                    for dep in task.depends_on:
                        # å¦‚æœæ˜¯æ•°å­—ç´¢å¼•ï¼ˆå¦‚ "0"ï¼‰ï¼Œè½¬æ¢ä¸ºå¯¹åº”çš„ IDï¼ˆå¦‚ "task_0"ï¼‰
                        if dep in task_id_map:
                            new_depends_on.append(task_id_map[dep])
                        else:
                            # å¦‚æœå·²ç»æ˜¯æ­£ç¡®çš„ ID æ ¼å¼ï¼ˆå¦‚ "task_0"ï¼‰ï¼Œä¿æŒä¸å˜
                            new_depends_on.append(dep)
                    task.depends_on = new_depends_on
                    print(f"[COMMANDER] ä»»åŠ¡ {task.id} çš„ä¾èµ–å·²è½¬æ¢: {new_depends_on}")

            # v3.0: å‡†å¤‡å­ä»»åŠ¡æ•°æ®ï¼ˆæ”¯æŒæ˜¾å¼ä¾èµ–å…³ç³» DAGï¼‰
            subtasks_data = [
                SubTaskCreate(
                    expert_type=task.expert_type,
                    task_description=task.description,
                    input_data=task.input_data,
                    sort_order=idx,
                    execution_mode="sequential",
                    depends_on=task.depends_on if task.depends_on else None
                )
                for idx, task in enumerate(commander_response.tasks)
            ]

            # v3.0: ç«‹å³æŒä¹…åŒ–åˆ°æ•°æ®åº“ (é€šè¿‡ TaskManager)
            task_session = None
            if db_session and thread_id:
                task_session, is_reused = get_or_create_task_session(
                    db=db_session,
                    thread_id=thread_id,
                    user_query=user_query,
                    plan_summary=commander_response.strategy,
                    estimated_steps=commander_response.estimated_steps,
                    subtasks_data=subtasks_data,
                    execution_mode="sequential"
                )
                session_source = "å¤ç”¨" if is_reused else "æ–°å»º"
                print(f"[COMMANDER] TaskSession {session_source}: {task_session.session_id}")

            # è½¬æ¢ä¸ºå†…éƒ¨å­—å…¸æ ¼å¼ï¼ˆç”¨äº LangGraph çŠ¶æ€æµè½¬ï¼‰
            sub_tasks_list = task_session.sub_tasks if task_session else []
            task_list = []
            for idx, subtask in enumerate(sub_tasks_list):
                commander_task = commander_response.tasks[idx]
                task_list.append({
                    "id": subtask.id,
                    "task_id": commander_task.id,
                    "expert_type": subtask.expert_type,
                    "description": subtask.task_description,
                    "input_data": subtask.input_data,
                    "sort_order": subtask.sort_order,
                    "status": subtask.status,
                    "depends_on": commander_task.depends_on if commander_task.depends_on else [],
                    "output_result": None,
                    "started_at": None,
                    "completed_at": None
                })

            print(f"[COMMANDER] ç”Ÿæˆäº† {len(task_list)} ä¸ªä»»åŠ¡ã€‚ç­–ç•¥: {commander_response.strategy}")

            # P1 ä¼˜åŒ–: é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®åˆ°ç¼“å­˜
            await _preload_expert_configs(task_list, db_session)

            # v3.0: æ„å»ºäº‹ä»¶é˜Ÿåˆ—
            event_queue = []
            
            # å‘é€ plan.created äº‹ä»¶
            if task_session:
                plan_event = event_plan_created(
                    session_id=task_session.session_id,
                    summary=commander_response.strategy,
                    estimated_steps=commander_response.estimated_steps,
                    execution_mode="sequential",
                    tasks=[
                        {
                            "id": t.id,
                            "task_id": commander_response.tasks[idx].id,
                            "expert_type": t.expert_type,
                            "description": t.task_description,
                            "sort_order": t.sort_order,
                            "status": t.status,
                            "depends_on": commander_response.tasks[idx].depends_on if commander_response.tasks[idx].depends_on else []
                        }
                        for idx, t in enumerate(task_session.sub_tasks)
                    ]
                )
                event_queue.append({"type": "sse", "event": sse_event_to_string(plan_event)})

            return {
                "task_list": task_list,
                "strategy": commander_response.strategy,
                "current_task_index": 0,
                "expert_results": [],
                "task_session_id": task_session.session_id if task_session else None,
                "event_queue": event_queue,
                # ä¿ç•™å‰ç«¯å…¼å®¹çš„å…ƒæ•°æ®
                "__task_plan": {
                    "task_count": len(task_list),
                    "strategy": commander_response.strategy,
                    "estimated_steps": commander_response.estimated_steps,
                    "tasks": task_list
                }
            }

        except Exception as e:
            print(f"[ERROR] Commander è§„åˆ’å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "task_list": [],
                "strategy": f"Error: {str(e)}",
                "current_task_index": 0,
                "event_queue": []
            }
