"""
Commander èŠ‚ç‚¹ - ä»»åŠ¡è§„åˆ’

å°†å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡ï¼Œæ”¯æŒæ˜¾å¼ä¾èµ–å…³ç³»ï¼ˆDAGï¼‰
"""
import os
from typing import Dict, Any, List, Optional
from datetime import datetime
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.runnables import RunnableConfig
from pydantic import BaseModel, Field, field_validator
from sqlmodel import Session

from agents.state import AgentState
from utils.json_parser import parse_llm_json
from utils.llm_factory import get_llm_instance
from constants import COMMANDER_SYSTEM_PROMPT
from database import engine


class SubTaskOutput(BaseModel):
    """å•ä¸ªå­ä»»åŠ¡ç»“æ„ (Commander ä½¿ç”¨)
    
    æ”¯æŒæ˜¾å¼ä¾èµ–å…³ç³» (DAG)ï¼Œé€šè¿‡ id å’Œ depends_on å®ç°ç²¾å‡†æ•°æ®ç®¡é“
    """
    id: str = Field(default="", description="ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆçŸ­IDï¼Œå¦‚ task_1, task_2ï¼‰")
    expert_type: str = Field(description="æ‰§è¡Œæ­¤ä»»åŠ¡çš„ä¸“å®¶ç±»å‹ï¼ˆå¯ä»¥æ˜¯ç³»ç»Ÿå†…ç½®ä¸“å®¶æˆ–è‡ªå®šä¹‰ä¸“å®¶ï¼‰")
    description: str = Field(description="ä»»åŠ¡æè¿°")
    input_data: Dict[str, Any] = Field(default={}, description="è¾“å…¥å‚æ•°")
    priority: int = Field(default=0, description="ä¼˜å…ˆçº§ (0=æœ€é«˜)")
    depends_on: List[str] = Field(default=[], description="ä¾èµ–çš„ä»»åŠ¡IDåˆ—è¡¨ã€‚å¦‚æœä»»åŠ¡Béœ€è¦ä»»åŠ¡Açš„è¾“å‡ºï¼Œåˆ™å¡«å…¥ ['task_a']")
    
    @field_validator('depends_on', mode='before')
    @classmethod
    def parse_depends_on(cls, v):
        """å…¼å®¹å¤„ç†ï¼šå¦‚æœ LLM è¿”å›äº†æ•´æ•°ä¾èµ–ï¼ˆå¦‚ [0]ï¼‰ï¼Œå¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸² ["0"]"""
        if v is None:
            return []
        
        # æƒ…å†µ 1: LLM å‘ç–¯è¿”äº†ä¸ªå•ä¸ª int/str (ä¸æ˜¯åˆ—è¡¨)
        if isinstance(v, (int, str)):
            return [str(v)]
            
        # æƒ…å†µ 2: æ­£å¸¸çš„åˆ—è¡¨ï¼Œä½†é‡Œé¢æ··äº† int
        if isinstance(v, list):
            return [str(item) for item in v]
            
        return v


class CommanderOutput(BaseModel):
    """æŒ‡æŒ¥å®˜è¾“å‡º - å­ä»»åŠ¡åˆ—è¡¨"""
    tasks: List[SubTaskOutput] = Field(description="å­ä»»åŠ¡åˆ—è¡¨")
    strategy: str = Field(description="æ‰§è¡Œç­–ç•¥æ¦‚è¿°")
    estimated_steps: int = Field(description="é¢„è®¡æ­¥éª¤æ•°")


async def _preload_expert_configs(task_list: List[Dict], db_session: Any) -> None:
    """
    P1 ä¼˜åŒ–: é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®åˆ°ç¼“å­˜
    
    åœ¨ Commander é˜¶æ®µå°±å¹¶è¡ŒåŠ è½½æ‰€æœ‰éœ€è¦çš„ä¸“å®¶é…ç½®ï¼Œ
    é¿å… GenericWorker æ‰§è¡Œæ—¶å†é€ä¸ªæŸ¥è¯¢æ•°æ®åº“ã€‚
    
    Args:
        task_list: ä»»åŠ¡åˆ—è¡¨
        db_session: æ•°æ®åº“ä¼šè¯
    """
    if not task_list or not db_session:
        return
    
    # æå–æ‰€æœ‰å”¯ä¸€çš„ä¸“å®¶ç±»å‹
    expert_types = list(set(task.get("expert_type") for task in task_list if task.get("expert_type")))
    if not expert_types:
        return
    
    print(f"[COMMANDER] P1ä¼˜åŒ–: é¢„åŠ è½½ {len(expert_types)} ä¸ªä¸“å®¶é…ç½®...")
    
    # å¹¶è¡ŒåŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®
    from agents.services.expert_manager import get_expert_config_cached
    
    loaded_count = 0
    for expert_type in expert_types:
        try:
            # å…ˆä»ç¼“å­˜æ£€æŸ¥
            cached = get_expert_config_cached(expert_type)
            if cached:
                loaded_count += 1
                continue
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®åº“åŠ è½½
            from agents.services.expert_manager import get_expert_config
            config = get_expert_config(expert_type, db_session)
            if config:
                loaded_count += 1
        except Exception as e:
            print(f"[COMMANDER] é¢„åŠ è½½ä¸“å®¶ '{expert_type}' å¤±è´¥: {e}")
    
    print(f"[COMMANDER] P1ä¼˜åŒ–: æˆåŠŸé¢„åŠ è½½ {loaded_count}/{len(expert_types)} ä¸ªä¸“å®¶é…ç½®")


async def commander_node(state: AgentState, config: RunnableConfig = None) -> Dict[str, Any]:
    """
    [æŒ‡æŒ¥å®˜] å°†å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡ã€‚
    v3.0 æ›´æ–°ï¼šç«‹å³æŒä¹…åŒ–åˆ°æ•°æ®åº“ï¼Œå‘é€ plan.created äº‹ä»¶
    v3.1 æ›´æ–°ï¼šä½¿ç”¨ç‹¬ç«‹æ•°æ®åº“ä¼šè¯ï¼Œé¿å… MemorySaver åºåˆ—åŒ–é—®é¢˜
    v3.3 æ›´æ–°ï¼šæµå¼æ€è€ƒ + JSON ç”Ÿæˆï¼Œå…ˆå±•ç¤ºæ€è€ƒè¿‡ç¨‹ï¼Œåè¾“å‡ºä»»åŠ¡è§„åˆ’
    v3.4 æ›´æ–°ï¼šä½¿ç”¨ Shared Queue æ¨¡å¼å®ç°çœŸæ­£çš„å®æ—¶æµå¼è¾“å‡º
    """
    from agents.services.expert_manager import get_expert_config, get_expert_config_cached
    from agents.services.expert_manager import get_all_expert_list, format_expert_list_for_prompt
    from agents.services.task_manager import get_or_create_task_session
    from models import SubTaskCreate
    from utils.event_generator import (
        event_plan_created, event_plan_started, event_plan_thinking,
        sse_event_to_string
    )
    import uuid
    
    # ğŸ”¥ğŸ”¥ğŸ”¥ v3.4: è·å–å…±äº«é˜Ÿåˆ— (Side Channel)
    stream_queue = None
    if config:
        stream_queue = config.get("configurable", {}).get("stream_queue")
        if stream_queue:
            print(f"[COMMANDER] è·å–åˆ° stream_queueï¼Œå°†å®æ—¶æ¨é€æ€è€ƒå†…å®¹")
    
    # ğŸ”¥ åˆå§‹åŒ–äº‹ä»¶é˜Ÿåˆ—ï¼ˆç”¨äºæ”¶é›†æ‰€æœ‰äº‹ä»¶ï¼‰
    event_queue = []
    
    messages = state["messages"]
    last_message = messages[-1]
    user_query = last_message.content if isinstance(last_message, HumanMessage) else str(last_message.content)
    
    # è·å– thread_id
    thread_id = state.get("thread_id")
    
    # ğŸ”¥ ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼ˆé¿å… MemorySaver åºåˆ—åŒ–é—®é¢˜ï¼‰
    with Session(engine) as db_session:
        try:
            # åŠ è½½é…ç½® (æ•°æ®åº“æˆ–å›é€€)
            commander_config = get_expert_config("commander", db_session)
            
            # å¦‚æœæ•°æ®åº“è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°ç¼“å­˜
            if not commander_config:
                commander_config = get_expert_config_cached("commander")
            
            if not commander_config:
                # å›é€€ï¼šä½¿ç”¨å¸¸é‡ä¸­çš„ Prompt å’Œç¡¬ç¼–ç çš„æ¨¡å‹
                system_prompt = COMMANDER_SYSTEM_PROMPT
                model = os.getenv("MODEL_NAME", "deepseek-chat")
                temperature = 0.5
                print(f"[COMMANDER] ä½¿ç”¨é»˜è®¤å›é€€é…ç½®: model={model}")
            else:
                # ä½¿ç”¨æ•°æ®åº“é…ç½®
                system_prompt = commander_config["system_prompt"]
                model = commander_config["model"]
                temperature = commander_config["temperature"]
                print(f"[COMMANDER] åŠ è½½é…ç½®: model={model}, temperature={temperature}")
            
            # æ³¨å…¥åŠ¨æ€ä¸“å®¶åˆ—è¡¨åˆ° System Prompt
            try:
                # è·å–æ‰€æœ‰å¯ç”¨ä¸“å®¶ï¼ˆåŒ…æ‹¬åŠ¨æ€åˆ›å»ºçš„ä¸“å®¶ï¼‰
                all_experts = get_all_expert_list(db_session)
                expert_list_str = format_expert_list_for_prompt(all_experts)
                
                # å°è¯•æ³¨å…¥ä¸“å®¶åˆ—è¡¨åˆ° Promptï¼ˆå¦‚æœ Prompt æ”¯æŒåŠ¨æ€å ä½ç¬¦ï¼‰
                if "{dynamic_expert_list}" in system_prompt:
                    system_prompt = system_prompt.format(dynamic_expert_list=expert_list_str)
                    print(f"[COMMANDER] å·²æ³¨å…¥åŠ¨æ€ä¸“å®¶åˆ—è¡¨ï¼Œå…± {len(all_experts)} ä¸ªä¸“å®¶")
                else:
                    # å¦‚æœ Prompt ä¸åŒ…å«å ä½ç¬¦ï¼Œä¿ç•™åŸæœ‰é€»è¾‘ï¼ˆå‘åå…¼å®¹ï¼‰
                    print(f"[COMMANDER] Prompt ä¸åŒ…å«åŠ¨æ€å ä½ç¬¦ï¼Œè·³è¿‡ä¸“å®¶åˆ—è¡¨æ³¨å…¥")
            except Exception as e:
                # æ³¨å…¥å¤±è´¥æ—¶ä¸ä¸­æ–­æµç¨‹ï¼Œä¿ç•™åŸå§‹ Prompt
                print(f"[COMMANDER] ä¸“å®¶åˆ—è¡¨æ³¨å…¥å¤±è´¥ï¼ˆå·²å¿½ç•¥ï¼‰: {e}")
            
            # æ‰§è¡Œ LLM è¿›è¡Œè§„åˆ’
            # ä»æ¨¡å‹åç§°æ¨æ–­ provider
            from providers_config import get_model_config
            from agents.graph import get_commander_llm_lazy

            model_config = get_model_config(model)

            if model_config and 'provider' in model_config:
                # ä½¿ç”¨æ¨æ–­å‡ºçš„ provider åˆ›å»º LLM
                provider = model_config['provider']
                # ä¼˜å…ˆä½¿ç”¨æ¨¡å‹é…ç½®ä¸­çš„ temperatureï¼ˆå¦‚æœæœ‰ï¼‰
                final_temperature = model_config.get('temperature', temperature)
                # è·å–å®é™…çš„ API æ¨¡å‹åç§°ï¼ˆproviders.yaml ä¸­å®šä¹‰çš„ model å­—æ®µï¼‰
                actual_model = model_config.get('model', model)
                llm = get_llm_instance(
                    provider=provider,
                    streaming=True,
                    temperature=final_temperature
                )
                print(f"[COMMANDER] æ¨¡å‹ '{model}' -> '{actual_model}' ä½¿ç”¨ provider: {provider}, temperature: {final_temperature}")
                llm_with_config = llm.bind(model=actual_model, temperature=final_temperature)
            else:
                # å›é€€åˆ° commander_llmï¼ˆç¡¬ç¼–ç çš„ provider ä¼˜å…ˆçº§ï¼‰
                print(f"[COMMANDER] æ¨¡å‹ '{model}' æœªæ‰¾åˆ° provider é…ç½®ï¼Œå›é€€åˆ° commander_llm")
                llm_with_config = get_commander_llm_lazy().bind(model=model, temperature=temperature)

            # ğŸ”¥ğŸ”¥ğŸ”¥ v3.3: æµå¼æ€è€ƒ + JSON ç”Ÿæˆ
            # 1ï¸âƒ£ è·å–æˆ–ç”Ÿæˆ session_id
            # å¦‚æœ chat.py å·²ç»å‘é€äº† plan.startedï¼Œä½¿ç”¨ç›¸åŒçš„ session_id
            preview_session_id = state.get("preview_session_id") or str(uuid.uuid4())
            
            # ğŸ”¥ åªæœ‰åœ¨ chat.py æ²¡æœ‰å‘é€ plan.started çš„æƒ…å†µä¸‹ï¼Œæ‰åœ¨è¿™é‡Œå‘é€
            if not state.get("preview_session_id"):
                started_event = event_plan_started(
                    session_id=preview_session_id,
                    title="ä»»åŠ¡è§„åˆ’",
                    content="æ­£åœ¨åˆ†æéœ€æ±‚...",
                    status="running"
                )
                event_queue.append({"type": "sse", "event": sse_event_to_string(started_event)})
                print(f"[COMMANDER] å‘é€ plan.started: {preview_session_id}")
            else:
                print(f"[COMMANDER] å¤ç”¨ chat.py å‘é€çš„ plan.started: {preview_session_id}")
            
            # 2ï¸âƒ£ æµå¼ç”Ÿæˆï¼šåŒºåˆ† Thinking å’Œ JSON é˜¶æ®µ
            thinking_content = ""
            json_buffer = ""
            is_json_phase = False
            json_start_detected = False
            
            print("[COMMANDER] å¼€å§‹æµå¼ç”Ÿæˆ...")
            print(f"[COMMANDER] stream_queue: {'å·²è·å–' if stream_queue else 'æœªè·å–'}")
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ å¼ºåŒ– Promptï¼šæ˜ç¡®è¦æ±‚å…ˆæ€è€ƒå†è¾“å‡º JSON
            human_prompt = f"""ç”¨æˆ·æŸ¥è¯¢: {user_query}

ã€é‡è¦ã€‘ä½ å¿…é¡»æŒ‰ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š

**æ­¥éª¤ 1 - éœ€æ±‚åˆ†æï¼ˆå¿…é¡»ï¼‰:**
è¯·å…ˆä»¥è‡ªç„¶è¯­è¨€è¯¦ç»†åˆ†æè¿™ä¸ªéœ€æ±‚ã€‚åŒ…æ‹¬ï¼š
- ç”¨æˆ·çš„æ ¸å¿ƒæ„å›¾æ˜¯ä»€ä¹ˆï¼Ÿ
- éœ€è¦å“ªäº›æ­¥éª¤æ¥å®Œæˆï¼Ÿ
- æ¯ä¸ªæ­¥éª¤åº”è¯¥åˆ†é…ç»™å“ªä¸ªä¸“å®¶ï¼Ÿ
- æ­¥éª¤ä¹‹é—´çš„ä¾èµ–å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ

**æ­¥éª¤ 2 - ä»»åŠ¡è§„åˆ’ï¼ˆå¿…é¡»ï¼‰:**
åœ¨åˆ†æå®Œæˆåï¼Œè¾“å‡ºä¸€ä¸ª ```json ä»£ç å—ï¼ŒåŒ…å«ç»“æ„åŒ–çš„ä»»åŠ¡æ•°æ®ã€‚

æ³¨æ„ï¼šä¸è¦ç›´æ¥è¾“å‡º JSONï¼Œå¿…é¡»å…ˆè¿›è¡Œè¯¦ç»†çš„è‡ªç„¶è¯­è¨€åˆ†æï¼"""
            
            chunk_count = 0
            debug_chunks = []  # æ”¶é›†å‰10ä¸ª chunk ç”¨äºè°ƒè¯•
            
            async for chunk in llm_with_config.astream(
                [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=human_prompt)
                ],
                config=RunnableConfig(
                    tags=["commander", "streaming"],
                    metadata={"node_type": "commander", "mode": "streaming"}
                )
            ):
                content = chunk.content if hasattr(chunk, "content") else str(chunk)
                chunk_count += 1
                
                # ğŸ”¥ æ”¶é›†å‰10ä¸ª chunk ç”¨äºè°ƒè¯•
                if chunk_count <= 10:
                    debug_chunks.append(content)
                    print(f"[COMMANDER] Chunk {chunk_count}: {repr(content[:80])}")
                
                if not content:
                    continue
                
                # ğŸ”¥ æ¯ 50 ä¸ª chunk æ‰“å°ä¸€æ¬¡æ—¥å¿—
                if chunk_count % 50 == 0:
                    print(f"[COMMANDER] å·²å¤„ç† {chunk_count} chunks, thinking_phase={not is_json_phase}, content_len={len(content)}, thinking_len={len(thinking_content)}")
                
                # ğŸ”¥ æ£€æµ‹ JSON å¼€å§‹æ ‡è®°ï¼ˆå¤šç§æƒ…å†µï¼‰
                if not is_json_phase:
                    # æƒ…å†µ1: æ£€æµ‹åˆ°ä»£ç å—æ ‡è®° ```json æˆ– ```
                    if "```json" in content or "```" in content:
                        print(f"[COMMANDER] ğŸ“¦ æ£€æµ‹åˆ° JSON å¼€å§‹æ ‡è®°ï¼Œåˆ‡æ¢åˆ° JSON é˜¶æ®µ")
                        is_json_phase = True
                        json_start_detected = True
                        # æå– ```json ä¹‹å‰çš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰ä½œä¸ºæœ€åçš„ thinking
                        before_json = content.split("```")[0]
                        if before_json.strip():
                            thinking_content += before_json
                            thinking_event = event_plan_thinking(
                                session_id=preview_session_id,
                                delta=before_json
                            )
                            event_str = sse_event_to_string(thinking_event)
                            event_queue.append({"type": "sse", "event": event_str})
                            # ğŸ”¥ğŸ”¥ğŸ”¥ å®æ—¶æ¨é€åˆ°å…±äº«é˜Ÿåˆ—
                            if stream_queue:
                                await stream_queue.put({"type": "sse", "event": event_str})
                        # å‰©ä½™éƒ¨åˆ†è¿›å…¥ json_buffer
                        json_parts = content.split("```", 1)
                        if len(json_parts) > 1:
                            json_buffer += json_parts[1]
                        continue
                    
                    # æƒ…å†µ2: æ£€æµ‹åˆ°çº¯ JSON å¼€å§‹ï¼ˆLLM ç›´æ¥è¾“å‡º JSON è€Œæ²¡æœ‰ä»£ç å—ï¼‰
                    # æ£€æµ‹æ¡ä»¶ï¼šå†…å®¹ä»¥ '{' å¼€å¤´ï¼Œä¸”æˆ‘ä»¬å·²ç»æ¥æ”¶äº†ä¸€äº›å†…å®¹ï¼ˆé¿å…è¯¯åˆ¤ç¬¬ä¸€ä¸ªå­—ç¬¦ï¼‰
                    if content.strip().startswith("{") and chunk_count > 1 and len(thinking_content) < 50:
                        print(f"[COMMANDER] âš ï¸ æ£€æµ‹åˆ°çº¯ JSON è¾“å‡ºï¼ˆæ— ä»£ç å—ï¼‰ï¼Œåˆ‡æ¢åˆ° JSON é˜¶æ®µ")
                        print(f"[COMMANDER] å½“å‰ thinking_content é•¿åº¦: {len(thinking_content)}, å†…å®¹: {thinking_content[:100]}...")
                        is_json_phase = True
                        json_start_detected = True
                        json_buffer += content
                        continue
                    
                    # ğŸ“ Thinking é˜¶æ®µï¼šå®æ—¶å‘é€ plan.thinking
                    thinking_content += content
                    thinking_event = event_plan_thinking(
                        session_id=preview_session_id,
                        delta=content
                    )
                    event_str = sse_event_to_string(thinking_event)
                    event_queue.append({"type": "sse", "event": event_str})
                    # ğŸ”¥ğŸ”¥ğŸ”¥ å®æ—¶æ¨é€åˆ°å…±äº«é˜Ÿåˆ—
                    if stream_queue:
                        # ğŸ”¥ ä½¿ç”¨å­—å…¸æ ¼å¼ï¼Œä¸ chat.py çš„ Consumer åŒ¹é…
                        await stream_queue.put({"type": "sse", "event": event_str})
                        if chunk_count <= 5:
                            print(f"[COMMANDER] ğŸš€ å‘é€ plan.thinking: {content[:50]}...")
                else:
                    # ğŸ“¦ JSON é˜¶æ®µï¼šé™é»˜æ‹¼æ¥ï¼Œä¸å‘é€ SSE
                    # æ£€æµ‹ JSON ç»“æŸæ ‡è®°
                    if "```" in content:
                        # æå– ``` ä¹‹å‰çš„å†…å®¹
                        json_parts = content.split("```", 1)
                        json_buffer += json_parts[0]
                        # ä¹‹åçš„å†…å®¹å¿½ç•¥ï¼ˆç»“æŸæ ‡è®°åçš„å†…å®¹ï¼‰
                    else:
                        json_buffer += content
            
            print(f"[COMMANDER] æµå¼ç”Ÿæˆå®Œæˆã€‚æ€è€ƒé•¿åº¦: {len(thinking_content)}, JSONé•¿åº¦: {len(json_buffer)}")
            
            # 3ï¸âƒ£ è§£æ JSON
            # æ¸…ç† JSON å†…å®¹ï¼ˆç§»é™¤å¯èƒ½çš„ json æ ‡è®°å‰ç¼€ï¼‰
            json_str = json_buffer.strip()
            if json_str.startswith("json"):
                json_str = json_str[4:].strip()
            
            try:
                commander_response = parse_llm_json(
                    json_str,
                    CommanderOutput,
                    strict=False,
                    clean_markdown=False  # å·²ç»æ‰‹åŠ¨æ¸…ç†äº†
                )
                print(f"[COMMANDER] JSON è§£ææˆåŠŸï¼Œç”Ÿæˆ {len(commander_response.tasks)} ä¸ªä»»åŠ¡")
            except Exception as parse_err:
                print(f"[COMMANDER] JSON è§£æå¤±è´¥: {parse_err}")
                print(f"[COMMANDER] åŸå§‹ JSON å†…å®¹: {json_str[:500]}...")
                raise

            # v3.1: å…œåº•å¤„ç† - å¦‚æœ LLM æ²¡æœ‰ç”Ÿæˆ idï¼Œè‡ªåŠ¨ç”Ÿæˆ
            for idx, task in enumerate(commander_response.tasks):
                if not task.id:
                    task.id = f"task_{idx}"
                    print(f"[COMMANDER] è‡ªåŠ¨ä¸ºä»»åŠ¡ {idx} ç”Ÿæˆ id: {task.id}")
            
            # v3.2: ä¿®å¤ä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥ - å°† depends_on ä¸­çš„ç´¢å¼•æ ¼å¼è½¬æ¢ä¸º ID æ ¼å¼
            task_id_map = {str(idx): task.id for idx, task in enumerate(commander_response.tasks)}
            for task in commander_response.tasks:
                if task.depends_on:
                    new_depends_on = []
                    for dep in task.depends_on:
                        # å¦‚æœæ˜¯æ•°å­—ç´¢å¼•ï¼ˆå¦‚ "0"ï¼‰ï¼Œè½¬æ¢ä¸ºå¯¹åº”çš„ IDï¼ˆå¦‚ "task_0"ï¼‰
                        if dep in task_id_map:
                            new_depends_on.append(task_id_map[dep])
                        else:
                            # å¦‚æœå·²ç»æ˜¯æ­£ç¡®çš„ ID æ ¼å¼ï¼ˆå¦‚ "task_0"ï¼‰ï¼Œä¿æŒä¸å˜
                            new_depends_on.append(dep)
                    task.depends_on = new_depends_on
                    print(f"[COMMANDER] ä»»åŠ¡ {task.id} çš„ä¾èµ–å·²è½¬æ¢: {new_depends_on}")

            # v3.0: å‡†å¤‡å­ä»»åŠ¡æ•°æ®ï¼ˆæ”¯æŒæ˜¾å¼ä¾èµ–å…³ç³» DAGï¼‰
            subtasks_data = [
                SubTaskCreate(
                    expert_type=task.expert_type,
                    task_description=task.description,
                    input_data=task.input_data,
                    sort_order=idx,
                    execution_mode="sequential",
                    depends_on=task.depends_on if task.depends_on else None
                )
                for idx, task in enumerate(commander_response.tasks)
            ]

            # v3.0: ç«‹å³æŒä¹…åŒ–åˆ°æ•°æ®åº“ (é€šè¿‡ TaskManager)
            # ğŸ”¥ v3.3: ä½¿ç”¨ preview_session_id ç¡®ä¿äº‹ä»¶å’Œæ•°æ®åº“è®°å½•ä¸€è‡´
            task_session = None
            if db_session and thread_id:
                task_session, is_reused = get_or_create_task_session(
                    db=db_session,
                    thread_id=thread_id,
                    user_query=user_query,
                    plan_summary=commander_response.strategy,
                    estimated_steps=commander_response.estimated_steps,
                    subtasks_data=subtasks_data,
                    execution_mode="sequential",
                    session_id=preview_session_id  # ğŸ”¥ ä¼ å…¥é¢„è§ˆæ—¶ä½¿ç”¨çš„ session_id
                )
                session_source = "å¤ç”¨" if is_reused else "æ–°å»º"
                print(f"[COMMANDER] TaskSession {session_source}: {task_session.session_id}")

            # è½¬æ¢ä¸ºå†…éƒ¨å­—å…¸æ ¼å¼ï¼ˆç”¨äº LangGraph çŠ¶æ€æµè½¬ï¼‰
            sub_tasks_list = task_session.sub_tasks if task_session else []
            task_list = []
            for idx, subtask in enumerate(sub_tasks_list):
                commander_task = commander_response.tasks[idx]
                task_list.append({
                    "id": subtask.id,
                    "task_id": commander_task.id,
                    "expert_type": subtask.expert_type,
                    "description": subtask.task_description,
                    "input_data": subtask.input_data,
                    "sort_order": subtask.sort_order,
                    "status": subtask.status,
                    "depends_on": commander_task.depends_on if commander_task.depends_on else [],
                    "output_result": None,
                    "started_at": None,
                    "completed_at": None
                })

            print(f"[COMMANDER] ç”Ÿæˆäº† {len(task_list)} ä¸ªä»»åŠ¡ã€‚ç­–ç•¥: {commander_response.strategy}")

            # P1 ä¼˜åŒ–: é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®åˆ°ç¼“å­˜
            await _preload_expert_configs(task_list, db_session)

            # ğŸ”¥ v3.3: ä½¿ç”¨ preview_session_id ä¿æŒä¸€è‡´æ€§ï¼ŒTaskSession åˆ›å»ºåä¼šä½¿ç”¨ç›¸åŒçš„ ID
            # æ³¨æ„ï¼šè¿™é‡Œä¸å†åˆ›å»ºæ–°çš„ event_queueï¼Œè€Œæ˜¯å¤ç”¨ä¹‹å‰çš„äº‹ä»¶é˜Ÿåˆ—
            
            # 4ï¸âƒ£ å‘é€ plan.created äº‹ä»¶ï¼ˆå®ŒæˆçŠ¶æ€ï¼‰
            if task_session:
                plan_event = event_plan_created(
                    session_id=task_session.session_id,
                    summary=commander_response.strategy,
                    estimated_steps=commander_response.estimated_steps,
                    execution_mode="sequential",
                    tasks=[
                        {
                            "id": t.id,
                            "task_id": commander_response.tasks[idx].id,
                            "expert_type": t.expert_type,
                            "description": t.task_description,
                            "sort_order": t.sort_order,
                            "status": t.status,
                            "depends_on": commander_response.tasks[idx].depends_on if commander_response.tasks[idx].depends_on else []
                        }
                        for idx, t in enumerate(task_session.sub_tasks)
                    ]
                )
                event_queue.append({"type": "sse", "event": sse_event_to_string(plan_event)})

            return {
                "task_list": task_list,
                "strategy": commander_response.strategy,
                "current_task_index": 0,
                "expert_results": [],
                "task_session_id": task_session.session_id if task_session else None,
                "event_queue": event_queue,
                # ä¿ç•™å‰ç«¯å…¼å®¹çš„å…ƒæ•°æ®
                "__task_plan": {
                    "task_count": len(task_list),
                    "strategy": commander_response.strategy,
                    "estimated_steps": commander_response.estimated_steps,
                    "tasks": task_list
                }
            }

        except Exception as e:
            print(f"[ERROR] Commander è§„åˆ’å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "task_list": [],
                "strategy": f"Error: {str(e)}",
                "current_task_index": 0,
                "event_queue": []
            }
