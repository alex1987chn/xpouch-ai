import logging

logger = logging.getLogger(__name__)

# P1 ä¼˜åŒ–: ç»Ÿä¸€ä½¿ç”¨ tenacity è¿›è¡Œé‡è¯•
from tenacity import (
    retry,
    stop_after_attempt,
    wait_fixed,
    retry_if_exception_type,
    before_sleep_log
)

"""
Commander èŠ‚ç‚¹ - ä»»åŠ¡è§„åˆ’ä¸æ‹†è§£

[èŒè´£]
å°†ç”¨æˆ·å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºå¯æ‰§è¡Œçš„å­ä»»åŠ¡åºåˆ—ï¼ˆSubTasksï¼‰ï¼Œæ”¯æŒï¼š
- ä¸“å®¶åˆ†é…ï¼ˆexpert_typeï¼‰
- ä»»åŠ¡ä¾èµ–ï¼ˆDAGï¼Œé€šè¿‡ depends_on å®ç°ï¼‰
- ä¼˜å…ˆçº§æ’åºï¼ˆpriorityï¼‰

[æ‰§è¡Œæµç¨‹]
1. åˆ†æç”¨æˆ·æŸ¥è¯¢æ„å›¾
2. ç”Ÿæˆä»»åŠ¡åˆ—è¡¨ï¼ˆä½¿ç”¨ç»“æ„åŒ–è¾“å‡º CommanderOutputï¼‰
3. åˆ›å»º TaskSession å’Œ SubTasksï¼ˆæ•°æ®åº“æŒä¹…åŒ–ï¼‰
4. é¢„åŠ è½½ä¸“å®¶é…ç½®åˆ°ç¼“å­˜ï¼ˆP1 ä¼˜åŒ–ï¼‰
5. å‘é€ plan.created äº‹ä»¶ï¼ˆé©±åŠ¨å‰ç«¯æ˜¾ç¤º Thinking Stepsï¼‰
6. è§¦å‘ HITL ä¸­æ–­ï¼ˆç­‰å¾…ç”¨æˆ·ç¡®è®¤è®¡åˆ’ï¼‰

[è¾“å‡ºç»“æ„]
CommanderOutput:
  - tasks: SubTask åˆ—è¡¨
    - id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†
    - expert_type: æ‰§è¡Œä¸“å®¶ç±»å‹
    - description: ä»»åŠ¡æè¿°
    - depends_on: ä¾èµ–ä»»åŠ¡IDåˆ—è¡¨ï¼ˆä¸Šæ¸¸è¾“å‡ºæ³¨å…¥ä¸Šä¸‹æ–‡ï¼‰
    - priority: æ‰§è¡Œä¼˜å…ˆçº§
  - strategy: æ‰§è¡Œç­–ç•¥æ¦‚è¿°
  - estimated_steps: é¢„ä¼°æ­¥éª¤æ•°

[ä¾èµ–å¤„ç†]
- ä¸‹æ¸¸ä»»åŠ¡è‡ªåŠ¨è·å–ä¸Šæ¸¸ä»»åŠ¡è¾“å‡ºä½œä¸ºä¸Šä¸‹æ–‡
- å®¹é”™ï¼šç¼ºå¤±ä¾èµ–æ—¶æç¤º LLM åŸºäºç°æœ‰ä¿¡æ¯å°½åŠ›å®Œæˆ

[æ•°æ®åº“æ“ä½œ]
- åˆ›å»º TaskSessionï¼ˆä»»åŠ¡ä¼šè¯ï¼‰
- æ‰¹é‡åˆ›å»º SubTaskï¼ˆå­ä»»åŠ¡ï¼‰
- å…³è” Threadï¼ˆå¯¹è¯çº¿ç¨‹ï¼‰

[HITL é›†æˆ]
- ç”Ÿæˆ plan.created äº‹ä»¶åæš‚åœï¼ˆinterruptï¼‰
- ç”¨æˆ·å¯ä¿®æ”¹/åˆ é™¤/é‡æ’ä»»åŠ¡
- ç¡®è®¤å Dispatcher æŒ‰æ–°è®¡åˆ’æ‰§è¡Œ
"""
import os
import asyncio
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.runnables import RunnableConfig
from pydantic import BaseModel, Field, field_validator, ValidationError
from sqlmodel import Session

from agents.state import AgentState
from utils.json_parser import parse_llm_json
from utils.llm_factory import get_llm_instance
from constants import COMMANDER_SYSTEM_PROMPT
from database import engine


# ============================================================================
# Commander 2.0: Pydantic ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹
# ============================================================================

class Task(BaseModel):
    """ä»»åŠ¡å®šä¹‰ - æ”¯æŒ DAG ä¾èµ–å…³ç³»"""
    id: str = Field(default="", description="ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆçŸ­IDï¼Œå¦‚ task_1, task_2ï¼‰")
    expert_type: str = Field(description="æ‰§è¡Œæ­¤ä»»åŠ¡çš„ä¸“å®¶ç±»å‹")
    description: str = Field(description="ä»»åŠ¡æè¿°")
    input_data: Dict[str, Any] = Field(default={}, description="è¾“å…¥å‚æ•°")
    priority: int = Field(default=0, description="ä¼˜å…ˆçº§ (0=æœ€é«˜)")
    dependencies: List[str] = Field(default=[], description="ä¾èµ–çš„ä»»åŠ¡IDåˆ—è¡¨")
    
    @field_validator('dependencies', mode='before')
    @classmethod
    def parse_dependencies(cls, v):
        """å…¼å®¹å¤„ç†ï¼šæ•´æ•°ä¾èµ–è½¬ä¸ºå­—ç¬¦ä¸²"""
        if v is None:
            return []
        if isinstance(v, (int, str)):
            return [str(v)]
        if isinstance(v, list):
            return [str(item) for item in v]
        return v


class ExecutionPlan(BaseModel):
    """
    Commander 2.0 æ‰§è¡Œè®¡åˆ’è¾“å‡º
    
    ä½¿ç”¨ Pydantic ç»“æ„åŒ–è¾“å‡ºï¼Œç¡®ä¿ LLM ç”Ÿæˆç¬¦åˆ Schema çš„æ•°æ®
    """
    thought_process: str = Field(
        default="", 
        description="è§„åˆ’æ€è€ƒè¿‡ç¨‹ï¼šåˆ†æéœ€æ±‚ã€æ‹†è§£æ­¥éª¤ã€åˆ†é…ä¸“å®¶çš„æ¨ç†è¿‡ç¨‹"
    )
    strategy: str = Field(
        description="æ‰§è¡Œç­–ç•¥æ¦‚è¿°ï¼šå¦‚'å¹¶è¡Œæ‰§è¡Œ'ã€'é¡ºåºæ‰§è¡Œ'ã€'åˆ†é˜¶æ®µäº¤ä»˜'ç­‰"
    )
    estimated_steps: int = Field(
        description="é¢„è®¡æ­¥éª¤æ•°"
    )
    tasks: List[Task] = Field(
        description="å­ä»»åŠ¡åˆ—è¡¨ï¼Œæ”¯æŒä¾èµ–å…³ç³»ï¼ˆDAGï¼‰"
    )


# å‘åå…¼å®¹ï¼šä¿ç•™æ—§æ¨¡å‹åˆ«å
SubTaskOutput = Task
CommanderOutput = ExecutionPlan


async def _preload_expert_configs(task_list: List[Dict], db_session: Any) -> None:
    """
    P1 ä¼˜åŒ–: é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®åˆ°ç¼“å­˜
    
    åœ¨ Commander é˜¶æ®µå°±å¹¶è¡ŒåŠ è½½æ‰€æœ‰éœ€è¦çš„ä¸“å®¶é…ç½®ï¼Œ
    é¿å… GenericWorker æ‰§è¡Œæ—¶å†é€ä¸ªæŸ¥è¯¢æ•°æ®åº“ã€‚
    
    Args:
        task_list: ä»»åŠ¡åˆ—è¡¨
        db_session: æ•°æ®åº“ä¼šè¯
    """
    if not task_list or not db_session:
        return
    
    # æå–æ‰€æœ‰å”¯ä¸€çš„ä¸“å®¶ç±»å‹
    expert_types = list(set(task.get("expert_type") for task in task_list if task.get("expert_type")))
    if not expert_types:
        return
    
    print(f"[COMMANDER] P1ä¼˜åŒ–: é¢„åŠ è½½ {len(expert_types)} ä¸ªä¸“å®¶é…ç½®...")
    
    # å¹¶è¡ŒåŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®
    from agents.services.expert_manager import get_expert_config_cached
    
    loaded_count = 0
    for expert_type in expert_types:
        try:
            # å…ˆä»ç¼“å­˜æ£€æŸ¥
            cached = get_expert_config_cached(expert_type)
            if cached:
                loaded_count += 1
                continue
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®åº“åŠ è½½
            from agents.services.expert_manager import get_expert_config
            config = get_expert_config(expert_type, db_session)
            if config:
                loaded_count += 1
        except Exception as e:
            print(f"[COMMANDER] é¢„åŠ è½½ä¸“å®¶ '{expert_type}' å¤±è´¥: {e}")
    
    print(f"[COMMANDER] P1ä¼˜åŒ–: æˆåŠŸé¢„åŠ è½½ {loaded_count}/{len(expert_types)} ä¸ªä¸“å®¶é…ç½®")


async def commander_node(state: AgentState, config: RunnableConfig = None) -> Dict[str, Any]:
    """
    [æŒ‡æŒ¥å®˜] å°†å¤æ‚æŸ¥è¯¢æ‹†è§£ä¸ºå­ä»»åŠ¡ã€‚
    v3.0 æ›´æ–°ï¼šç«‹å³æŒä¹…åŒ–åˆ°æ•°æ®åº“ï¼Œå‘é€ plan.created äº‹ä»¶
    v3.1 æ›´æ–°ï¼šä½¿ç”¨ç‹¬ç«‹æ•°æ®åº“ä¼šè¯ï¼Œé¿å… MemorySaver åºåˆ—åŒ–é—®é¢˜
    v3.3 æ›´æ–°ï¼šæµå¼æ€è€ƒ + JSON ç”Ÿæˆï¼Œå…ˆå±•ç¤ºæ€è€ƒè¿‡ç¨‹ï¼Œåè¾“å‡ºä»»åŠ¡è§„åˆ’
    v3.4 æ›´æ–°ï¼šä½¿ç”¨äº‹ä»¶é©±åŠ¨æµå¼è¾“å‡ºï¼Œé€šè¿‡ event_queue å®æ—¶æ¨é€ plan.thinking äº‹ä»¶
    """
    from agents.services.expert_manager import get_expert_config, get_expert_config_cached
    from agents.services.expert_manager import get_all_expert_list, format_expert_list_for_prompt
    from agents.services.task_manager import get_or_create_task_session
    from models import SubTaskCreate
    from utils.event_generator import (
        event_plan_created, event_plan_started, event_plan_thinking,
        sse_event_to_string
    )
    import uuid
    
    # ğŸ”¥ åˆå§‹åŒ–äº‹ä»¶é˜Ÿåˆ—ï¼ˆç”¨äºæ”¶é›†æ‰€æœ‰äº‹ä»¶ï¼‰
    event_queue = []
    
    messages = state["messages"]
    last_message = messages[-1]
    user_query = last_message.content if isinstance(last_message, HumanMessage) else str(last_message.content)
    
    # è·å– thread_id
    thread_id = state.get("thread_id")
    
    # ğŸ”¥ ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼ˆé¿å… MemorySaver åºåˆ—åŒ–é—®é¢˜ï¼‰
    with Session(engine) as db_session:
        try:
            # åŠ è½½é…ç½® (æ•°æ®åº“æˆ–å›é€€)
            commander_config = get_expert_config("commander", db_session)
            
            # å¦‚æœæ•°æ®åº“è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°ç¼“å­˜
            if not commander_config:
                commander_config = get_expert_config_cached("commander")
            
            if not commander_config:
                # å›é€€ï¼šä½¿ç”¨å¸¸é‡ä¸­çš„ Prompt å’Œç¡¬ç¼–ç çš„æ¨¡å‹
                system_prompt = COMMANDER_SYSTEM_PROMPT
                model = os.getenv("MODEL_NAME", "deepseek-chat")
                temperature = 0.5
                print(f"[COMMANDER] ä½¿ç”¨é»˜è®¤å›é€€é…ç½®: model={model}")
            else:
                # ä½¿ç”¨æ•°æ®åº“é…ç½®
                system_prompt = commander_config["system_prompt"]
                model = commander_config["model"]
                temperature = commander_config["temperature"]
                print(f"[COMMANDER] åŠ è½½é…ç½®: model={model}, temperature={temperature}")
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ Commander 2.0: å ä½ç¬¦è‡ªåŠ¨å¡«å……
            # å¡«å…… {user_query} å’Œ {dynamic_expert_list}
            try:
                # è·å–æ‰€æœ‰å¯ç”¨ä¸“å®¶ï¼ˆåŒ…æ‹¬åŠ¨æ€åˆ›å»ºçš„ä¸“å®¶ï¼‰
                all_experts = get_all_expert_list(db_session)
                expert_list_str = format_expert_list_for_prompt(all_experts)
                
                # æ„å»ºå ä½ç¬¦æ˜ å°„
                placeholder_map = {
                    "user_query": user_query,
                    "dynamic_expert_list": expert_list_str
                }
                
                # æ›¿æ¢æ‰€æœ‰æ”¯æŒçš„å ä½ç¬¦
                for placeholder, value in placeholder_map.items():
                    placeholder_pattern = f"{{{placeholder}}}"
                    if placeholder_pattern in system_prompt:
                        system_prompt = system_prompt.replace(placeholder_pattern, value)
                        print(f"[COMMANDER] å·²æ³¨å…¥å ä½ç¬¦: {{{placeholder}}}")
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªå¡«å……çš„å ä½ç¬¦ï¼ˆè­¦å‘Šä½†ä¸ä¸­æ–­ï¼‰
                import re
                remaining_placeholders = re.findall(r'\{([a-zA-Z_][a-zA-Z0-9_]*)\}', system_prompt)
                if remaining_placeholders:
                    print(f"[COMMANDER] è­¦å‘Š: ä»¥ä¸‹å ä½ç¬¦æœªå¡«å……: {remaining_placeholders}")
                    
            except Exception as e:
                # æ³¨å…¥å¤±è´¥æ—¶ä¸ä¸­æ–­æµç¨‹ï¼Œä¿ç•™åŸå§‹ Prompt
                print(f"[COMMANDER] å ä½ç¬¦å¡«å……å¤±è´¥ï¼ˆå·²å¿½ç•¥ï¼‰: {e}")
            
            # æ‰§è¡Œ LLM è¿›è¡Œè§„åˆ’
            # ä»æ¨¡å‹åç§°æ¨æ–­ provider
            from providers_config import get_model_config
            from agents.graph import get_commander_llm_lazy

            model_config = get_model_config(model)

            if model_config and 'provider' in model_config:
                # ä½¿ç”¨æ¨æ–­å‡ºçš„ provider åˆ›å»º LLM
                provider = model_config['provider']
                # ä¼˜å…ˆä½¿ç”¨æ¨¡å‹é…ç½®ä¸­çš„ temperatureï¼ˆå¦‚æœæœ‰ï¼‰
                final_temperature = model_config.get('temperature', temperature)
                # è·å–å®é™…çš„ API æ¨¡å‹åç§°ï¼ˆproviders.yaml ä¸­å®šä¹‰çš„ model å­—æ®µï¼‰
                actual_model = model_config.get('model', model)
                llm = get_llm_instance(
                    provider=provider,
                    streaming=True,
                    temperature=final_temperature
                )
                print(f"[COMMANDER] æ¨¡å‹ '{model}' -> '{actual_model}' ä½¿ç”¨ provider: {provider}, temperature: {final_temperature}")
                llm_with_config = llm.bind(model=actual_model, temperature=final_temperature)
            else:
                # å›é€€åˆ° commander_llmï¼ˆç¡¬ç¼–ç çš„ provider ä¼˜å…ˆçº§ï¼‰
                print(f"[COMMANDER] æ¨¡å‹ '{model}' æœªæ‰¾åˆ° provider é…ç½®ï¼Œå›é€€åˆ° commander_llm")
                llm_with_config = get_commander_llm_lazy().bind(model=model, temperature=temperature)

            # ğŸ”¥ğŸ”¥ğŸ”¥ Commander 2.0: JSON Mode + Pydantic å¼ºæ ¡éªŒ
            # 1ï¸âƒ£ è·å–æˆ–ç”Ÿæˆ session_id
            preview_session_id = state.get("preview_session_id") or str(uuid.uuid4())
            
            # ğŸ”¥ åªæœ‰åœ¨ chat.py æ²¡æœ‰å‘é€ plan.started çš„æƒ…å†µä¸‹ï¼Œæ‰åœ¨è¿™é‡Œå‘é€
            if not state.get("preview_session_id"):
                started_event = event_plan_started(
                    session_id=preview_session_id,
                    title="ä»»åŠ¡è§„åˆ’",
                    content="æ­£åœ¨åˆ†æéœ€æ±‚...",
                    status="running"
                )
                event_queue.append({"type": "sse", "event": sse_event_to_string(started_event)})
                print(f"[COMMANDER] å‘é€ plan.started: {preview_session_id}")
            else:
                print(f"[COMMANDER] å¤ç”¨ chat.py å‘é€çš„ plan.started: {preview_session_id}")
            
            # 2ï¸âƒ£ ä½¿ç”¨ JSON Mode + Pydantic å¼ºæ ¡éªŒç”Ÿæˆè®¡åˆ’
            # ğŸ”¥ Commander 2.0: DeepSeek å…¼å®¹çš„ JSON Mode å®ç°
            human_prompt = f"ç”¨æˆ·æŸ¥è¯¢: {user_query}\n\nè¯·åˆ†æéœ€æ±‚å¹¶ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ã€‚"
            
            print("[COMMANDER] ä½¿ç”¨ JSON Mode + Pydantic æ ¡éªŒç”Ÿæˆæ‰§è¡Œè®¡åˆ’...")
            commander_response = await _generate_plan_with_json_mode(
                llm_with_config, system_prompt, human_prompt, 
                preview_session_id, event_queue
            )

            # v3.1: å…œåº•å¤„ç† - å¦‚æœ LLM æ²¡æœ‰ç”Ÿæˆ idï¼Œè‡ªåŠ¨ç”Ÿæˆ
            for idx, task in enumerate(commander_response.tasks):
                if not task.id:
                    task.id = f"task_{idx}"
                    print(f"[COMMANDER] è‡ªåŠ¨ä¸ºä»»åŠ¡ {idx} ç”Ÿæˆ id: {task.id}")
            
            # v3.2: ä¿®å¤ä¾èµ–ä¸Šä¸‹æ–‡æ³¨å…¥ - å°† dependencies ä¸­çš„ç´¢å¼•æ ¼å¼è½¬æ¢ä¸º ID æ ¼å¼
            task_id_map = {str(idx): task.id for idx, task in enumerate(commander_response.tasks)}
            for task in commander_response.tasks:
                if task.dependencies:
                    new_dependencies = []
                    for dep in task.dependencies:
                        # å¦‚æœæ˜¯æ•°å­—ç´¢å¼•ï¼ˆå¦‚ "0"ï¼‰ï¼Œè½¬æ¢ä¸ºå¯¹åº”çš„ IDï¼ˆå¦‚ "task_0"ï¼‰
                        if dep in task_id_map:
                            new_dependencies.append(task_id_map[dep])
                        else:
                            # å¦‚æœå·²ç»æ˜¯æ­£ç¡®çš„ ID æ ¼å¼ï¼ˆå¦‚ "task_0"ï¼‰ï¼Œä¿æŒä¸å˜
                            new_dependencies.append(dep)
                    task.dependencies = new_dependencies
                    print(f"[COMMANDER] ä»»åŠ¡ {task.id} çš„ä¾èµ–å·²è½¬æ¢: {new_dependencies}")

            # v3.0: å‡†å¤‡å­ä»»åŠ¡æ•°æ®ï¼ˆæ”¯æŒæ˜¾å¼ä¾èµ–å…³ç³» DAGï¼‰
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¼ é€’ task_id ç”¨äº depends_on æ˜ å°„
            subtasks_data = [
                SubTaskCreate(
                    expert_type=task.expert_type,
                    task_description=task.description,
                    input_data=task.input_data,
                    sort_order=idx,
                    execution_mode="sequential",
                    depends_on=task.dependencies if task.dependencies else None,
                    task_id=task.id  # ğŸ”¥ å…³é”®ï¼šä¼ é€’ Commander ç”Ÿæˆçš„ task ID
                )
                for idx, task in enumerate(commander_response.tasks)
            ]

            # v3.0: ç«‹å³æŒä¹…åŒ–åˆ°æ•°æ®åº“ (é€šè¿‡ TaskManager)
            # ğŸ”¥ v3.3: ä½¿ç”¨ preview_session_id ç¡®ä¿äº‹ä»¶å’Œæ•°æ®åº“è®°å½•ä¸€è‡´
            task_session = None
            if db_session and thread_id:
                task_session, is_reused = get_or_create_task_session(
                    db=db_session,
                    thread_id=thread_id,
                    user_query=user_query,
                    plan_summary=commander_response.strategy,
                    estimated_steps=commander_response.estimated_steps,
                    subtasks_data=subtasks_data,
                    execution_mode="sequential",
                    session_id=preview_session_id  # ğŸ”¥ ä¼ å…¥é¢„è§ˆæ—¶ä½¿ç”¨çš„ session_id
                )
                session_source = "å¤ç”¨" if is_reused else "æ–°å»º"
                print(f"[COMMANDER] TaskSession {session_source}: {task_session.session_id}")
                
                # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ›´æ–° thread.task_session_idï¼Œç¡®ä¿å‰ç«¯èƒ½æŸ¥è¯¢åˆ°
                from models import Thread
                thread = db_session.get(Thread, thread_id)
                if thread:
                    thread.task_session_id = task_session.session_id
                    thread.agent_type = "ai"  # ğŸ”¥ åŒæ—¶æ›´æ–° agent_type
                    db_session.add(thread)
                    db_session.commit()
                    print(f"[COMMANDER] âœ… å·²æ›´æ–° thread.task_session_id: {task_session.session_id}")

            # è½¬æ¢ä¸ºå†…éƒ¨å­—å…¸æ ¼å¼ï¼ˆç”¨äº LangGraph çŠ¶æ€æµè½¬ï¼‰
            sub_tasks_list = task_session.sub_tasks if task_session else []
            task_list = []
            for idx, subtask in enumerate(sub_tasks_list):
                commander_task = commander_response.tasks[idx]
                task_list.append({
                    "id": subtask.id,
                    "task_id": commander_task.id,
                    "expert_type": subtask.expert_type,
                    "description": subtask.task_description,
                    "input_data": subtask.input_data,
                    "sort_order": subtask.sort_order,
                    "status": subtask.status,
                    "depends_on": commander_task.dependencies if commander_task.dependencies else [],
                    "output_result": None,
                    "started_at": None,
                    "completed_at": None
                })

            print(f"[COMMANDER] ç”Ÿæˆäº† {len(task_list)} ä¸ªä»»åŠ¡ã€‚ç­–ç•¥: {commander_response.strategy}")

            # P1 ä¼˜åŒ–: é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®åˆ°ç¼“å­˜
            await _preload_expert_configs(task_list, db_session)

            # ğŸ”¥ v3.3: ä½¿ç”¨ preview_session_id ä¿æŒä¸€è‡´æ€§ï¼ŒTaskSession åˆ›å»ºåä¼šä½¿ç”¨ç›¸åŒçš„ ID
            # æ³¨æ„ï¼šè¿™é‡Œä¸å†åˆ›å»ºæ–°çš„ event_queueï¼Œè€Œæ˜¯å¤ç”¨ä¹‹å‰çš„äº‹ä»¶é˜Ÿåˆ—
            
            # 4ï¸âƒ£ å‘é€ plan.created äº‹ä»¶ï¼ˆå®ŒæˆçŠ¶æ€ï¼‰
            if task_session:
                plan_event = event_plan_created(
                    session_id=task_session.session_id,
                    summary=commander_response.strategy,
                    estimated_steps=commander_response.estimated_steps,
                    execution_mode="sequential",
                    tasks=[
                        {
                            "id": t.id,
                            "task_id": commander_response.tasks[idx].id,
                            "expert_type": t.expert_type,
                            "description": t.task_description,
                            "sort_order": t.sort_order,
                            "status": t.status,
                            "depends_on": commander_response.tasks[idx].dependencies if commander_response.tasks[idx].dependencies else []
                        }
                        for idx, t in enumerate(task_session.sub_tasks)
                    ]
                )
                event_queue.append({"type": "sse", "event": sse_event_to_string(plan_event)})

            return {
                "task_list": task_list,
                "strategy": commander_response.strategy,
                "current_task_index": 0,
                "expert_results": [],
                "task_session_id": task_session.session_id if task_session else None,
                "event_queue": event_queue,
                # ä¿ç•™å‰ç«¯å…¼å®¹çš„å…ƒæ•°æ®
                "__task_plan": {
                    "task_count": len(task_list),
                    "strategy": commander_response.strategy,
                    "estimated_steps": commander_response.estimated_steps,
                    "tasks": task_list
                }
            }

        except Exception as e:
            logger.error(f"[ERROR] Commander è§„åˆ’å¤±è´¥: {e}", exc_info=True)
            return {
                "task_list": [],
                "strategy": f"Error: {str(e)}",
                "current_task_index": 0,
                "event_queue": []
            }


def _extract_json_string(content: str) -> str:
    """
    ä» LLM å“åº”ä¸­æå– JSON å­—ç¬¦ä¸²
    
    å¤„ç†ä»¥ä¸‹æƒ…å†µ:
    1. Markdown ä»£ç å— (```json ... ```)
    2. çº¯ JSON æ–‡æœ¬
    3. å‰åæœ‰é¢å¤–æ–‡æœ¬çš„æƒ…å†µ
    """
    content = content.strip()
    
    # æƒ…å†µ 1: Markdown ä»£ç å—
    if content.startswith("```"):
        lines = content.split("\n")
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ª ```
        start_idx = 0
        end_idx = len(lines) - 1
        
        # è·³è¿‡å¼€å¤´çš„ ``` æˆ– ```json
        for i, line in enumerate(lines):
            if line.strip().startswith("```"):
                start_idx = i + 1
                break
        
        # æ‰¾åˆ°ç»“å°¾çš„ ```
        for i in range(len(lines) - 1, -1, -1):
            if lines[i].strip() == "```":
                end_idx = i
                break
        
        json_content = "\n".join(lines[start_idx:end_idx])
        return json_content.strip()
    
    # æƒ…å†µ 2: å°è¯•æ‰¾åˆ° JSON å¯¹è±¡çš„å¼€å§‹å’Œç»“æŸ
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
    start = content.find("{")
    end = content.rfind("}")
    
    if start != -1 and end != -1 and end > start:
        return content[start:end+1]
    
    # æƒ…å†µ 3: å·²ç»æ˜¯çº¯ JSON
    return content


async def _generate_plan_once(
    llm_with_config,
    enhanced_system_prompt: str,
    human_prompt: str,
    preview_session_id: str,
    event_queue: list
) -> ExecutionPlan:
    """
    å•æ¬¡ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼ˆç”¨äº tenacity é‡è¯•ï¼‰
    """
    from utils.event_generator import event_plan_thinking, sse_event_to_string
    
    json_mode_llm = llm_with_config.bind(
        response_format={"type": "json_object"}
    )
    
    response = await json_mode_llm.ainvoke(
        [
            SystemMessage(content=enhanced_system_prompt),
            HumanMessage(content=human_prompt)
        ],
        config=RunnableConfig(
            tags=["commander", "json_mode"],
            metadata={"node_type": "commander", "mode": "json_object"}
        )
    )
    
    raw_content = response.content if hasattr(response, 'content') else str(response)
    
    # å‘é€ thinking äº‹ä»¶
    thinking_preview = raw_content[:200] + "..." if len(raw_content) > 200 else raw_content
    thinking_event = event_plan_thinking(
        session_id=preview_session_id,
        delta=f"[è§„åˆ’åˆ†æä¸­...]\n{thinking_preview}"
    )
    event_queue.append({"type": "sse", "event": sse_event_to_string(thinking_event)})
    
    # æå–å’Œæ ¡éªŒ JSON
    cleaned_content = _extract_json_string(raw_content)
    return ExecutionPlan.model_validate_json(cleaned_content)


@retry(
    retry=retry_if_exception_type((ValidationError, Exception)),
    stop=stop_after_attempt(2),
    wait=wait_fixed(0.5),
    before_sleep=before_sleep_log(logger, logging.WARNING),
    reraise=True
)
async def _generate_plan_with_json_mode(
    llm_with_config,
    system_prompt: str,
    human_prompt: str,
    preview_session_id: str,
    event_queue: list
) -> ExecutionPlan:
    """
    Commander 2.0: ä½¿ç”¨ JSON Mode + Pydantic å¼ºæ ¡éªŒç”Ÿæˆæ‰§è¡Œè®¡åˆ’
    
    P1 ä¼˜åŒ–: ä½¿ç”¨ tenacity ç»Ÿä¸€é‡è¯•æœºåˆ¶
    """
    enhanced_system_prompt = system_prompt + """

IMPORTANT: You MUST output a valid JSON object. No conversation, no markdown code blocks, just raw JSON text."""
    
    try:
        return await _generate_plan_once(
            llm_with_config, enhanced_system_prompt, human_prompt,
            preview_session_id, event_queue
        )
    except ValidationError as e:
        logger.warning(f"[COMMANDER] Pydantic æ ¡éªŒå¤±è´¥: {e}")
        raise
    except Exception as e:
        logger.warning(f"[COMMANDER] ç”Ÿæˆè®¡åˆ’å¤±è´¥: {e}")
        raise


# ä¿ç•™æ—§å‡½æ•°ä½œä¸ºå…œåº•ï¼ˆå½“ JSON Mode å®Œå…¨ä¸å¯ç”¨æ—¶ï¼‰
async def _streaming_planning_fallback(
    llm_with_config,
    system_prompt: str,
    human_prompt: str,
    preview_session_id: str,
    event_queue: list
) -> ExecutionPlan:
    """
    å…œåº•æ–¹æ¡ˆï¼šä½¿ç”¨æµå¼è§£æç”Ÿæˆæ‰§è¡Œè®¡åˆ’
    
    å½“ JSON Mode ä¹Ÿå®Œå…¨ä¸å¯ç”¨æ—¶ä½¿ç”¨
    """
    from utils.event_generator import event_plan_thinking, sse_event_to_string
    
    thinking_content = ""
    json_buffer = ""
    is_json_phase = False
    
    print("[COMMANDER] Fallback: ä½¿ç”¨æµå¼è§£æ...")
    
    async for chunk in llm_with_config.astream(
        [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ],
        config=RunnableConfig(
            tags=["commander", "streaming", "fallback"],
            metadata={"node_type": "commander", "mode": "fallback"}
        )
    ):
        content = chunk.content if hasattr(chunk, "content") else str(chunk)
        if not content:
            continue
        
        if not is_json_phase:
            if "```json" in content or "```" in content:
                is_json_phase = True
                before_json = content.split("```")[0]
                if before_json.strip():
                    thinking_content += before_json
                    thinking_event = event_plan_thinking(
                        session_id=preview_session_id,
                        delta=before_json
                    )
                    event_queue.append({"type": "sse", "event": sse_event_to_string(thinking_event)})
                json_parts = content.split("```", 1)
                if len(json_parts) > 1:
                    json_buffer += json_parts[1]
                continue
            
            thinking_content += content
            thinking_event = event_plan_thinking(
                session_id=preview_session_id,
                delta=content
            )
            event_queue.append({"type": "sse", "event": sse_event_to_string(thinking_event)})
        else:
            if "```" in content:
                json_parts = content.split("```", 1)
                json_buffer += json_parts[0]
            else:
                json_buffer += content
    
    # è§£æ JSON
    json_str = json_buffer.strip()
    if json_str.startswith("json"):
        json_str = json_str[4:].strip()
    
    try:
        commander_response = parse_llm_json(
            json_str,
            ExecutionPlan,
            strict=False,
            clean_markdown=False
        )
        print(f"[COMMANDER] æµå¼è§£ææˆåŠŸï¼Œç”Ÿæˆ {len(commander_response.tasks)} ä¸ªä»»åŠ¡")
        return commander_response
    except Exception as parse_err:
        print(f"[COMMANDER] æµå¼è§£æå¤±è´¥: {parse_err}")
        raise
