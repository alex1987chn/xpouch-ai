"""
é€šç”¨ä¸“å®¶æ‰§è¡ŒèŠ‚ç‚¹

ç”¨äºå¤„ç†åŠ¨æ€åˆ›å»ºçš„è‡ªå®šä¹‰ä¸“å®¶ï¼Œæ ¹æ® state["current_task"]["expert_type"]
ä»æ•°æ®åº“åŠ è½½ä¸“å®¶é…ç½®å¹¶æ‰§è¡Œã€‚
"""
import os
import re
import asyncio  # ğŸ”¥ æ–°å¢ï¼šç”¨äºå¼‚æ­¥ä¿å­˜ä¸“å®¶æ‰§è¡Œç»“æœ
from typing import Dict, Any, Optional
from datetime import datetime
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig

from agents.state import AgentState
from agents.services.expert_manager import get_expert_config_cached
from utils.llm_factory import get_effective_model, get_expert_llm
from providers_config import get_model_config
from services.memory_manager import memory_manager  # ğŸ”¥ å¯¼å…¥è®°å¿†ç®¡ç†å™¨
from tools import ALL_TOOLS  # ğŸ”¥ æ–°å¢ï¼šå¯¼å…¥å·¥å…·é›†

# ğŸ”¥ æ–°å¢ï¼šæ”¯æŒæµå¼ Artifact ç”Ÿæˆçš„ä¸“å®¶ç±»å‹
# è¿™äº›ä¸“å®¶é€šå¸¸ç”Ÿæˆé•¿æ–‡æœ¬å†…å®¹ï¼ˆæŠ¥å‘Šã€åˆ†æç­‰ï¼‰ï¼Œæµå¼ä½“éªŒæ›´å¥½
# ä¸åŒ…å«å¯èƒ½è°ƒç”¨å·¥å…·çš„ä¸“å®¶ï¼ˆsearch, coder ç­‰ï¼‰ä»¥é¿å…æµå¼å·¥å…·è§£æå¤æ‚æ€§
STREAMING_EXPERT_TYPES = {'writer', 'researcher', 'analyzer', 'planner'}


def _enhance_system_prompt(system_prompt: str) -> str:
    """
    ã€å¢å¼ºç‰ˆã€‘System Prompt æ³¨å…¥
    åŸå: _inject_current_time
    åŠŸèƒ½: æ³¨å…¥æ—¶é—´ + å¼ºåˆ¶å·¥å…·ä½¿ç”¨æŒ‡ä»¤ + é˜²å·æ‡’é€»è¾‘
    """
    now = datetime.now()
    weekdays = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"]
    weekday_str = weekdays[now.weekday()]
    time_str = now.strftime(f"%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S {weekday_str}")
    date_str = now.strftime("%Y-%m-%d")

    # ğŸ”¥ æ ¸å¿ƒå¢å¼ºï¼šç»™æ¨¡å‹æ´—è„‘ï¼Œå¼ºåˆ¶å®ƒä½¿ç”¨å·¥å…·ï¼Œç¦æ­¢è„‘è¡¥
    enhanced_prompt = f"""ã€å½“å‰ç³»ç»Ÿæ—¶é—´ã€‘ï¼š{time_str}
ã€å½“å‰æ—¥æœŸã€‘ï¼š{date_str}

{system_prompt}

ã€å·¥å…·ä½¿ç”¨å¼ºåˆ¶æŒ‡ä»¤ (Mandatory Tool Usage)ã€‘ï¼š
ä½ æ‹¥æœ‰å¼ºå¤§çš„å¤–éƒ¨å·¥å…·ï¼Œé’ˆå¯¹ä»¥ä¸‹æƒ…å†µ **å¿…é¡»** è°ƒç”¨å·¥å…·ï¼Œ**ä¸¥ç¦** ä»…å‡­è®­ç»ƒæ•°æ®å›ç­”ï¼š
1. **æ¶‰åŠå…·ä½“ URL**ï¼šå¦‚æœä»»åŠ¡åŒ…å« http/https é“¾æ¥ï¼ˆå¦‚ GitHub, æŠ€æœ¯åšå®¢ï¼‰ï¼Œ**å¿…é¡»** è°ƒç”¨ `read_webpage` è¯»å–å…¨æ–‡ã€‚
2. **æ¶‰åŠå‚æ•°å¯¹æ¯”/æœ€æ–°æŠ€æœ¯**ï¼šå¦‚æœä»»åŠ¡è¦æ±‚"ç ”ç©¶ DeepSeek-V3"ã€"å‚æ•°å¯¹æ¯”"ï¼Œ**å¿…é¡»** è°ƒç”¨ `search_web` æˆ– `read_webpage` è·å–ä¸€æ‰‹æ•°æ®ã€‚

ã€é˜²å·æ‡’åè®® (Anti-Laziness Protocol)ã€‘ï¼š
1. **ç¦æ­¢å¤ç”¨ä¸Šä¸‹æ–‡**ï¼šå³ä½¿ä½ è§‰å¾—ä¹‹å‰çš„å¯¹è¯é‡Œå¥½åƒæåˆ°è¿‡ç›¸å…³ä¿¡æ¯ï¼Œé’ˆå¯¹å½“å‰çš„å…·ä½“ä»»åŠ¡ï¼ˆç‰¹åˆ«æ˜¯ GitHub é˜…è¯»ä»»åŠ¡ï¼‰ï¼Œä½ ä¾ç„¶**å¿…é¡»**é‡æ–°æ‰§è¡Œå·¥å…·è°ƒç”¨ã€‚
2. **çœ‹åˆ° URL å°±å»è¯»**ï¼šä¸è¦ç›¯ç€ URL å‘å‘†ï¼Œä¸è¦çŒœæµ‹ URL é‡Œçš„å†…å®¹ã€‚ç›´æ¥è°ƒç”¨ `read_webpage`ï¼
3. **ä¸€æ­¥ä¸€åŠ¨**ï¼šä¸è¦è¯•å›¾åœ¨ä¸€ä¸ªå›åˆé‡ŒæŠŠæ‰€æœ‰äº‹åšå®Œã€‚å…ˆè°ƒå·¥å…· -> æ‹¿åˆ°ç»“æœ -> å†åˆ†æã€‚

ã€æ‰§è¡Œé€»è¾‘ã€‘ï¼š
æ£€æµ‹åˆ°ä»»åŠ¡éœ€æ±‚ -> å†³å®šå·¥å…· (Search æˆ– Read) -> **è¾“å‡º Tool Call** -> (ç­‰å¾…æ‰§è¡Œ) -> è·å– Artifact -> ç”Ÿæˆå›ç­”ã€‚
"""
    return enhanced_prompt


async def generic_worker_node(state: Dict[str, Any], llm=None) -> Dict[str, Any]:
    """
    é€šç”¨ä¸“å®¶æ‰§è¡ŒèŠ‚ç‚¹

    æ ¹æ® state["current_task"]["expert_type"] ä»æ•°æ®åº“åŠ è½½ä¸“å®¶é…ç½®å¹¶æ‰§è¡Œã€‚
    ç”¨äºå¤„ç†åŠ¨æ€åˆ›å»ºçš„è‡ªå®šä¹‰ä¸“å®¶ã€‚

    æ”¯æŒå·¥å…·è°ƒç”¨æµç¨‹ï¼š
    1. é¦–æ¬¡è°ƒç”¨ï¼šLLM å¯èƒ½è¿”å› tool_calls
    2. å·¥å…·æ‰§è¡Œåï¼šLLM çœ‹åˆ° ToolMessageï¼Œç”Ÿæˆæœ€ç»ˆå›å¤

    ğŸ”¥ v3.2 æ–°å¢ï¼šæ”¯æŒ Artifact å®æ—¶æµå¼æ¸²æŸ“ï¼ˆReal-time Streamingï¼‰
    - writer, researcher, analyzer, planner ç­‰ä¸“å®¶ä½¿ç”¨ astream æµå¼ç”Ÿæˆ
    - search, coder ç­‰å¯èƒ½è°ƒç”¨å·¥å…·çš„ä¸“å®¶ä¿æŒ ainvoke æ¨¡å¼

    Args:
        state: AgentStateï¼ŒåŒ…å« task_list, current_task_index ç­‰
        llm: å¯é€‰çš„ LLM å®ä¾‹ï¼Œå¦‚æœä¸æä¾›åˆ™æ ¹æ®ä¸“å®¶é…ç½®åˆ›å»º

    Returns:
        Dict: æ‰§è¡Œç»“æœï¼ŒåŒ…å« output_result, status, artifact ç­‰
    """
    from langchain_core.messages import ToolMessage

    # è·å–å½“å‰ä»»åŠ¡
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    existing_messages = state.get("messages", [])
    
    if current_index >= len(task_list):
        return {
            "output_result": "æ²¡æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡",
            "status": "failed",
            "error": "Task index out of range",
            "started_at": datetime.now().isoformat(),
            "completed_at": datetime.now().isoformat()
        }
    
    current_task = task_list[current_index]
    expert_type = current_task.get("expert_type", "")
    description = current_task.get("description", "")
    input_data = current_task.get("input_data", {})
    
    # ğŸ”¥ åˆ¤æ–­æ˜¯å¦ä¸ºæµå¼ä¸“å®¶ï¼ˆæ”¯æŒå®æ—¶ Artifact æ¸²æŸ“ï¼‰
    is_streaming_expert = expert_type in STREAMING_EXPERT_TYPES
    
    if not expert_type:
        return {
            "output_result": "ä»»åŠ¡ç¼ºå°‘ expert_type å­—æ®µ",
            "status": "failed",
            "error": "Missing expert_type in task",
            "started_at": datetime.now().isoformat(),
            "completed_at": datetime.now().isoformat()
        }
    
    # ä»ç¼“å­˜åŠ è½½ä¸“å®¶é…ç½®
    expert_config = get_expert_config_cached(expert_type)
    
    # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå¯èƒ½æ˜¯è‡ªå®šä¹‰ä¸“å®¶ï¼Œå°è¯•ç›´æ¥æŸ¥æ•°æ®åº“
    if not expert_config:
        print(f"[GenericWorker] ç¼“å­˜ä¸­æœªæ‰¾åˆ° '{expert_type}'ï¼Œå°è¯•ä»æ•°æ®åº“åŠ è½½...")
        from database import engine
        from sqlmodel import Session
        from agents.services.expert_manager import get_expert_config
        
        with Session(engine) as session:
            expert_config = get_expert_config(expert_type, session)
            if expert_config:
                print(f"[GenericWorker] ä»æ•°æ®åº“åŠ è½½ '{expert_type}' æˆåŠŸ")
    
    if not expert_config:
        return {
            "output_result": f"ä¸“å®¶ '{expert_type}' æœªæ‰¾åˆ°",
            "status": "failed",
            "error": f"Expert '{expert_type}' not found in database",
            "started_at": datetime.now().isoformat(),
            "completed_at": datetime.now().isoformat()
        }
    
    started_at = datetime.now()

    # âœ… å‘é€ task.started äº‹ä»¶ï¼ˆä¸“å®¶å¼€å§‹æ‰§è¡Œï¼‰
    from utils.event_generator import event_task_started, sse_event_to_string
    task_id = current_task.get("id", str(current_index))
    started_event = event_task_started(
        task_id=task_id,
        expert_type=expert_type,
        description=description
    )
    # å°† started äº‹ä»¶æ”¾å…¥ state çš„ event_queueï¼Œè®© dispatcher æˆ–å…¶ä»–èŠ‚ç‚¹å¤„ç†
    initial_event_queue = state.get("event_queue", [])
    initial_event_queue.append({"type": "sse", "event": sse_event_to_string(started_event)})
    print(f"[GenericWorker] å·²ç”Ÿæˆ task.started äº‹ä»¶: {expert_type}")

    try:
        # è·å–ä¸“å®¶é…ç½®å‚æ•°
        system_prompt = expert_config["system_prompt"]
        expert_name = expert_config.get("name", expert_type)
        
        # åº”ç”¨æ¨¡å‹å…œåº•æœºåˆ¶
        configured_model = expert_config.get("model")
        effective_model = get_effective_model(configured_model)
        
        # è·å–æ¨¡å‹é…ç½®ä»¥ç¡®å®šå®é™…çš„ API æ¨¡å‹åç§°å’Œæ¸©åº¦
        model_config = get_model_config(effective_model)
        if model_config:
            actual_model = model_config.get("model", effective_model)
            temperature = model_config.get("temperature", expert_config.get("temperature", 0.7))
        else:
            actual_model = effective_model
            temperature = expert_config.get("temperature", 0.7)
        
        print(f"[GenericWorker] Running '{expert_type}' ({expert_name}) with model={actual_model}, temp={temperature}")
        
        # å¦‚æœæ²¡æœ‰æä¾› LLM å®ä¾‹ï¼Œæ ¹æ®é…ç½®åˆ›å»º
        if llm is None:
            # æ ¹æ®æ¨¡å‹é…ç½®è·å– provider
            if model_config:
                provider = model_config.get("provider")
                llm = get_expert_llm(provider=provider, model=actual_model, temperature=temperature)
            else:
                llm = get_expert_llm(model=actual_model, temperature=temperature)

        # ç»‘å®šæ¨¡å‹å’Œæ¸©åº¦å‚æ•°
        llm_with_config = llm.bind(
            model=actual_model,
            temperature=temperature
        )

        # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå¢å¼º System Prompt (æ³¨å…¥æ—¶é—´ + å·¥å…·æŒ‡ä»¤)
        enhanced_system_prompt = _enhance_system_prompt(system_prompt)

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ„å»ºæ¶ˆæ¯åˆ—è¡¨
        # å¦‚æœæœ‰ç°æœ‰çš„ messagesï¼ˆåŒ…å« ToolMessageï¼‰ï¼Œåˆ™ä½¿ç”¨å®ƒä»¬
        # å¦åˆ™åˆ›å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
        has_tool_message = False
        if existing_messages:
            # å·¥å…·æ‰§è¡Œåçš„æƒ…å†µï¼šmessages åŒ…å« AIMessage(tool_calls) + ToolMessage
            # æˆ‘ä»¬éœ€è¦ä¿ç•™è¿™äº›ä¸Šä¸‹æ–‡ï¼Œè®© LLM çœ‹åˆ°å·¥å…·ç»“æœ
            # æ£€æŸ¥æœ€åä¸€æ¡æ˜¯å¦æ˜¯ ToolMessage
            if existing_messages and isinstance(existing_messages[-1], ToolMessage):
                has_tool_message = True
            messages_for_llm = [
                SystemMessage(content=enhanced_system_prompt),
                *existing_messages  # åŒ…å« AIMessage(tool_calls) å’Œ ToolMessage
            ]
        else:
            # é¦–æ¬¡è°ƒç”¨ï¼šåˆ›å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
            messages_for_llm = [
                SystemMessage(content=enhanced_system_prompt),
                HumanMessage(content=f"ä»»åŠ¡æè¿°: {description}\n\nè¾“å…¥å‚æ•°:\n{_format_input_data(input_data)}")
            ]

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ¹æ®æ˜¯å¦æœ‰ ToolMessage å†³å®šæ˜¯å¦ç»‘å®šå·¥å…·
        # å¦‚æœå·²ç»æœ‰ ToolMessageï¼ˆå·¥å…·æ‰§è¡Œå®Œæˆï¼‰ï¼Œåˆ™ä¸ç»‘å®šå·¥å…·ï¼Œé˜²æ­¢æ— é™å¾ªç¯
        if has_tool_message:
            llm_to_use = llm_with_config
        else:
            # ğŸ”¥ æ–°å¢ï¼šä¸ºæ‰€æœ‰ä¸“å®¶ç»‘å®šå·¥å…·ï¼ˆè”ç½‘æœç´¢ã€æ—¶é—´ã€è®¡ç®—å™¨ï¼‰
            # å¦‚æœ LLM æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œåˆ™ç»‘å®šå·¥å…·é›†
            try:
                llm_to_use = llm_with_config.bind_tools(ALL_TOOLS)
            except Exception as e:
                print(f"[GenericWorker] âš ï¸ å·¥å…·ç»‘å®šå¤±è´¥ï¼ˆæ¨¡å‹å¯èƒ½ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼‰: {e}")
                llm_to_use = llm_with_config

        # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå½“ has_tool_message=True æ—¶ï¼Œåœ¨æ¶ˆæ¯æœ«å°¾æ·»åŠ æ˜ç¡®çš„"ä»»åŠ¡å®Œæˆ"æç¤º
        if has_tool_message:
            # åœ¨æ¶ˆæ¯åˆ—è¡¨æœ«å°¾æ·»åŠ ä¸€ä¸ª HumanMessageï¼Œæ˜ç¡®å‘Šè¯‰ LLM ä»»åŠ¡å®Œæˆ
            messages_for_llm.append(HumanMessage(content="[ç³»ç»Ÿæç¤ºï¼šä»¥ä¸Šæ˜¯å·¥å…·æ‰§è¡Œç»“æœï¼Œè¯·åŸºäºæ­¤ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤ï¼Œä»»åŠ¡å·²å®Œæˆï¼Œä¸è¦å†è°ƒç”¨ä»»ä½•å·¥å…·]"))

        # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒæ”¹åŠ¨ï¼šæµå¼ vs éæµå¼åˆ†æ”¯
        if is_streaming_expert and not has_tool_message:
            # ================================================================
            # ğŸ”¥ æµå¼æ¨¡å¼ï¼šä½¿ç”¨ astream å®æ—¶å‘é€ Artifact chunks
            # é€‚ç”¨äº writer, researcher, analyzer, planner ç­‰ç”Ÿæˆé•¿æ–‡æœ¬çš„ä¸“å®¶
            # ================================================================
            response, artifact_id, full_content = await _handle_streaming_response(
                llm_to_use=llm_to_use,
                messages_for_llm=messages_for_llm,
                expert_type=expert_type,
                expert_name=expert_name,
                task_id=task_id,
                initial_event_queue=initial_event_queue
            )
            has_tool_calls = False  # æµå¼æ¨¡å¼ä¸‹ä¸å¤„ç†å·¥å…·è°ƒç”¨
        else:
            # ================================================================
            # ğŸ”¥ éæµå¼æ¨¡å¼ï¼šä½¿ç”¨ ainvoke ç­‰å¾…å®Œæ•´å“åº”
            # é€‚ç”¨äº search, coder ç­‰å¯èƒ½è°ƒç”¨å·¥å…·çš„ä¸“å®¶
            # ================================================================
            response = await llm_to_use.ainvoke(
                messages_for_llm,
                config=RunnableConfig(
                    tags=["expert", expert_type, "generic_worker"],
                    metadata={"node_type": "expert", "expert_type": expert_type}
                )
            )
            artifact_id = None  # éæµå¼æ¨¡å¼ç¨åç”Ÿæˆ
            full_content = None

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
        has_tool_calls = hasattr(response, "tool_calls") and response.tool_calls

        if has_tool_calls:
            print(f"[GenericWorker] ğŸ”§ LLM è¿”å›äº†å·¥å…·è°ƒç”¨ï¼æ•°é‡: {len(response.tool_calls)}")
            for tool_call in response.tool_calls:
                print(f"[GenericWorker]   - å·¥å…·: {tool_call.get('name', 'unknown')}")
            # ğŸ”¥ğŸ”¥ å…³é”®ï¼šè¿”å› messages è®© ToolNode å¤„ç†å·¥å…·è°ƒç”¨
            # æ­¤æ—¶ä¸ç”Ÿæˆ task.completed äº‹ä»¶ï¼Œå› ä¸ºä»»åŠ¡è¿˜æ²¡å®Œæˆ
            return {
                "messages": [response],  # åŒ…å« tool_calls çš„ AIMessage
                "task_list": task_list,
                "current_task_index": current_index,  # ä¸å¢åŠ  indexï¼Œç­‰å·¥å…·æ‰§è¡Œå®Œå†è¯´
                "event_queue": initial_event_queue,  # åªè¿”å› started äº‹ä»¶
                "__expert_info": {
                    "expert_type": expert_type,
                    "expert_name": expert_name,
                    "task_id": task_id,
                    "status": "waiting_for_tool",
                    "tool_calls": response.tool_calls
                }
            }

        # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ­£å¸¸å®Œæˆä»»åŠ¡
        print(f"[GenericWorker] â„¹ï¸ LLM è¿”å›äº†æ™®é€šæ–‡æœ¬å“åº”ï¼Œæœªè°ƒç”¨å·¥å…·")

        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)

        print(f"[GenericWorker] '{expert_type}' completed (è€—æ—¶: {duration_ms/1000:.2f}s)")

        # -------------------------------------------------------------
        # ğŸ”¥ æ–°å¢é€»è¾‘ï¼šå¦‚æœæ˜¯è®°å¿†ä¸“å®¶ï¼Œæ‰§è¡Œ"å†™å…¥æ•°æ®åº“"æ“ä½œ
        # -------------------------------------------------------------
        if expert_type == "memorize_expert":
            memory_content = response.content.strip()
            # ä» state è·å– user_idï¼Œé»˜è®¤ä½¿ç”¨ default_user
            user_id = state.get("user_id", "default_user")
            
            if memory_content:
                print(f"[GenericWorker] æ­£åœ¨ä¿å­˜è®°å¿†: {memory_content}")
                try:
                    # å¼‚æ­¥è°ƒç”¨ memory_manager ä¿å­˜ (å†…éƒ¨ä½¿ç”¨äº† to_thread)
                    await memory_manager.add_memory(
                        user_id=user_id,
                        content=memory_content,
                        source="conversation",
                        memory_type="fact"
                    )
                    print(f"[GenericWorker] è®°å¿†ä¿å­˜æˆåŠŸ!")
                    # ä¿®æ”¹è¿”å›ç»™ç”¨æˆ·çš„ outputï¼Œè®©åé¦ˆæ›´è‡ªç„¶
                    response_content_original = response.content
                    response.content = f"å·²ä¸ºæ‚¨è®°å½•ï¼š{response_content_original}"
                except Exception as mem_err:
                    print(f"[GenericWorker] è®°å¿†ä¿å­˜å¤±è´¥: {mem_err}")
                    response.content = f"è®°å½•æ—¶é‡åˆ°é—®é¢˜ï¼Œä½†æˆ‘ä¼šè®°ä½ï¼š{memory_content}"
        # -------------------------------------------------------------

        # ğŸ”¥ æ£€æµ‹ artifact ç±»å‹ï¼ˆæµå¼æ¨¡å¼ä¸‹ä½¿ç”¨å·²ç´¯ç§¯çš„å†…å®¹ï¼‰
        content_for_detection = full_content if full_content else response.content
        artifact_type = _detect_artifact_type(content_for_detection, expert_type)

        # âœ… v3.2 ä¿®å¤ï¼šå¢åŠ  current_task_index ä»¥æ”¯æŒå¾ªç¯
        # Generic Worker æ‰§è¡Œå®Œä»»åŠ¡åï¼Œéœ€è¦é€’å¢ index æ‰èƒ½æ‰§è¡Œä¸‹ä¸€ä¸ªä»»åŠ¡
        next_index = current_index + 1

        # âœ… æ›´æ–°ä»»åŠ¡åˆ—è¡¨ä¸­çš„ä»»åŠ¡çŠ¶æ€
        task_list[current_index]["output_result"] = {"content": response.content}
        task_list[current_index]["status"] = "completed"
        task_list[current_index]["completed_at"] = completed_at.isoformat()

        # âœ… æ·»åŠ åˆ° expert_resultsï¼ˆç”¨äºåç»­ä»»åŠ¡ä¾èµ–å’Œæœ€ç»ˆèšåˆï¼‰
        expert_result = {
            "task_id": current_task.get("id", str(current_index)),
            "expert_type": expert_type,
            "description": description,
            "output": response.content,
            "status": "completed",
            "duration_ms": duration_ms
        }

        # è·å–ç°æœ‰çš„ expert_results å¹¶è¿½åŠ æ–°ç»“æœ
        expert_results = state.get("expert_results", [])
        expert_results = expert_results + [expert_result]

        # ğŸ”¥ ç”Ÿæˆæˆ–å¤ç”¨ artifact_id
        if artifact_id is None:
            # éæµå¼æ¨¡å¼ï¼šç”Ÿæˆæ–°çš„ artifact_id
            from uuid import uuid4
            artifact_id = str(uuid4())

        # âœ… æ„å»º artifact å¯¹è±¡ï¼ˆç¬¦åˆ ArtifactCreate æ¨¡å‹ï¼‰
        # ğŸ”¥ å…³é”®ï¼šåŒ…å« artifact_idï¼Œç¡®ä¿ä¸æµå¼è¿‡ç¨‹ä¸­çš„ ID ä¸€è‡´
        artifact = {
            "type": artifact_type,
            "title": f"{expert_name}ç»“æœ",
            "content": response.content,
            "language": None,  # å¯é€‰å­—æ®µï¼ŒPydantic æ¨¡å‹éœ€è¦
            "sort_order": 0,   # é»˜è®¤æ’åº
            "artifact_id": artifact_id  # ğŸ”¥ å…³é”®ï¼šä¿æŒ ID ä¸€è‡´æ€§
        }

        # âœ… å¼‚æ­¥ä¿å­˜ä¸“å®¶æ‰§è¡Œç»“æœåˆ°æ•°æ®åº“ï¼ˆP0 ä¼˜åŒ–ï¼šä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        # ğŸ”¥ ä¿®å¤ï¼šä¸ä¼ é€’ db_sessionï¼Œåœ¨ async_save_expert_result ä¸­åˆ›å»ºç‹¬ç«‹çš„ Session
        if task_id:
            try:
                from utils.async_task_queue import async_save_expert_result
                # ä½¿ç”¨åå°çº¿ç¨‹å¼‚æ­¥ä¿å­˜ï¼Œä¸é˜»å¡ LLM å“åº”è¿”å›
                asyncio.create_task(async_save_expert_result(
                    task_id=task_id,
                    expert_type=expert_type,
                    output_result=response.content,
                    artifact_data=artifact,
                    duration_ms=duration_ms
                ))
                print(f"[GenericWorker] âœ… ä¸“å®¶æ‰§è¡Œç»“æœå·²æäº¤åå°çº¿ç¨‹æ± ä¿å­˜: {expert_type}")
            except Exception as save_err:
                print(f"[GenericWorker] âš ï¸ åå°ä¿å­˜æäº¤å¤±è´¥: {save_err}")
        else:
            print(f"[GenericWorker] âš ï¸ è·³è¿‡ä¿å­˜: task_id={task_id}")

        # âœ… ç”Ÿæˆäº‹ä»¶é˜Ÿåˆ—ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºä¸“å®¶å’Œ artifactï¼‰
        from utils.event_generator import (
            event_task_completed, event_artifact_generated, sse_event_to_string
        )

        event_queue = []

        # ğŸ”¥ æµå¼æ¨¡å¼ä¸‹ï¼šå‘é€ artifact.completed äº‹ä»¶
        # éæµå¼æ¨¡å¼ä¸‹ï¼šå‘é€ artifact.generated äº‹ä»¶
        if is_streaming_expert and not has_tool_message:
            # æµå¼æ¨¡å¼ï¼šå‘é€ artifact.completed å®Œæˆäº‹ä»¶
            from utils.event_generator import event_artifact_completed
            artifact_completed_event = event_artifact_completed(
                artifact_id=artifact_id,
                task_id=task_id,
                expert_type=expert_type,
                full_content=response.content
            )
            event_queue.append({"type": "sse", "event": sse_event_to_string(artifact_completed_event)})
            print(f"[GenericWorker] å·²ç”Ÿæˆ artifact.completed äº‹ä»¶: {artifact_id}")
        else:
            # éæµå¼æ¨¡å¼ï¼šå‘é€ä¼ ç»Ÿçš„ artifact.generated äº‹ä»¶
            artifact_event = event_artifact_generated(
                task_id=task_id,
                expert_type=expert_type,
                artifact_id=artifact_id,
                artifact_type=artifact_type,
                content=response.content,
                title=f"{expert_name}ç»“æœ"
            )
            event_queue.append({"type": "sse", "event": sse_event_to_string(artifact_event)})
            print(f"[GenericWorker] å·²ç”Ÿæˆ artifact.generated äº‹ä»¶: {artifact_type}")

        # 1. å‘é€ task.completed äº‹ä»¶ï¼ˆä¸“å®¶æ‰§è¡Œå®Œæˆï¼‰
        task_completed_event = event_task_completed(
            task_id=task_id,
            expert_type=expert_type,
            description=description,
            output=response.content[:500] + "..." if len(response.content) > 500 else response.content,
            duration_ms=duration_ms,
            artifact_count=1
        )
        event_queue.append({"type": "sse", "event": sse_event_to_string(task_completed_event)})
        print(f"[GenericWorker] å·²ç”Ÿæˆ task.completed äº‹ä»¶: {expert_type}")

        # âœ… åˆå¹¶ started äº‹ä»¶å’Œ completed äº‹ä»¶
        full_event_queue = initial_event_queue + event_queue

        return {
            "messages": [response],  # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šå¿…é¡»æŠŠ LLM çš„æœ€ç»ˆå›å¤æ›´æ–°åˆ°å›¾çŠ¶æ€çš„æ¶ˆæ¯å†å²ä¸­ï¼ğŸ”¥ğŸ”¥ğŸ”¥
            "task_list": task_list,
            "expert_results": expert_results,
            "current_task_index": next_index,  # âœ… å¢åŠ  index
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms,
            "artifact": artifact,
            "event_queue": full_event_queue,  # âœ… æ·»åŠ å®Œæ•´äº‹ä»¶é˜Ÿåˆ—ï¼ˆåŒ…å« started å’Œ completedï¼‰
            # âœ… æ·»åŠ  __expert_info ç”¨äº chat.py è¯†åˆ«å’Œæ”¶é›† artifacts
            "__expert_info": {
                "expert_type": expert_type,
                "expert_name": expert_name,
                "task_id": task_id,
                "status": "completed",
                "artifact_id": artifact_id  # ğŸ”¥ åŒ…å« artifact_id
            }
        }
        
    except Exception as e:
        print(f"[GenericWorker] '{expert_type}' failed: {e}")

        # âœ… å¤±è´¥æ—¶ä¹Ÿè¦å¢åŠ  indexï¼Œå¦åˆ™ä¼šå¡æ­»å¾ªç¯
        next_index = current_index + 1

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        task_list[current_index]["status"] = "failed"

        # è·å–ç°æœ‰çš„ expert_results å¹¶æ·»åŠ å¤±è´¥è®°å½•
        expert_results = state.get("expert_results", [])
        task_id = current_task.get("id", str(current_index))
        expert_result = {
            "task_id": task_id,
            "expert_type": expert_type,
            "description": description,
            "output": f"ä¸“å®¶æ‰§è¡Œå¤±è´¥: {str(e)}",
            "status": "failed",
            "error": str(e),
            "duration_ms": 0
        }
        expert_results = expert_results + [expert_result]

        # âœ… ç”Ÿæˆ task.failed äº‹ä»¶
        from utils.event_generator import event_task_failed, sse_event_to_string

        event_queue = []
        failed_event = event_task_failed(
            task_id=task_id,
            expert_type=expert_type,
            description=description,
            error=str(e)
        )
        event_queue.append({"type": "sse", "event": sse_event_to_string(failed_event)})
        print(f"[GenericWorker] å·²ç”Ÿæˆ task.failed äº‹ä»¶: {expert_type}")

        # âœ… åˆå¹¶ started äº‹ä»¶å’Œ failed äº‹ä»¶
        full_event_queue = initial_event_queue + event_queue

        return {
            "task_list": task_list,
            "expert_results": expert_results,
            "current_task_index": next_index,  # âœ… å³ä½¿å¤±è´¥ä¹Ÿå¢åŠ  index
            "output_result": f"ä¸“å®¶æ‰§è¡Œå¤±è´¥: {str(e)}",
            "status": "failed",
            "error": str(e),
            "started_at": started_at.isoformat(),
            "completed_at": datetime.now().isoformat(),
            "event_queue": full_event_queue,  # âœ… æ·»åŠ å®Œæ•´äº‹ä»¶é˜Ÿåˆ—ï¼ˆåŒ…å« started å’Œ failedï¼‰
            # âœ… æ·»åŠ  __expert_info ç”¨äºæ ‡è¯†å¤±è´¥çš„ä¸“å®¶
            "__expert_info": {
                "expert_type": expert_type,
                "expert_name": expert_config.get("name", expert_type) if expert_config else expert_type,
                "task_id": task_id,
                "status": "failed",
                "error": str(e)
            }
        }


async def _handle_streaming_response(
    llm_to_use,
    messages_for_llm: list,
    expert_type: str,
    expert_name: str,
    task_id: str,
    initial_event_queue: list
) -> tuple:
    """
    ğŸ”¥ å¤„ç†æµå¼ LLM å“åº”ï¼ˆReal-time Artifact Streamingï¼‰
    
    ä½¿ç”¨ astream å®æ—¶ç”Ÿæˆå†…å®¹ï¼Œå¹¶é€šè¿‡ SSE å‘é€ artifact chunks åˆ°å‰ç«¯ã€‚
    
    Args:
        llm_to_use: é…ç½®å¥½çš„ LLM å®ä¾‹
        messages_for_llm: æ¶ˆæ¯åˆ—è¡¨
        expert_type: ä¸“å®¶ç±»å‹
        expert_name: ä¸“å®¶åç§°
        task_id: ä»»åŠ¡ID
        initial_event_queue: åˆå§‹äº‹ä»¶é˜Ÿåˆ—ï¼ˆç”¨äºç´¯ç§¯ chunk äº‹ä»¶ï¼‰
    
    Returns:
        tuple: (AIMessage response, artifact_id, full_content)
    """
    from uuid import uuid4
    from langchain_core.messages import AIMessage
    from utils.event_generator import event_artifact_start, event_artifact_chunk, sse_event_to_string
    
    # ğŸ”¥ Step 1: é¢„ç”Ÿæˆ artifact_idï¼ˆä¿è¯æ•´ä¸ªæµç¨‹ ID ä¸€è‡´ï¼‰
    artifact_id = str(uuid4())
    
    # é¢„è®¾ artifact ç±»å‹ï¼ˆåŸºäºä¸“å®¶ç±»å‹æ¨æ–­ï¼‰
    type_mapping = {
        'writer': 'markdown',
        'researcher': 'markdown',
        'analyzer': 'markdown',
        'planner': 'markdown'
    }
    artifact_type = type_mapping.get(expert_type, 'text')
    
    print(f"[Streaming] å¼€å§‹æµå¼ç”Ÿæˆ Artifact: {artifact_id} (expert: {expert_type})")
    
    # ğŸ”¥ Step 2: å‘é€ artifact.start äº‹ä»¶
    start_event = event_artifact_start(
        task_id=task_id,
        expert_type=expert_type,
        artifact_id=artifact_id,
        title=f"{expert_name}ç»“æœ",
        type=artifact_type
    )
    initial_event_queue.append({"type": "sse", "event": sse_event_to_string(start_event)})
    print(f"[Streaming] å·²å‘é€ artifact.start: {artifact_id}")
    
    # ğŸ”¥ Step 3: ä½¿ç”¨ astream æµå¼ç”Ÿæˆ
    full_content = ""
    chunk_count = 0
    
    try:
        async for chunk in llm_to_use.astream(
            messages_for_llm,
            config=RunnableConfig(
                tags=["expert", expert_type, "generic_worker", "streaming"],
                metadata={"node_type": "expert", "expert_type": expert_type, "mode": "streaming"}
            )
        ):
            # æå–å¢é‡å†…å®¹
            content_delta = chunk.content if hasattr(chunk, "content") else str(chunk)
            
            if content_delta:
                full_content += content_delta
                chunk_count += 1
                
                # ğŸ”¥ å‘é€ artifact.chunk äº‹ä»¶ï¼ˆå®æ—¶æ¨é€åˆ°å‰ç«¯ï¼‰
                chunk_event = event_artifact_chunk(
                    artifact_id=artifact_id,
                    delta=content_delta
                )
                initial_event_queue.append({"type": "sse", "event": sse_event_to_string(chunk_event)})
                
                # æ¯ 10 ä¸ª chunk æ‰“å°ä¸€æ¬¡æ—¥å¿—ï¼Œé¿å…æ—¥å¿—åˆ·å±
                if chunk_count % 10 == 0:
                    print(f"[Streaming] å·²å‘é€ {chunk_count} chunks, å†…å®¹é•¿åº¦: {len(full_content)}")
        
        print(f"[Streaming] æµå¼ç”Ÿæˆå®Œæˆ: {chunk_count} chunks, æ€»é•¿åº¦: {len(full_content)}")
        
    except Exception as e:
        print(f"[Streaming] æµå¼ç”Ÿæˆå‡ºé”™: {e}")
        # å³ä½¿å‡ºé”™ä¹Ÿè¿”å›å·²ç”Ÿæˆçš„å†…å®¹
    
    # ğŸ”¥ Step 4: æ„å»º AIMessage è¿”å›ï¼ˆä¸ ainvoke è¿”å›æ ¼å¼ä¸€è‡´ï¼‰
    response = AIMessage(content=full_content)
    
    return response, artifact_id, full_content


def _format_input_data(data: Dict) -> str:
    """æ ¼å¼åŒ–è¾“å…¥æ•°æ®ä¸ºæ–‡æœ¬"""
    if not data:
        return "ï¼ˆæ— é¢å¤–å‚æ•°ï¼‰"
    
    lines = []
    for key, value in data.items():
        if isinstance(value, (list, dict)):
            lines.append(f"- {key}: {value}")
        else:
            lines.append(f"- {key}: {value}")
    
    return "\n".join(lines)


def _detect_artifact_type(content: str, expert_key: str) -> str:
    """
    æ£€æµ‹ artifact ç±»å‹
    
    ç®€åŒ–ç‰ˆï¼Œé»˜è®¤è¿”å› "text"ï¼Œä½†ä¼šå°è¯•æ£€æµ‹ HTML å’Œ Markdown å†…å®¹ã€‚
    """
    content_lower = content.lower().strip()
    
    # 1. HTML æ£€æµ‹
    if (content_lower.startswith("<!doctype html") or
        content_lower.startswith("<html") or
        ("<html" in content_lower and "</html>" in content_lower)):
        return "html"
    
    # æ£€æµ‹ HTML ä»£ç å—
    html_code_block = re.search(r'```html\n([\s\S]*?)```', content, re.IGNORECASE)
    if html_code_block:
        return "html"
    
    # 2. Markdown æ£€æµ‹
    has_markdown = any(marker in content for marker in ['# ', '## ', '### ', '> ', '- ', '* '])
    has_code_block = '```' in content
    
    if has_markdown or has_code_block:
        return "markdown"
    
    # 3. é»˜è®¤è¿”å› text
    return "text"
