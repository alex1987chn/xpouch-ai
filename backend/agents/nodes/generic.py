"""
Generic Worker èŠ‚ç‚¹ - é€šç”¨ä¸“å®¶æ‰§è¡Œ

[èŒè´£]
æ‰§è¡Œå•ä¸ªä¸“å®¶ä»»åŠ¡ï¼Œæ”¯æŒï¼š
- ä¸“å®¶é…ç½®åŠ¨æ€åŠ è½½ï¼ˆæ•°æ®åº“ + ç¼“å­˜ï¼‰
- å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰
- æ‰¹å¤„ç† Artifact äº¤ä»˜ï¼ˆå®Œæˆåå…¨é‡æ¨é€ï¼‰
- ä¸Šä¸‹æ–‡ç»„è£…ï¼ˆä¸Šæ¸¸ä¾èµ–ä»»åŠ¡è¾“å‡ºæ³¨å…¥ï¼‰

[æ‰§è¡Œæµç¨‹]
1. ä» state è·å–å½“å‰ä»»åŠ¡ï¼ˆcurrent_task_indexï¼‰
2. åŠ è½½ä¸“å®¶é…ç½®ï¼ˆsystem_prompt, model, temperatureï¼‰
3. ç»„è£…ä¸Šä¸‹æ–‡ï¼ˆç³»ç»Ÿæç¤º + ä¸Šæ¸¸ä»»åŠ¡è¾“å‡º + å½“å‰ä»»åŠ¡è¾“å…¥ï¼‰
4. è°ƒç”¨ LLMï¼ˆæ‰¹å¤„ç†æ¨¡å¼ï¼‰
5. å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœ‰ï¼‰
6. ç”Ÿæˆ Artifactï¼ˆä»£ç /æ–‡æ¡£/HTMLï¼‰- æ‰¹å¤„ç†äº¤ä»˜
7. å‘é€ task.completed äº‹ä»¶ï¼ˆåŒ…å«å®Œæ•´ Artifactï¼‰
8. æ›´æ–°ä»»åŠ¡çŠ¶æ€åˆ°æ•°æ®åº“
9. é€’å¢ current_task_indexï¼Œè¿”å›æ§åˆ¶ç»™ Dispatcher

[å·¥å…·è°ƒç”¨æµç¨‹]
é¦–æ¬¡è°ƒç”¨ -> LLM è¿”å› tool_calls -> ToolNode æ‰§è¡Œ -> 
å†æ¬¡è°ƒç”¨ -> LLM çœ‹åˆ° ToolMessage -> ç”Ÿæˆæœ€ç»ˆå›å¤

[æ‰¹å¤„ç†äº¤ä»˜]
æ‰€æœ‰ä¸“å®¶ç»Ÿä¸€ä½¿ç”¨ ainvoke ç­‰å¾…å®Œæ•´å“åº”ï¼š
- ç”Ÿæˆçš„ Artifact åœ¨ task.completed äº‹ä»¶ä¸­å…¨é‡æ¨é€
- å‰ç«¯åœ¨ä»»åŠ¡å®Œæˆæ—¶ä¸€æ¬¡æ€§æ¸²æŸ“å®Œæ•´å†…å®¹
- ç®€åŒ–æ¶æ„ï¼Œé¿å…æµå¼åŒæ­¥é—®é¢˜

[ä¾èµ–æ³¨å…¥]
- æ ¹æ® depends_on æŸ¥æ‰¾ä¸Šæ¸¸ä»»åŠ¡è¾“å‡º
- æ³¨å…¥åˆ°å½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡
- ç¼ºå¤±ä¾èµ–æ—¶å®¹é”™å¤„ç†ï¼ˆæç¤º LLM å°½åŠ›å®Œæˆï¼‰

[Artifact ç”Ÿæˆ]
- ä» LLM å“åº”æå–ä»£ç å—
- è¯†åˆ«è¯­è¨€ç±»å‹ï¼ˆè‡ªåŠ¨æ£€æµ‹æˆ–æŒ‡å®šï¼‰
- åˆ›å»º Artifact è®°å½•ï¼ˆæ•°æ®åº“ + äº‹ä»¶æ¨é€ï¼‰
- æ”¯æŒå¤šä¸ª Artifactï¼ˆä¸€ä¸ªä»»åŠ¡å¯äº§å‡ºå¤šä¸ªæ–‡ä»¶ï¼‰

[é”™è¯¯å¤„ç†]
- ä¸“å®¶é…ç½®ä¸å­˜åœ¨ï¼šè¿”å› failed çŠ¶æ€
- LLM è°ƒç”¨å¼‚å¸¸ï¼šè®°å½•é”™è¯¯ï¼Œæ ‡è®°å¤±è´¥
- å·¥å…·æ‰§è¡Œå¤±è´¥ï¼šè¿”å›é”™è¯¯ä¿¡æ¯ï¼ŒLLM ç”Ÿæˆå®¹é”™å›å¤

[çŠ¶æ€æ›´æ–°]
- task_list[current_index]: æ›´æ–° output_result, status, completed_at
- expert_results: è¿½åŠ æ‰§è¡Œç»“æœï¼ˆä¾›ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ï¼‰
- event_queue: æ¨é€ task.started/completed äº‹ä»¶
"""
import os
import re
import asyncio  # ğŸ”¥ ç”¨äºå¼‚æ­¥ä¿å­˜ä¸“å®¶æ‰§è¡Œç»“æœ
import json
from datetime import datetime
from typing import Dict, Any, Optional, List, Union
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage, BaseMessage
from langchain_core.runnables import RunnableConfig
from tools import ALL_TOOLS as BASE_TOOLS  # ğŸ”¥ MCP: å¯¼å…¥åŸºç¡€å·¥å…·é›†

from agents.state import AgentState
from agents.services.expert_manager import get_expert_config_cached
from utils.llm_factory import get_effective_model, get_expert_llm
from providers_config import get_model_config, load_providers_config
from services.memory_manager import memory_manager  # ğŸ”¥ å¯¼å…¥è®°å¿†ç®¡ç†å™¨
from tools import ALL_TOOLS  # ğŸ”¥ å¯¼å…¥å·¥å…·é›†
from utils.prompt_utils import enhance_system_prompt_with_tools  # v3.6: æå–åˆ°å·¥å…·å‡½æ•°


def normalize_message_content(content: Union[str, List, Any]) -> str:
    """
    å°†æ¶ˆæ¯å†…å®¹è§„èŒƒåŒ–ä¸ºå­—ç¬¦ä¸²æ ¼å¼ã€‚
    
    æŸäº›æ¨¡å‹ï¼ˆå¦‚ DeepSeekï¼‰è¦æ±‚ message content å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œ
    ä½† ToolMessage çš„ content å¯èƒ½æ˜¯ list[str | dict]ï¼Œéœ€è¦è½¬æ¢ã€‚
    
    Args:
        content: åŸå§‹å†…å®¹ï¼Œå¯èƒ½æ˜¯ str, list, dict ç­‰
        
    Returns:
        str: è§„èŒƒåŒ–åçš„å­—ç¬¦ä¸²å†…å®¹
    """
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        # å°†åˆ—è¡¨è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
        return json.dumps(content, ensure_ascii=False)
    if isinstance(content, dict):
        return json.dumps(content, ensure_ascii=False)
    # å…¶ä»–ç±»å‹è½¬ä¸ºå­—ç¬¦ä¸²
    return str(content)


def normalize_messages_for_llm(messages: List[BaseMessage], content_mode: str = "auto") -> List[BaseMessage]:
    """
    è§„èŒƒåŒ–æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¹æ®æ¨¡å‹è¦æ±‚å¤„ç† content æ ¼å¼ã€‚
    
    ä¸åŒæ¨¡å‹å¯¹ message content çš„è¦æ±‚ä¸åŒï¼š
    - string æ¨¡å¼ï¼šcontent å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼ˆDeepSeek, MiniMax, Moonshot ç­‰å›½äº§æ¨¡å‹ï¼‰
    - auto æ¨¡å¼ï¼šåŸç”Ÿæ”¯æŒ list[str | dict]ï¼ˆOpenAI, Anthropic, Gemini ç­‰ï¼‰
    
    Args:
        messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
        content_mode: å†…å®¹æ¨¡å¼ï¼Œ"string" æˆ– "auto"
        
    Returns:
        List[BaseMessage]: è§„èŒƒåŒ–åçš„æ¶ˆæ¯åˆ—è¡¨
    """
    # auto æ¨¡å¼ä¸‹ä¸éœ€è¦è½¬æ¢ï¼Œç›´æ¥è¿”å›åŸæ¶ˆæ¯
    if content_mode == "auto":
        return messages
    
    # string æ¨¡å¼ä¸‹éœ€è¦è½¬æ¢ ToolMessage content
    normalized = []
    for msg in messages:
        if isinstance(msg, ToolMessage):
            # ToolMessage çš„ content å¯èƒ½æ˜¯ list/dictï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            normalized_content = normalize_message_content(msg.content)
            if normalized_content != msg.content:
                # åˆ›å»ºæ–°çš„ ToolMessageï¼Œä¿ç•™å…¶ä»–å­—æ®µ
                normalized.append(ToolMessage(
                    content=normalized_content,
                    tool_call_id=msg.tool_call_id,
                    name=msg.name,
                    additional_kwargs=msg.additional_kwargs,
                    response_metadata=msg.response_metadata,
                ))
            else:
                normalized.append(msg)
        else:
            normalized.append(msg)
    return normalized


async def generic_worker_node(state: Dict[str, Any], config: RunnableConfig = None, llm=None) -> Dict[str, Any]:
    """
    é€šç”¨ä¸“å®¶æ‰§è¡ŒèŠ‚ç‚¹

    æ ¹æ® state["current_task"]["expert_type"] ä»æ•°æ®åº“åŠ è½½ä¸“å®¶é…ç½®å¹¶æ‰§è¡Œã€‚
    ç”¨äºå¤„ç†åŠ¨æ€åˆ›å»ºçš„è‡ªå®šä¹‰ä¸“å®¶ã€‚

    æ”¯æŒå·¥å…·è°ƒç”¨æµç¨‹ï¼š
    1. é¦–æ¬¡è°ƒç”¨ï¼šLLM å¯èƒ½è¿”å› tool_calls
    2. å·¥å…·æ‰§è¡Œåï¼šLLM çœ‹åˆ° ToolMessageï¼Œç”Ÿæˆæœ€ç»ˆå›å¤

    ğŸ”¥ v4.0 é‡æ„ï¼šæ‰¹å¤„ç†æ¨¡å¼
    - æ‰€æœ‰ä¸“å®¶ç»Ÿä¸€ä½¿ç”¨ ainvoke ç­‰å¾…å®Œæ•´å“åº”
    - Artifact åœ¨ task.completed äº‹ä»¶ä¸­å…¨é‡æ¨é€
    - ç®€åŒ–æ¶æ„ï¼Œé¿å…æµå¼åŒæ­¥é—®é¢˜

    Args:
        state: AgentStateï¼ŒåŒ…å« task_list, current_task_index ç­‰
        llm: å¯é€‰çš„ LLM å®ä¾‹ï¼Œå¦‚æœä¸æä¾›åˆ™æ ¹æ®ä¸“å®¶é…ç½®åˆ›å»º

    Returns:
        Dict: æ‰§è¡Œç»“æœï¼ŒåŒ…å« output_result, status, artifact ç­‰
    """
    from langchain_core.messages import ToolMessage

    # è·å–å½“å‰ä»»åŠ¡
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    existing_messages = state.get("messages", [])
    
    if current_index >= len(task_list):
        return {
            "output_result": "æ²¡æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡",
            "status": "failed",
            "error": "Task index out of range",
            "started_at": datetime.now().isoformat(),
            "completed_at": datetime.now().isoformat()
        }
    
    current_task = task_list[current_index]
    expert_type = current_task.get("expert_type", "")
    description = current_task.get("description", "")
    input_data = current_task.get("input_data", {})
    
    if not expert_type:
        return {
            "output_result": "ä»»åŠ¡ç¼ºå°‘ expert_type å­—æ®µ",
            "status": "failed",
            "error": "Missing expert_type in task",
            "started_at": datetime.now().isoformat(),
            "completed_at": datetime.now().isoformat()
        }
    
    # ä»ç¼“å­˜åŠ è½½ä¸“å®¶é…ç½®
    expert_config = get_expert_config_cached(expert_type)
    
    # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå¯èƒ½æ˜¯è‡ªå®šä¹‰ä¸“å®¶ï¼Œå°è¯•ç›´æ¥æŸ¥æ•°æ®åº“
    if not expert_config:
        print(f"[GenericWorker] ç¼“å­˜ä¸­æœªæ‰¾åˆ° '{expert_type}'ï¼Œå°è¯•ä»æ•°æ®åº“åŠ è½½...")
        from database import engine
        from sqlmodel import Session
        from agents.services.expert_manager import get_expert_config
        
        with Session(engine) as session:
            expert_config = get_expert_config(expert_type, session)
            if expert_config:
                print(f"[GenericWorker] ä»æ•°æ®åº“åŠ è½½ '{expert_type}' æˆåŠŸ")
    
    if not expert_config:
        return {
            "output_result": f"ä¸“å®¶ '{expert_type}' æœªæ‰¾åˆ°",
            "status": "failed",
            "error": f"Expert '{expert_type}' not found in database",
            "started_at": datetime.now().isoformat(),
            "completed_at": datetime.now().isoformat()
        }
    
    started_at = datetime.now()

    # âœ… å‘é€ task.started äº‹ä»¶ï¼ˆä¸“å®¶å¼€å§‹æ‰§è¡Œï¼‰
    from utils.event_generator import event_task_started, sse_event_to_string
    task_id = current_task.get("id", str(current_index))
    started_event = event_task_started(
        task_id=task_id,
        expert_type=expert_type,
        description=description
    )
    # å°† started äº‹ä»¶æ”¾å…¥ state çš„ event_queueï¼Œè®© dispatcher æˆ–å…¶ä»–èŠ‚ç‚¹å¤„ç†
    initial_event_queue = state.get("event_queue", [])
    initial_event_queue.append({"type": "sse", "event": sse_event_to_string(started_event)})
    print(f"[GenericWorker] å·²ç”Ÿæˆ task.started äº‹ä»¶: {expert_type}")

    try:
        # è·å–ä¸“å®¶é…ç½®å‚æ•°
        system_prompt = expert_config["system_prompt"]
        expert_name = expert_config.get("name", expert_type)
        
        # åº”ç”¨æ¨¡å‹å…œåº•æœºåˆ¶
        configured_model = expert_config.get("model")
        effective_model = get_effective_model(configured_model)
        
        # è·å–æ¨¡å‹é…ç½®ä»¥ç¡®å®šå®é™…çš„ API æ¨¡å‹åç§°å’Œæ¸©åº¦
        model_config = get_model_config(effective_model)
        if model_config:
            actual_model = model_config.get("model", effective_model)
            temperature = model_config.get("temperature", expert_config.get("temperature", 0.7))
            provider = model_config.get("provider")
        else:
            actual_model = effective_model
            temperature = expert_config.get("temperature", 0.7)
            provider = None
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ è·å– provider çš„ content_mode é…ç½®
        content_mode = "string"  # é»˜è®¤ä½¿ç”¨ string æ¨¡å¼ï¼ˆå®‰å…¨ï¼‰
        if provider:
            providers_config = load_providers_config()
            provider_config = providers_config.get("providers", {}).get(provider, {})
            content_mode = provider_config.get("content_mode", "string")
        
        print(f"[GenericWorker] Running '{expert_type}' ({expert_name}) with model={actual_model}, temp={temperature}, content_mode={content_mode}")
        
        # å¦‚æœæ²¡æœ‰æä¾› LLM å®ä¾‹ï¼Œæ ¹æ®é…ç½®åˆ›å»º
        if llm is None:
            # æ ¹æ®æ¨¡å‹é…ç½®è·å– provider
            if model_config:
                provider = model_config.get("provider")
                llm = get_expert_llm(provider=provider, model=actual_model, temperature=temperature)
            else:
                llm = get_expert_llm(model=actual_model, temperature=temperature)

        # ç»‘å®šæ¨¡å‹å’Œæ¸©åº¦å‚æ•°
        llm_with_config = llm.bind(
            model=actual_model,
            temperature=temperature
        )

        # ğŸ”¥ğŸ”¥ğŸ”¥ GenericWorker 2.0: å ä½ç¬¦å¡«å…… + System Prompt å¢å¼º
        # å¡«å…… {input} å ä½ç¬¦ï¼ˆä»»åŠ¡æè¿°ï¼‰
        if "{input}" in system_prompt:
            system_prompt = system_prompt.replace("{input}", description)
            print(f"[GenericWorker] å·²æ³¨å…¥å ä½ç¬¦: {{input}} = {description[:50]}...")
        
        # å¢å¼º System Prompt (æ³¨å…¥æ—¶é—´ + å·¥å…·æŒ‡ä»¤)
        enhanced_system_prompt = enhance_system_prompt_with_tools(system_prompt)

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ„å»ºæ¶ˆæ¯åˆ—è¡¨
        # å¦‚æœæœ‰ç°æœ‰çš„ messagesï¼ˆåŒ…å« ToolMessageï¼‰ï¼Œåˆ™ä½¿ç”¨å®ƒä»¬
        # å¦åˆ™åˆ›å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
        has_tool_message = False
        if existing_messages:
            # å·¥å…·æ‰§è¡Œåçš„æƒ…å†µï¼šmessages åŒ…å« AIMessage(tool_calls) + ToolMessage
            # æˆ‘ä»¬éœ€è¦ä¿ç•™è¿™äº›ä¸Šä¸‹æ–‡ï¼Œè®© LLM çœ‹åˆ°å·¥å…·ç»“æœ
            # æ£€æŸ¥æœ€åä¸€æ¡æ˜¯å¦æ˜¯ ToolMessage
            if existing_messages and isinstance(existing_messages[-1], ToolMessage):
                has_tool_message = True
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šè§„èŒƒåŒ– ToolMessage content
            # æ ¹æ® provider çš„ content_mode å†³å®šæ˜¯å¦è½¬æ¢ï¼ˆstring æ¨¡å¼éœ€è½¬æ¢ï¼Œauto æ¨¡å¼ä¿æŒåŸæ ·ï¼‰
            normalized_existing = normalize_messages_for_llm(existing_messages, content_mode)
            
            messages_for_llm = [
                SystemMessage(content=enhanced_system_prompt),
                *normalized_existing  # åŒ…å« AIMessage(tool_calls) å’Œ ToolMessage
            ]
        else:
            # é¦–æ¬¡è°ƒç”¨ï¼šåˆ›å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
            # ğŸ”¥ğŸ”¥ğŸ”¥ æ™ºèƒ½ä¸Šä¸‹æ–‡ç»„è£…ï¼šå¤„ç†ä¾èµ–ç¼ºå¤±çš„æƒ…å†µ
            expert_results = state.get("expert_results", [])
            depends_on = current_task.get("depends_on", [])
            
            # æ„å»ºä¸Šä¸‹æ–‡æç¤º
            context_parts = []
            missing_deps = []
            
            if depends_on:
                # æŸ¥æ‰¾ä¾èµ–ä»»åŠ¡çš„è¾“å‡º
                # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒä¿é™©åŒ¹é…ï¼Œæ”¯æŒ task_id å’Œ db_uuid
                for dep_id in depends_on:
                    dep_result = next(
                        (r for r in expert_results 
                         if r.get("task_id") == dep_id or r.get("db_uuid") == dep_id), 
                        None
                    )
                    if dep_result and dep_result.get("output"):
                        context_parts.append(f"ã€ä¸Šæ¸¸ä»»åŠ¡ {dep_id} çš„è¾“å‡ºã€‘:\n{dep_result['output'][:2000]}...")
                        print(f"[GenericWorker] âœ… æ‰¾åˆ°ä¾èµ– {dep_id}: {len(dep_result['output'])} å­—ç¬¦")
                    else:
                        missing_deps.append(dep_id)
                        print(f"[GenericWorker] âš ï¸ æœªæ‰¾åˆ°ä¾èµ– {dep_id}, å¯ç”¨ç»“æœ: {[r.get('task_id') for r in expert_results]}")
            
            # ç»„è£…ä»»åŠ¡æç¤º
            task_prompt = f"ä»»åŠ¡æè¿°: {description}\n\n"
            
            if context_parts:
                task_prompt += "å‚è€ƒä¸Šä¸‹æ–‡:\n" + "\n---\n".join(context_parts) + "\n\n"
            
            # ğŸ”¥ å…³é”®ï¼šæ³¨å…¥å®¹é”™æŒ‡ä»¤
            if missing_deps:
                task_prompt += f"""âš ï¸ æ³¨æ„ï¼šéƒ¨åˆ†ä¸Šæ¸¸ä¾èµ–ä»»åŠ¡ ({', '.join(missing_deps)}) å·²è¢«ç§»é™¤æˆ–æœªæ‰§è¡Œã€‚
å¦‚æœä»»åŠ¡æè¿°ä¸­å¼•ç”¨äº†è¿™äº›ç¼ºå¤±éƒ¨åˆ†ï¼ˆå¦‚ä»£ç ã€æ•°æ®ç­‰ï¼‰ï¼Œè¯·å¿½ç•¥è¯¥å¼•ç”¨ï¼Œ
å¹¶åŸºäºå½“å‰ç°æœ‰çš„ä¿¡æ¯ï¼Œå°½æœ€å¤§åŠªåŠ›å®Œæˆä»»åŠ¡ã€‚ä¸è¦åœ¨è¾“å‡ºä¸­æŠ±æ€¨ç¼ºå°‘ä¿¡æ¯ã€‚\n\n"""
            
            task_prompt += f"è¾“å…¥å‚æ•°:\n{_format_input_data(input_data)}"
            
            messages_for_llm = [
                SystemMessage(content=enhanced_system_prompt),
                HumanMessage(content=task_prompt)
            ]

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ¹æ®æ˜¯å¦æœ‰ ToolMessage å†³å®šæ˜¯å¦ç»‘å®šå·¥å…·
        # å¦‚æœå·²ç»æœ‰ ToolMessageï¼ˆå·¥å…·æ‰§è¡Œå®Œæˆï¼‰ï¼Œåˆ™ä¸ç»‘å®šå·¥å…·ï¼Œé˜²æ­¢æ— é™å¾ªç¯
        if has_tool_message:
            llm_to_use = llm_with_config
        else:
            # ğŸ”¥ æ–°å¢ï¼šä¸ºæ‰€æœ‰ä¸“å®¶ç»‘å®šå·¥å…·ï¼ˆè”ç½‘æœç´¢ã€æ—¶é—´ã€è®¡ç®—å™¨ï¼‰
            # å¦‚æœ LLM æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œåˆ™ç»‘å®šå·¥å…·é›†
            # ğŸ”¥ ç¯å¢ƒå˜é‡æ§åˆ¶ï¼šENABLE_TOOL_CALLING=false å¯ç¦ç”¨å·¥å…·è°ƒç”¨ï¼ˆå¹³æ»‘å‡çº§å…¼å®¹ï¼‰
            enable_tools = os.getenv("ENABLE_TOOL_CALLING", "true").lower() == "true"
            if enable_tools:
                try:
                    # ğŸ”¥ MCP: ä» config è·å–åŠ¨æ€æ³¨å…¥çš„å·¥å…·
                    mcp_tools = []
                    if config and hasattr(config, 'get'):
                        mcp_tools = config.get('configurable', {}).get('mcp_tools', [])
                    
                    # ğŸ”¥ MCP: åˆå¹¶åŸºç¡€å·¥å…·å’ŒåŠ¨æ€ MCP å·¥å…·
                    runtime_tools = list(BASE_TOOLS) + list(mcp_tools)
                    
                    llm_to_use = llm_with_config.bind_tools(runtime_tools)
                    print(f"[GenericWorker] ğŸ”§ å·¥å…·å·²ç»‘å®š: {len(runtime_tools)} ä¸ªå·¥å…· (åŸºç¡€: {len(BASE_TOOLS)}, MCP: {len(mcp_tools)})")
                except Exception as e:
                    print(f"[GenericWorker] âš ï¸ å·¥å…·ç»‘å®šå¤±è´¥ï¼ˆæ¨¡å‹å¯èƒ½ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼‰: {e}")
                    llm_to_use = llm_with_config
            else:
                print(f"[GenericWorker] â­ï¸ å·¥å…·è°ƒç”¨å·²ç¦ç”¨ï¼ˆENABLE_TOOL_CALLING=falseï¼‰")
                llm_to_use = llm_with_config

        # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå½“ has_tool_message=True æ—¶ï¼Œåœ¨æ¶ˆæ¯æœ«å°¾æ·»åŠ æ˜ç¡®çš„"ä»»åŠ¡å®Œæˆ"æç¤º
        if has_tool_message:
            # åœ¨æ¶ˆæ¯åˆ—è¡¨æœ«å°¾æ·»åŠ ä¸€ä¸ª HumanMessageï¼Œæ˜ç¡®å‘Šè¯‰ LLM ä»»åŠ¡å®Œæˆ
            messages_for_llm.append(HumanMessage(content="[ç³»ç»Ÿæç¤ºï¼šä»¥ä¸Šæ˜¯å·¥å…·æ‰§è¡Œç»“æœï¼Œè¯·åŸºäºæ­¤ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤ï¼Œä»»åŠ¡å·²å®Œæˆï¼Œä¸è¦å†è°ƒç”¨ä»»ä½•å·¥å…·]"))

        # ğŸ”¥ğŸ”¥ğŸ”¥ v4.0 é‡æ„ï¼šç»Ÿä¸€ä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼
        # æ‰€æœ‰ä¸“å®¶ç»Ÿä¸€ä½¿ç”¨ ainvoke ç­‰å¾…å®Œæ•´å“åº”
        # Artifact åœ¨ task.completed äº‹ä»¶ä¸­å…¨é‡æ¨é€
        response = await llm_to_use.ainvoke(
            messages_for_llm,
            config=RunnableConfig(
                tags=["expert", expert_type, "generic_worker"],
                metadata={"node_type": "expert", "expert_type": expert_type}
            )
        )
        
        # ç”Ÿæˆ artifact_id
        import uuid
        artifact_id = str(uuid.uuid4())

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
        has_tool_calls = hasattr(response, "tool_calls") and response.tool_calls

        if has_tool_calls:
            print(f"[GenericWorker] ğŸ”§ LLM è¿”å›äº†å·¥å…·è°ƒç”¨ï¼æ•°é‡: {len(response.tool_calls)}")
            for tool_call in response.tool_calls:
                print(f"[GenericWorker]   - å·¥å…·: {tool_call.get('name', 'unknown')}")
            # ğŸ”¥ğŸ”¥ å…³é”®ï¼šè¿”å› messages è®© ToolNode å¤„ç†å·¥å…·è°ƒç”¨
            # æ­¤æ—¶ä¸ç”Ÿæˆ task.completed äº‹ä»¶ï¼Œå› ä¸ºä»»åŠ¡è¿˜æ²¡å®Œæˆ
            return {
                "messages": [response],  # åŒ…å« tool_calls çš„ AIMessage
                "task_list": task_list,
                "current_task_index": current_index,  # ä¸å¢åŠ  indexï¼Œç­‰å·¥å…·æ‰§è¡Œå®Œå†è¯´
                "event_queue": initial_event_queue,  # åªè¿”å› started äº‹ä»¶
                "__expert_info": {
                    "expert_type": expert_type,
                    "expert_name": expert_name,
                    "task_id": task_id,
                    "status": "waiting_for_tool",
                    "tool_calls": response.tool_calls
                }
            }

        # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ­£å¸¸å®Œæˆä»»åŠ¡
        print(f"[GenericWorker] â„¹ï¸ LLM è¿”å›äº†æ™®é€šæ–‡æœ¬å“åº”ï¼Œæœªè°ƒç”¨å·¥å…·")

        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)

        print(f"[GenericWorker] '{expert_type}' completed (è€—æ—¶: {duration_ms/1000:.2f}s)")

        # -------------------------------------------------------------
        # ğŸ”¥ æ–°å¢é€»è¾‘ï¼šå¦‚æœæ˜¯è®°å¿†ä¸“å®¶ï¼Œæ‰§è¡Œ"å†™å…¥æ•°æ®åº“"æ“ä½œ
        # -------------------------------------------------------------
        if expert_type == "memorize_expert":
            memory_content = response.content.strip()
            # ä» state è·å– user_idï¼Œé»˜è®¤ä½¿ç”¨ default_user
            user_id = state.get("user_id", "default_user")
            
            if memory_content:
                print(f"[GenericWorker] æ­£åœ¨ä¿å­˜è®°å¿†: {memory_content}")
                try:
                    # å¼‚æ­¥è°ƒç”¨ memory_manager ä¿å­˜ (å†…éƒ¨ä½¿ç”¨äº† to_thread)
                    await memory_manager.add_memory(
                        user_id=user_id,
                        content=memory_content,
                        source="conversation",
                        memory_type="fact"
                    )
                    print(f"[GenericWorker] è®°å¿†ä¿å­˜æˆåŠŸ!")
                    # ä¿®æ”¹è¿”å›ç»™ç”¨æˆ·çš„ outputï¼Œè®©åé¦ˆæ›´è‡ªç„¶
                    response_content_original = response.content
                    response.content = f"å·²ä¸ºæ‚¨è®°å½•ï¼š{response_content_original}"
                except Exception as mem_err:
                    print(f"[GenericWorker] è®°å¿†ä¿å­˜å¤±è´¥: {mem_err}")
                    response.content = f"è®°å½•æ—¶é‡åˆ°é—®é¢˜ï¼Œä½†æˆ‘ä¼šè®°ä½ï¼š{memory_content}"
        # -------------------------------------------------------------

        # ğŸ”¥ æ£€æµ‹ artifact ç±»å‹
        artifact_type = _detect_artifact_type(response.content, expert_type)

        # âœ… v3.2 ä¿®å¤ï¼šå¢åŠ  current_task_index ä»¥æ”¯æŒå¾ªç¯
        # Generic Worker æ‰§è¡Œå®Œä»»åŠ¡åï¼Œéœ€è¦é€’å¢ index æ‰èƒ½æ‰§è¡Œä¸‹ä¸€ä¸ªä»»åŠ¡
        next_index = current_index + 1

        # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šåˆ›å»º task_list å‰¯æœ¬è§¦å‘ LangGraph çŠ¶æ€æ›´æ–°
        # ç›´æ¥ä¿®æ”¹åˆ—è¡¨å…ƒç´ ä¸ä¼šæ”¹å˜å¼•ç”¨ï¼ŒLangGraph æ£€æµ‹ä¸åˆ°å˜åŒ–
        import copy
        task_list = copy.deepcopy(task_list)
        
        # âœ… æ›´æ–°ä»»åŠ¡åˆ—è¡¨ä¸­çš„ä»»åŠ¡çŠ¶æ€
        task_list[current_index]["output_result"] = {"content": response.content}
        task_list[current_index]["status"] = "completed"
        task_list[current_index]["completed_at"] = completed_at.isoformat()

        # âœ… æ·»åŠ åˆ° expert_resultsï¼ˆç”¨äºåç»­ä»»åŠ¡ä¾èµ–å’Œæœ€ç»ˆèšåˆï¼‰
        # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ task_id (Commander ID, å¦‚ "task_0") è€Œä¸æ˜¯ id (UUID)
        # ä¸‹æ¸¸ä»»åŠ¡é€šè¿‡ depends_on: ["task_0"] æŸ¥æ‰¾ï¼Œå¿…é¡»ç”¨ç›¸åŒæ ¼å¼æ‰èƒ½åŒ¹é…
        semantic_id = current_task.get("task_id")  # Commander ID (å¦‚ "task_0")
        db_uuid = current_task.get("id")  # æ•°æ®åº“ UUID (å¦‚ "550e8400...")
        record_id = semantic_id if semantic_id else db_uuid  # ä¼˜å…ˆä½¿ç”¨ semantic_id
        
        expert_result = {
            "task_id": record_id,  # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨ Commander ID è®©ä¸‹æ¸¸èƒ½åŒ¹é…åˆ°
            "db_uuid": db_uuid,    # ä¿ç•™ UUID æ–¹ä¾¿è°ƒè¯•
            "expert_type": expert_type,
            "description": description,
            "output": response.content,
            "status": "completed",
            "duration_ms": duration_ms
        }
        
        print(f"[GenericWorker] ä¿å­˜ä¸“å®¶ç»“æœ: task_id={record_id}, db_uuid={db_uuid}, expert={expert_type}")

        # è·å–ç°æœ‰çš„ expert_results å¹¶è¿½åŠ æ–°ç»“æœ
        expert_results = state.get("expert_results", [])
        expert_results = expert_results + [expert_result]

        # âœ… æ„å»º artifact å¯¹è±¡ï¼ˆç¬¦åˆ ArtifactCreate æ¨¡å‹ï¼‰
        artifact = {
            "type": artifact_type,
            "title": f"{expert_name}ç»“æœ",
            "content": response.content,
            "language": None,  # å¯é€‰å­—æ®µï¼ŒPydantic æ¨¡å‹éœ€è¦
            "sort_order": 0,   # é»˜è®¤æ’åº
            "artifact_id": artifact_id
        }

        # âœ… å¼‚æ­¥ä¿å­˜ä¸“å®¶æ‰§è¡Œç»“æœåˆ°æ•°æ®åº“ï¼ˆP0 ä¼˜åŒ–ï¼šä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        # ğŸ”¥ ä¿®å¤ï¼šä¸ä¼ é€’ db_sessionï¼Œåœ¨ async_save_expert_result ä¸­åˆ›å»ºç‹¬ç«‹çš„ Session
        if task_id:
            try:
                from utils.async_task_queue import async_save_expert_result
                # ä½¿ç”¨åå°çº¿ç¨‹å¼‚æ­¥ä¿å­˜ï¼Œä¸é˜»å¡ LLM å“åº”è¿”å›
                asyncio.create_task(async_save_expert_result(
                    task_id=task_id,
                    expert_type=expert_type,
                    output_result=response.content,
                    artifact_data=artifact,
                    duration_ms=duration_ms
                ))
                print(f"[GenericWorker] âœ… ä¸“å®¶æ‰§è¡Œç»“æœå·²æäº¤åå°çº¿ç¨‹æ± ä¿å­˜: {expert_type}")
            except Exception as save_err:
                print(f"[GenericWorker] âš ï¸ åå°ä¿å­˜æäº¤å¤±è´¥: {save_err}")
        else:
            print(f"[GenericWorker] âš ï¸ è·³è¿‡ä¿å­˜: task_id={task_id}")

        # âœ… ç”Ÿæˆäº‹ä»¶é˜Ÿåˆ—ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºä¸“å®¶å’Œ artifactï¼‰
        from utils.event_generator import (
            event_task_completed, event_artifact_generated, sse_event_to_string
        )

        event_queue = []

        # ğŸ”¥ v4.0 é‡æ„ï¼šç»Ÿä¸€å‘é€ artifact.generated äº‹ä»¶ï¼ˆæ‰¹å¤„ç†æ¨¡å¼ï¼‰
        # æ‰€æœ‰ä¸“å®¶å®Œæˆåå‘é€å®Œæ•´çš„ artifact å†…å®¹
        artifact_event = event_artifact_generated(
            task_id=task_id,
            expert_type=expert_type,
            artifact_id=artifact_id,
            artifact_type=artifact_type,
            content=response.content,
            title=f"{expert_name}ç»“æœ"
        )
        event_queue.append({"type": "sse", "event": sse_event_to_string(artifact_event)})
        print(f"[GenericWorker] å·²ç”Ÿæˆ artifact.generated äº‹ä»¶: {artifact_type}")

        # 1. å‘é€ task.completed äº‹ä»¶ï¼ˆä¸“å®¶æ‰§è¡Œå®Œæˆï¼‰
        task_completed_event = event_task_completed(
            task_id=task_id,
            expert_type=expert_type,
            description=description,
            output=response.content[:500] + "..." if len(response.content) > 500 else response.content,
            duration_ms=duration_ms,
            artifact_count=1
        )
        event_queue.append({"type": "sse", "event": sse_event_to_string(task_completed_event)})
        print(f"[GenericWorker] å·²ç”Ÿæˆ task.completed äº‹ä»¶: {expert_type}")

        # âœ… åˆå¹¶ started äº‹ä»¶å’Œ completed äº‹ä»¶
        full_event_queue = initial_event_queue + event_queue

        return {
            "messages": [response],  # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šå¿…é¡»æŠŠ LLM çš„æœ€ç»ˆå›å¤æ›´æ–°åˆ°å›¾çŠ¶æ€çš„æ¶ˆæ¯å†å²ä¸­ï¼ğŸ”¥ğŸ”¥ğŸ”¥
            "task_list": task_list,
            "expert_results": expert_results,
            "current_task_index": next_index,  # âœ… å¢åŠ  index
            "output_result": response.content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms,
            "artifact": artifact,
            "event_queue": full_event_queue,  # âœ… æ·»åŠ å®Œæ•´äº‹ä»¶é˜Ÿåˆ—ï¼ˆåŒ…å« started å’Œ completedï¼‰
            # âœ… æ·»åŠ  __expert_info ç”¨äº chat.py è¯†åˆ«å’Œæ”¶é›† artifacts
            "__expert_info": {
                "expert_type": expert_type,
                "expert_name": expert_name,
                "task_id": task_id,
                "status": "completed",
                "artifact_id": artifact_id  # ğŸ”¥ åŒ…å« artifact_id
            }
        }
        
    except Exception as e:
        print(f"[GenericWorker] '{expert_type}' failed: {e}")

        # âœ… å¤±è´¥æ—¶ä¹Ÿè¦å¢åŠ  indexï¼Œå¦åˆ™ä¼šå¡æ­»å¾ªç¯
        next_index = current_index + 1

        # ğŸ”¥ åˆ›å»ºå‰¯æœ¬è§¦å‘çŠ¶æ€æ›´æ–°
        import copy
        task_list = copy.deepcopy(task_list)
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        task_list[current_index]["status"] = "failed"

        # è·å–ç°æœ‰çš„ expert_results å¹¶æ·»åŠ å¤±è´¥è®°å½•
        expert_results = state.get("expert_results", [])
        # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ task_id (Commander ID) è€Œä¸æ˜¯ id (UUID)
        semantic_id = current_task.get("task_id")
        db_uuid = current_task.get("id")
        task_id = semantic_id if semantic_id else db_uuid
        expert_result = {
            "task_id": task_id,  # ğŸ”¥ ä½¿ç”¨ Commander ID
            "db_uuid": db_uuid,  # ä¿ç•™ UUID
            "expert_type": expert_type,
            "description": description,
            "output": f"ä¸“å®¶æ‰§è¡Œå¤±è´¥: {str(e)}",
            "status": "failed",
            "error": str(e),
            "duration_ms": 0
        }
        expert_results = expert_results + [expert_result]

        # âœ… ç”Ÿæˆ task.failed äº‹ä»¶
        from utils.event_generator import event_task_failed, sse_event_to_string

        event_queue = []
        failed_event = event_task_failed(
            task_id=task_id,
            expert_type=expert_type,
            description=description,
            error=str(e)
        )
        event_queue.append({"type": "sse", "event": sse_event_to_string(failed_event)})
        print(f"[GenericWorker] å·²ç”Ÿæˆ task.failed äº‹ä»¶: {expert_type}")

        # âœ… åˆå¹¶ started äº‹ä»¶å’Œ failed äº‹ä»¶
        full_event_queue = initial_event_queue + event_queue

        return {
            "task_list": task_list,
            "expert_results": expert_results,
            "current_task_index": next_index,  # âœ… å³ä½¿å¤±è´¥ä¹Ÿå¢åŠ  index
            "output_result": f"ä¸“å®¶æ‰§è¡Œå¤±è´¥: {str(e)}",
            "status": "failed",
            "error": str(e),
            "started_at": started_at.isoformat(),
            "completed_at": datetime.now().isoformat(),
            "event_queue": full_event_queue,  # âœ… æ·»åŠ å®Œæ•´äº‹ä»¶é˜Ÿåˆ—ï¼ˆåŒ…å« started å’Œ failedï¼‰
            # âœ… æ·»åŠ  __expert_info ç”¨äºæ ‡è¯†å¤±è´¥çš„ä¸“å®¶
            "__expert_info": {
                "expert_type": expert_type,
                "expert_name": expert_config.get("name", expert_type) if expert_config else expert_type,
                "task_id": task_id,
                "status": "failed",
                "error": str(e)
            }
        }


def _format_input_data(data: Dict) -> str:
    """æ ¼å¼åŒ–è¾“å…¥æ•°æ®ä¸ºæ–‡æœ¬"""
    if not data:
        return "ï¼ˆæ— é¢å¤–å‚æ•°ï¼‰"
    
    lines = []
    for key, value in data.items():
        if isinstance(value, (list, dict)):
            lines.append(f"- {key}: {value}")
        else:
            lines.append(f"- {key}: {value}")
    
    return "\n".join(lines)


def _detect_artifact_type(content: str, expert_key: str) -> str:
    """
    æ£€æµ‹ artifact ç±»å‹
    
    ç®€åŒ–ç‰ˆï¼Œé»˜è®¤è¿”å› "text"ï¼Œä½†ä¼šå°è¯•æ£€æµ‹ HTML å’Œ Markdown å†…å®¹ã€‚
    """
    content_lower = content.lower().strip()
    
    # 1. HTML æ£€æµ‹
    if (content_lower.startswith("<!doctype html") or
        content_lower.startswith("<html") or
        ("<html" in content_lower and "</html>" in content_lower)):
        return "html"
    
    # æ£€æµ‹ HTML ä»£ç å—
    html_code_block = re.search(r'```html\n([\s\S]*?)```', content, re.IGNORECASE)
    if html_code_block:
        return "html"
    
    # 2. Markdown æ£€æµ‹
    has_markdown = any(marker in content for marker in ['# ', '## ', '### ', '> ', '- ', '* '])
    has_code_block = '```' in content
    
    if has_markdown or has_code_block:
        return "markdown"
    
    # 3. é»˜è®¤è¿”å› text
    return "text"
