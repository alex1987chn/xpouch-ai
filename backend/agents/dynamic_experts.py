"""
åŠ¨æ€ä¸“å®¶æ‰§è¡Œç³»ç»Ÿï¼ˆä½¿ç”¨æ•°æ®åº“åŠ è½½çš„ Promptï¼‰

é‡æ„ä¸“å®¶æ‰§è¡Œé€»è¾‘ï¼š
1. ä½¿ç”¨ expert_loader ä»æ•°æ®åº“åŠ è½½é…ç½®
2. ä½¿ç”¨åŠ¨æ€æ¨¡å‹å’Œæ¸©åº¦å‚æ•°
3. æ”¯æŒç®¡ç†å‘˜å®æ—¶æ›´æ–° Prompt
"""
import os
from typing import Dict, Any, List, Optional
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from langchain_openai import ChatOpenAI
from datetime import datetime
from sqlmodel import Session, select

from agents.expert_loader import get_expert_config_cached, refresh_cache
from agents.experts import EXPERT_DESCRIPTIONS
from agents.model_fallback import get_effective_model, get_default_model
from models import SystemExpert


def create_expert_function(expert_key: str):
    """
    åˆ›å»ºä¸“å®¶å‡½æ•°å·¥å‚

    æ ¹æ®ä¸“å®¶ç±»å‹åŠ¨æ€ç”Ÿæˆæ‰§è¡Œå‡½æ•°

    Args:
        expert_key: ä¸“å®¶ç±»å‹æ ‡è¯†

    Returns:
        callable: ä¸“å®¶æ‰§è¡Œå‡½æ•°
    """
    async def expert_node(state: Dict[str, Any], llm: ChatOpenAI) -> Dict[str, Any]:
        """
        åŠ¨æ€ä¸“å®¶èŠ‚ç‚¹ï¼šä»æ•°æ®åº“åŠ è½½é…ç½®å¹¶æ‰§è¡Œ

        Args:
            state: å®Œæ•´çš„ AgentState
            llm: LLM å®ä¾‹

        Returns:
            Dict: æ›´æ–°åçš„ AgentState
        """
        # ä»æ•°æ®åº“/ç¼“å­˜åŠ è½½ä¸“å®¶é…ç½®
        expert_config = get_expert_config_cached(expert_key)

        if not expert_config:
            # é™çº§ï¼šä½¿ç”¨ç¡¬ç¼–ç  Prompt
            from agents.experts import EXPERT_PROMPTS
            # ğŸ‘ˆ ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç çš„ gpt-4o
            default_model = get_default_model()
            expert_config = {
                "expert_key": expert_key,
                "name": EXPERT_DESCRIPTIONS.get(expert_key, expert_key),
                "system_prompt": EXPERT_PROMPTS.get(expert_key, ""),
                "model": default_model,
                "temperature": 0.5
            }
            print(f"[DynamicExpert] Using fallback config for '{expert_key}': model={default_model}")

        system_prompt = expert_config["system_prompt"]
        # ğŸ‘ˆ åº”ç”¨æ¨¡å‹å…œåº•æœºåˆ¶
        model = get_effective_model(expert_config.get("model"))
        # ä»æ•°æ®åº“è¯»å–æ¸©åº¦ï¼Œä½†å¦‚æœæ¨¡å‹é…ç½®ä¸­æœ‰ç‰¹æ®Šçº¦æŸï¼Œåˆ™ä½¿ç”¨æ¨¡å‹é…ç½®ä¸­çš„å€¼
        db_temperature = expert_config["temperature"]

        # æ£€æŸ¥æ¨¡å‹é…ç½®ï¼Œè·å–å®é™…çš„ API æ¨¡å‹åç§°å’Œ temperature çº¦æŸ
        from providers_config import get_model_config
        model_config = get_model_config(model)
        if model_config:
            # ä½¿ç”¨ providers.yaml ä¸­å®šä¹‰çš„ model å­—æ®µï¼ˆå®é™… API æ¨¡å‹åç§°ï¼‰
            actual_model = model_config.get('model', model)
            temperature = model_config.get('temperature', db_temperature)
        else:
            # æœªæ‰¾åˆ°é…ç½®ï¼Œä½¿ç”¨åŸå§‹å€¼
            actual_model = model
            temperature = db_temperature

        print(f"[DynamicExpert] Running {expert_key} with model={actual_model}, temp={temperature} (db={db_temperature}, config={model})")

        # è·å–å½“å‰ä»»åŠ¡
        task_list = state.get("task_list", [])
        current_index = state.get("current_task_index", 0)

        if current_index >= len(task_list):
            return {"error": "æ²¡æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡"}

        current_task = task_list[current_index]
        description = current_task.get("description", "")
        input_data = current_task.get("input_data", {})

        # v3.1: æå–ä¾èµ–ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
        dependency_context = input_data.get("__dependency_context", "")
        # ä» input_data ä¸­ç§»é™¤å†…éƒ¨å­—æ®µï¼Œé¿å…æš´éœ²ç»™ LLM
        clean_input_data = {k: v for k, v in input_data.items() if not k.startswith("__")}

        started_at = datetime.now()

        try:
            # ä½¿ç”¨é…ç½®çš„æ¨¡å‹å’Œæ¸©åº¦å‚æ•°
            # æ³¨æ„ï¼šä½¿ç”¨ actual_modelï¼ˆproviders.yaml ä¸­å®šä¹‰çš„ API æ¨¡å‹åç§°ï¼‰
            llm_with_config = llm.bind(
                model=actual_model,
                temperature=temperature
            )

            # v3.1: æ„é€ å¸¦ä¾èµ–ä¸Šä¸‹æ–‡çš„ Prompt
            # å¦‚æœæœ‰å‰ç½®ä»»åŠ¡è¾“å‡ºï¼Œæ˜ç¡®æŒ‡ç¤ºä¸“å®¶å¿…é¡»åŸºäºè¿™äº›è¾“å‡ºæ‰§è¡Œ
            if dependency_context:
                human_message_content = f"""ã€é‡è¦ã€‘ä½ å¿…é¡»åŸºäºä»¥ä¸‹å‰ç½®ä»»åŠ¡çš„è¾“å‡ºç»“æœæ¥å®Œæˆå½“å‰ä»»åŠ¡ã€‚ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œå¿…é¡»ä»æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æå–å…³é”®æ•°æ®ã€‚

å‰ç½®ä»»åŠ¡è¾“å‡ºï¼ˆè¿™æ˜¯ä½ å”¯ä¸€çš„ä¿¡æ¯æ¥æºï¼‰ï¼š
{dependency_context}

---

å½“å‰ä»»åŠ¡æŒ‡ä»¤: {description}

é™„åŠ è¾“å…¥å‚æ•°:
{format_input_data(clean_input_data)}

---

âš ï¸ æ‰§è¡Œè¦æ±‚ï¼š
1. ä½ å¿…é¡»å¼•ç”¨å¹¶ä½¿ç”¨å‰ç½®ä»»åŠ¡è¾“å‡ºä¸­çš„å…·ä½“æ•°æ®
2. å¦‚æœå‰ç½®ä»»åŠ¡æä¾›äº†å¤šä¸ªé€‰é¡¹/æ•°æ®ç‚¹ï¼Œè¯·æ˜ç¡®è¯´æ˜ä½ ä½¿ç”¨äº†å“ªä¸€ä¸ª
3. ä¸è¦è¿”å›å ä½ç¬¦ï¼ˆå¦‚"[è¯·åœ¨æ­¤å¤„æ’å…¥...]"ï¼‰ï¼Œå¿…é¡»å¡«å…¥å®é™…ä»å‰ç½®è¾“å‡ºä¸­æå–çš„å†…å®¹"""
            else:
                human_message_content = f"""ä»»åŠ¡æè¿°: {description}

è¾“å…¥å‚æ•°:
{format_input_data(clean_input_data)}"""

            # ğŸ‘ˆ æ·»åŠ  RunnableConfig æ ‡ç­¾ï¼Œä¾¿äºæµå¼è¾“å‡ºè¿‡æ»¤
            from langchain_core.runnables import RunnableConfig
            response = await llm_with_config.ainvoke(
                [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=human_message_content)
                ],
                config=RunnableConfig(
                    tags=["expert", expert_key],
                    metadata={"node_type": "expert", "expert_type": expert_key}
                )
            )

            completed_at = datetime.now()
            duration_ms = int((completed_at - started_at).total_seconds() * 1000)

            print(f"[{expert_key.upper()}] ä¸“å®¶å®Œæˆ (è€—æ—¶: {duration_ms/1000:.2f}s)")

            # æ£€æŸ¥å¹¶æ¸…ç†è¾“å‡ºå†…å®¹ï¼ˆé¿å… task plan JSON æ³„éœ²åˆ°ç”¨æˆ·ç•Œé¢ï¼‰
            cleaned_content = _clean_expert_output(response.content, expert_key)

            result = {
                "output_result": cleaned_content,
                "status": "completed",
                "started_at": started_at.isoformat(),
                "completed_at": completed_at.isoformat(),
                "duration_ms": duration_ms
            }

            # æ ¹æ®ä¸“å®¶ç±»å‹å’Œå†…å®¹è‡ªåŠ¨ç¡®å®š artifact ç±»å‹
            artifact_type = _detect_artifact_type(cleaned_content, expert_key)

            # æ·»åŠ  artifactï¼ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹ï¼‰
            result["artifact"] = {
                "type": artifact_type,
                "title": f"{expert_config['name']}ç»“æœ",
                "content": cleaned_content,
                "source": f"{expert_key}_expert"
            }

            return result

        except Exception as e:
            print(f"[{expert_key.upper()}] ä¸“å®¶å¤±è´¥: {e}")
            return {
                "output_result": f"{expert_config['name']}å¤±è´¥: {str(e)}",
                "status": "failed",
                "error": str(e),
                "started_at": started_at.isoformat(),
                "completed_at": datetime.now().isoformat()
            }

    return expert_node


def _clean_expert_output(content: str, expert_key: str) -> str:
    """
    æ¸…ç†ä¸“å®¶è¾“å‡ºï¼Œé¿å… task plan JSON æ³„éœ²åˆ°ç”¨æˆ·ç•Œé¢
    
    æœ‰äº›ä¸“å®¶ï¼ˆå¦‚ writerã€plannerï¼‰å¯èƒ½ä¼šè¾“å‡º task plan æ ¼å¼çš„ JSONï¼Œ
    è¿™ä¸æ˜¯ç”¨æˆ·æƒ³è¦çš„ç»“æœã€‚è¿™ä¸ªå‡½æ•°ä¼šæ£€æµ‹å¹¶è½¬æ¢è¿™ç§è¾“å‡ºã€‚
    """
    import json
    import re
    
    content_stripped = content.strip()
    
    # ç§»é™¤ Markdown ä»£ç å—æ ‡è®°ï¼ˆå¦‚ ```json ... ```ï¼‰
    code_block_pattern = r'^```(?:json)?\s*([\s\S]*?)\s*```$'
    match = re.match(code_block_pattern, content_stripped)
    if match:
        content_stripped = match.group(1).strip()
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ task plan JSON
    if content_stripped.startswith('{') and content_stripped.endswith('}'):
        try:
            data = json.loads(content_stripped)
            # å¦‚æœåŒ…å« tasks å’Œ strategy å­—æ®µï¼Œè¯´æ˜æ˜¯ task plan
            if isinstance(data, dict) and 'tasks' in data and 'strategy' in data:
                print(f"[{expert_key.upper()}] æ£€æµ‹åˆ° task plan JSONï¼Œè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°")
                # è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°
                tasks = data.get('tasks', [])
                strategy = data.get('strategy', '')
                
                lines = ["## æ‰§è¡Œè®¡åˆ’", ""]
                if strategy:
                    lines.append(f"**ç­–ç•¥**: {strategy}")
                    lines.append("")
                
                if tasks:
                    lines.append("**ä»»åŠ¡åˆ—è¡¨**:")
                    for i, task in enumerate(tasks, 1):
                        expert_type = task.get('expert_type', 'unknown')
                        description = task.get('description', '')
                        lines.append(f"{i}. [{expert_type}] {description}")
                
                return "\n".join(lines)
        except json.JSONDecodeError:
            pass  # ä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼Œä¿æŒåŸæ ·
    
    return content

def format_input_data(input_data: Dict) -> str:
    """æ ¼å¼åŒ–è¾“å…¥æ•°æ®ä¸ºæ–‡æœ¬"""
    if not input_data:
        return "ï¼ˆæ— é¢å¤–å‚æ•°ï¼‰"

    lines = []
    for key, value in input_data.items():
        if isinstance(value, (list, dict)):
            lines.append(f"- {key}: {value}")
        else:
            lines.append(f"- {key}: {value}")

    return "\n".join(lines)


def _detect_artifact_type(content: str, expert_key: str) -> str:
    """
    æ ¹æ®å†…å®¹è‡ªåŠ¨æ£€æµ‹ artifact ç±»å‹

    ä¼˜å…ˆæ ¹æ®å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼Œå…¶æ¬¡æ ¹æ®ä¸“å®¶ç±»å‹å…œåº•

    Args:
        content: ä¸“å®¶è¾“å‡ºçš„å†…å®¹
        expert_key: ä¸“å®¶ç±»å‹æ ‡è¯†

    Returns:
        str: artifact ç±»å‹ (code | html | markdown | text | search)
    """
    content_lower = content.lower().strip()

    # 1. HTML æ£€æµ‹ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    # æ£€æµ‹å®Œæ•´çš„ HTML æ–‡æ¡£æˆ–åŒ…å« <html> æ ‡ç­¾çš„å†…å®¹
    if (content_lower.startswith("<!doctype html") or
        content_lower.startswith("<html") or
        ("<html" in content_lower and "</html>" in content_lower)):
        return "html"

    # 2. æ£€æµ‹æ˜¯å¦åŒ…å« HTML ä»£ç å—ï¼ˆ```htmlï¼‰
    import re
    html_code_block = re.search(r'```html\n([\s\S]*?)```', content, re.IGNORECASE)
    if html_code_block:
        html_content = html_code_block.group(1).lower().strip()
        if html_content.startswith("<") and (">" in html_content or "</" in html_content):
            return "html"

    # 3. æ£€æµ‹ Markdown æ ¼å¼
    has_markdown = any(marker in content for marker in ['# ', '## ', '### ', '> ', '- ', '* '])
    has_code_block = '```' in content

    # 4. æ ¹æ®ä¸“å®¶ç±»å‹å…œåº•
    artifact_type_map = {
        "coder": "code",
        "writer": "markdown",
        "search": "search",
        "planner": "markdown",
        "researcher": "markdown",
        "analyzer": "markdown",
        "image_analyzer": "text",
    }
    default_type = artifact_type_map.get(expert_key, "text")

    # 5. ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœ coder ç”Ÿæˆäº† markdownï¼Œè¿”å› markdown
    if expert_key == "coder" and has_markdown and not has_code_block:
        return "markdown"

    return default_type


# æ„å»ºä¸“å®¶å‡½æ•°æ˜ å°„
DYNAMIC_EXPERT_FUNCTIONS = {
    "search": create_expert_function("search"),
    "coder": create_expert_function("coder"),
    "researcher": create_expert_function("researcher"),
    "analyzer": create_expert_function("analyzer"),
    "writer": create_expert_function("writer"),
    "planner": create_expert_function("planner"),
    "image_analyzer": create_expert_function("image_analyzer"),
}


async def generic_worker_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    é€šç”¨å·¥ä½œèŠ‚ç‚¹ï¼šå¤„ç†è‡ªå®šä¹‰ä¸“å®¶ï¼ˆéç³»ç»Ÿå†…ç½®ä¸“å®¶ï¼‰
    
    å¯¹äºæ•°æ®åº“ä¸­åŠ¨æ€åˆ›å»ºçš„ä¸“å®¶ï¼Œä½¿ç”¨æ­¤é€šç”¨èŠ‚ç‚¹æ‰§è¡Œã€‚
    è¯¥èŠ‚ç‚¹è‡ªå·±è´Ÿè´£åˆ›å»º LLM å®ä¾‹å¹¶åŠ è½½ä¸“å®¶é…ç½®ã€‚
    
    Args:
        state: å®Œæ•´çš„ AgentState
    
    Returns:
        Dict: æ‰§è¡Œç»“æœï¼ˆåŒ…å« output_resultã€statusã€artifact ç­‰ï¼‰
    """
    # è·å–å½“å‰ä»»åŠ¡
    task_list = state.get("task_list", [])
    current_index = state.get("current_task_index", 0)
    
    if current_index >= len(task_list):
        return {"error": "æ²¡æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡"}
    
    current_task = task_list[current_index]
    expert_type = current_task.get("expert_type", "unknown")
    description = current_task.get("description", "")
    input_data = current_task.get("input_data", {})
    
    print(f"[GenericWorker] å¤„ç†è‡ªå®šä¹‰ä¸“å®¶: {expert_type}")
    
    # ä»ç¼“å­˜åŠ è½½ä¸“å®¶é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»æ•°æ®åº“åŠ è½½
    expert_config = get_expert_config_cached(expert_type)
    
    # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ•°æ®åº“åŠ è½½ï¼ˆæ”¯æŒåŠ¨æ€åˆ›å»ºçš„è‡ªå®šä¹‰ä¸“å®¶ï¼‰
    if not expert_config and "db_session" in state:
        db = state["db_session"]
        try:
            from agents.expert_loader import get_expert_config
            expert_config = get_expert_config(expert_type, db)
            print(f"[GenericWorker] ä»æ•°æ®åº“åŠ è½½ä¸“å®¶é…ç½®: {expert_type}")
        except Exception as e:
            print(f"[GenericWorker] ä»æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
    
    if not expert_config:
        return {
            "error": f"æœªæ‰¾åˆ°ä¸“å®¶ '{expert_type}' çš„é…ç½®",
            "status": "failed",
            "output_result": f"ä¸“å®¶ '{expert_type}' æœªé…ç½®"
        }
    
    # è‡ªå·±åˆ›å»º LLM å®ä¾‹
    from utils.llm_factory import get_expert_llm
    if 'provider' in expert_config:
        llm = get_expert_llm(provider=expert_config['provider'])
    else:
        llm = get_expert_llm()
    
    # åº”ç”¨æ¨¡å‹å…œåº•æœºåˆ¶
    model = get_effective_model(expert_config.get("model"))
    temperature = expert_config.get("temperature", 0.5)
    system_prompt = expert_config.get("system_prompt", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚")
    expert_name = expert_config.get("name", expert_type)
    
    print(f"[GenericWorker] ä½¿ç”¨æ¨¡å‹: {model}, æ¸©åº¦: {temperature}")
    
    started_at = datetime.now()
    
    try:
        # ä½¿ç”¨é…ç½®çš„æ¨¡å‹å’Œæ¸©åº¦å‚æ•°
        llm_with_config = llm.bind(
            model=model,
            temperature=temperature
        )
        
        from langchain_core.runnables import RunnableConfig
        response = await llm_with_config.ainvoke(
            [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"ä»»åŠ¡æè¿°: {description}\n\nè¾“å…¥å‚æ•°:\n{format_input_data(input_data)}")
            ],
            config=RunnableConfig(
                tags=["expert", expert_type, "generic"],
                metadata={"node_type": "expert", "expert_type": expert_type, "is_generic": True}
            )
        )
        
        completed_at = datetime.now()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)
        
        print(f"[GenericWorker] ä¸“å®¶å®Œæˆ (è€—æ—¶: {duration_ms/1000:.2f}s)")
        
        # æ£€æŸ¥å¹¶æ¸…ç†è¾“å‡ºå†…å®¹
        cleaned_content = _clean_expert_output(response.content, expert_type)
        
        # æ ¹æ®å†…å®¹è‡ªåŠ¨ç¡®å®š artifact ç±»å‹
        artifact_type = _detect_artifact_type(cleaned_content, expert_type)
        
        return {
            "output_result": cleaned_content,
            "status": "completed",
            "started_at": started_at.isoformat(),
            "completed_at": completed_at.isoformat(),
            "duration_ms": duration_ms,
            "artifact": {
                "type": artifact_type,
                "title": f"{expert_name}ç»“æœ",
                "content": cleaned_content,
                "source": f"{expert_type}_expert"
            }
        }
        
    except Exception as e:
        print(f"[GenericWorker] ä¸“å®¶å¤±è´¥: {e}")
        return {
            "output_result": f"{expert_name}å¤±è´¥: {str(e)}",
            "status": "failed",
            "error": str(e),
            "started_at": started_at.isoformat(),
            "completed_at": datetime.now().isoformat()
        }


def get_expert_function(expert_type: str):
    """
    è·å–ä¸“å®¶æ‰§è¡Œå‡½æ•°
    
    å¯¹äºç³»ç»Ÿå†…ç½®ä¸“å®¶ï¼ˆsearch, coder, researcher, analyzer, writer, planner, image_analyzerï¼‰ï¼Œ
    è¿”å›å¯¹åº”çš„ç¡¬ç¼–ç å‡½æ•°ã€‚
    
    å¯¹äºè‡ªå®šä¹‰ä¸“å®¶ï¼ˆæ•°æ®åº“ä¸­åŠ¨æ€åˆ›å»ºçš„ï¼‰ï¼Œè¿”å› generic_worker_nodeã€‚
    
    Args:
        expert_type: ä¸“å®¶ç±»å‹æ ‡è¯†
    
    Returns:
        callable: ä¸“å®¶æ‰§è¡Œå‡½æ•°
    """
    if expert_type in DYNAMIC_EXPERT_FUNCTIONS:
        return DYNAMIC_EXPERT_FUNCTIONS[expert_type]
    else:
        return generic_worker_node


def initialize_expert_cache(session):
    """
    åˆå§‹åŒ–ä¸“å®¶é…ç½®ç¼“å­˜

    åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ï¼Œé¢„åŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®

    Args:
        session: æ•°æ®åº“ä¼šè¯
    """
    from agents.expert_loader import get_expert_config_cached

    print("[DynamicExpert] Initializing expert cache...")

    # é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶
    for expert_key in DYNAMIC_EXPERT_FUNCTIONS.keys():
        config = get_expert_config_cached(expert_key, session)
        if config:
            print(f"  - Loaded: {config['name']} ({expert_key})")
        else:
            print(f"  - Not found: {expert_key} (will use fallback)")

    print("[DynamicExpert] Expert cache initialized")


def get_all_expert_list(db_session: Optional[Session] = None) -> List[tuple]:
    """
    è·å–æ‰€æœ‰å¯ç”¨ä¸“å®¶çš„åˆ—è¡¨ï¼ˆåŒ…æ‹¬åŠ¨æ€åˆ›å»ºçš„ä¸“å®¶ï¼‰
    
    ä»æ•°æ®åº“ä¸­è·å–æ‰€æœ‰ SystemExpert è®°å½•ï¼Œè¿”å›æ ¼å¼åŒ–çš„ä¸“å®¶ä¿¡æ¯åˆ—è¡¨ã€‚
    ç”¨äº Commander Node åŠ¨æ€æ³¨å…¥ä¸“å®¶åˆ—è¡¨åˆ° System Promptã€‚
    
    Args:
        db_session: æ•°æ®åº“ä¼šè¯ï¼Œå¦‚æœä¸º None åˆ™è¿”å›ç¡¬ç¼–ç ä¸“å®¶åˆ—è¡¨
        
    Returns:
        List[tuple]: ä¸“å®¶åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (expert_key, name, description) å…ƒç»„
        
    Example:
        >>> experts = get_all_expert_list(db_session)
        >>> print(experts)
        [('search', 'æœç´¢ä¸“å®¶', 'æ“…é•¿ä¿¡æ¯æœç´¢å’ŒæŸ¥è¯¢'), ('coder', 'ç¼–ç¨‹ä¸“å®¶', 'æ“…é•¿ä»£ç ç¼–å†™å’Œè°ƒè¯•')]
    """
    # ç¡¬ç¼–ç ä¸“å®¶åˆ—è¡¨ä½œä¸ºå›é€€
    fallback_experts = [
        ("search", "æœç´¢ä¸“å®¶", "ç”¨äºæœç´¢ã€æŸ¥è¯¢ä¿¡æ¯"),
        ("coder", "ç¼–ç¨‹ä¸“å®¶", "ç”¨äºä»£ç ç¼–å†™ã€è°ƒè¯•ã€ä¼˜åŒ–"),
        ("researcher", "ç ”ç©¶ä¸“å®¶", "ç”¨äºæ·±å…¥ç ”ç©¶ã€æ–‡çŒ®è°ƒç ”"),
        ("analyzer", "åˆ†æä¸“å®¶", "ç”¨äºæ•°æ®åˆ†æã€é€»è¾‘æ¨ç†"),
        ("writer", "å†™ä½œä¸“å®¶", "ç”¨äºæ–‡æ¡ˆæ’°å†™ã€å†…å®¹åˆ›ä½œ"),
        ("planner", "è§„åˆ’ä¸“å®¶", "ç”¨äºä»»åŠ¡è§„åˆ’ã€æ–¹æ¡ˆè®¾è®¡"),
        ("image_analyzer", "å›¾ç‰‡åˆ†æä¸“å®¶", "ç”¨äºå›¾ç‰‡å†…å®¹åˆ†æã€è§†è§‰è¯†åˆ«"),
    ]
    
    # å¦‚æœæ²¡æœ‰æä¾›æ•°æ®åº“ä¼šè¯ï¼Œç›´æ¥è¿”å›ç¡¬ç¼–ç åˆ—è¡¨
    if db_session is None:
        print("[DynamicExpert] æœªæä¾›æ•°æ®åº“ä¼šè¯ï¼Œä½¿ç”¨ç¡¬ç¼–ç ä¸“å®¶åˆ—è¡¨")
        return fallback_experts
    
    experts = []
    
    try:
        # ä»æ•°æ®åº“æŸ¥è¯¢æ‰€æœ‰ SystemExpertï¼ˆåŒ…æ‹¬åŠ¨æ€åˆ›å»ºçš„ï¼‰
        statement = select(SystemExpert).order_by(SystemExpert.expert_key)
        results = db_session.exec(statement).all()
        
        for expert in results:
            experts.append((
                expert.expert_key,
                expert.name,
                expert.description or "æš‚æ— æè¿°"
            ))
        
        print(f"[DynamicExpert] ä»æ•°æ®åº“åŠ è½½äº† {len(experts)} ä¸ªä¸“å®¶")
            
    except Exception as e:
        print(f"[DynamicExpert] è·å–ä¸“å®¶åˆ—è¡¨å¤±è´¥: {e}ï¼Œä½¿ç”¨ç¡¬ç¼–ç åˆ—è¡¨")
        # å‘ç”Ÿå¼‚å¸¸æ—¶è¿”å›ç¡¬ç¼–ç çš„ä¸“å®¶åˆ—è¡¨
        experts = fallback_experts
    
    return experts


def format_expert_list_for_prompt(experts: List[tuple]) -> str:
    """
    å°†ä¸“å®¶åˆ—è¡¨æ ¼å¼åŒ–ä¸ºé€‚åˆæ’å…¥ Prompt çš„å­—ç¬¦ä¸²
    
    æ ¼å¼ï¼š- expert_key (Name): Description
    
    Args:
        experts: ä¸“å®¶åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (expert_key, name, description) å…ƒç»„
        
    Returns:
        str: æ ¼å¼åŒ–åçš„ä¸“å®¶åˆ—è¡¨å­—ç¬¦ä¸²
        
    Example:
        >>> experts = [('search', 'æœç´¢ä¸“å®¶', 'æ“…é•¿ä¿¡æ¯æœç´¢')]
        >>> format_expert_list_for_prompt(experts)
        '- search (æœç´¢ä¸“å®¶): æ“…é•¿ä¿¡æ¯æœç´¢'
    """
    if not experts:
        return "ï¼ˆæš‚æ— å¯ç”¨ä¸“å®¶ï¼‰"
    
    lines = []
    for expert_key, name, description in experts:
        lines.append(f"- {expert_key} ({name}): {description}")
    
    return "\n".join(lines)
