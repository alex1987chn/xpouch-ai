"""
åŠ¨æ€ä¸“å®¶æ‰§è¡Œç³»ç»Ÿï¼ˆä½¿ç”¨æ•°æ®åº“åŠ è½½çš„ Promptï¼‰

é‡æ„ä¸“å®¶æ‰§è¡Œé€»è¾‘ï¼š
1. ä½¿ç”¨ expert_loader ä»æ•°æ®åº“åŠ è½½é…ç½®
2. ä½¿ç”¨åŠ¨æ€æ¨¡å‹å’Œæ¸©åº¦å‚æ•°
3. æ”¯æŒç®¡ç†å‘˜å®æ—¶æ›´æ–° Prompt
"""
import os
from typing import Dict, Any, List
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from langchain_openai import ChatOpenAI
from datetime import datetime

from agents.expert_loader import get_expert_config_cached, refresh_cache
from agents.experts import EXPERT_DESCRIPTIONS
from agents.model_fallback import get_effective_model, get_default_model


def create_expert_function(expert_key: str):
    """
    åˆ›å»ºä¸“å®¶å‡½æ•°å·¥å‚

    æ ¹æ®ä¸“å®¶ç±»å‹åŠ¨æ€ç”Ÿæˆæ‰§è¡Œå‡½æ•°

    Args:
        expert_key: ä¸“å®¶ç±»å‹æ ‡è¯†

    Returns:
        callable: ä¸“å®¶æ‰§è¡Œå‡½æ•°
    """
    async def expert_node(state: Dict[str, Any], llm: ChatOpenAI) -> Dict[str, Any]:
        """
        åŠ¨æ€ä¸“å®¶èŠ‚ç‚¹ï¼šä»æ•°æ®åº“åŠ è½½é…ç½®å¹¶æ‰§è¡Œ

        Args:
            state: å®Œæ•´çš„ AgentState
            llm: LLM å®ä¾‹

        Returns:
            Dict: æ›´æ–°åçš„ AgentState
        """
        # ä»æ•°æ®åº“/ç¼“å­˜åŠ è½½ä¸“å®¶é…ç½®
        expert_config = get_expert_config_cached(expert_key)

        if not expert_config:
            # é™çº§ï¼šä½¿ç”¨ç¡¬ç¼–ç  Prompt
            from agents.experts import EXPERT_PROMPTS
            # ğŸ‘ˆ ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç çš„ gpt-4o
            default_model = get_default_model()
            expert_config = {
                "expert_key": expert_key,
                "name": EXPERT_DESCRIPTIONS.get(expert_key, expert_key),
                "system_prompt": EXPERT_PROMPTS.get(expert_key, ""),
                "model": default_model,
                "temperature": 0.5
            }
            print(f"[DynamicExpert] Using fallback config for '{expert_key}': model={default_model}")

        system_prompt = expert_config["system_prompt"]
        # ğŸ‘ˆ åº”ç”¨æ¨¡å‹å…œåº•æœºåˆ¶
        model = get_effective_model(expert_config.get("model"))
        temperature = expert_config["temperature"]

        print(f"[DynamicExpert] Running {expert_key} with model={model}, temp={temperature}")

        # è·å–å½“å‰ä»»åŠ¡
        task_list = state.get("task_list", [])
        current_index = state.get("current_task_index", 0)

        if current_index >= len(task_list):
            return {"error": "æ²¡æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡"}

        current_task = task_list[current_index]
        description = current_task.get("description", "")
        input_data = current_task.get("input_data", {})

        started_at = datetime.now()

        try:
            # ä½¿ç”¨é…ç½®çš„æ¨¡å‹å’Œæ¸©åº¦å‚æ•°
            llm_with_config = llm.bind(
                model=model,
                temperature=temperature
            )

            # ğŸ‘ˆ æ·»åŠ  RunnableConfig æ ‡ç­¾ï¼Œä¾¿äºæµå¼è¾“å‡ºè¿‡æ»¤
            from langchain_core.runnables import RunnableConfig
            response = await llm_with_config.ainvoke(
                [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=f"ä»»åŠ¡æè¿°: {description}\n\nè¾“å…¥å‚æ•°:\n{format_input_data(input_data)}")
                ],
                config=RunnableConfig(
                    tags=["expert", expert_key],
                    metadata={"node_type": "expert", "expert_type": expert_key}
                )
            )

            completed_at = datetime.now()
            duration_ms = int((completed_at - started_at).total_seconds() * 1000)

            print(f"[{expert_key.upper()}] ä¸“å®¶å®Œæˆ (è€—æ—¶: {duration_ms/1000:.2f}s)")

            # æ£€æŸ¥å¹¶æ¸…ç†è¾“å‡ºå†…å®¹ï¼ˆé¿å… task plan JSON æ³„éœ²åˆ°ç”¨æˆ·ç•Œé¢ï¼‰
            cleaned_content = _clean_expert_output(response.content, expert_key)

            result = {
                "output_result": cleaned_content,
                "status": "completed",
                "started_at": started_at.isoformat(),
                "completed_at": completed_at.isoformat(),
                "duration_ms": duration_ms
            }

            # æ ¹æ®ä¸“å®¶ç±»å‹ç¡®å®š artifact ç±»å‹
            artifact_type_map = {
                "coder": "code",
                "writer": "markdown",
                "search": "search",
                "planner": "markdown",
                "researcher": "markdown",
                "analyzer": "markdown",
                "image_analyzer": "text",
            }
            artifact_type = artifact_type_map.get(expert_key, "text")

            # æ·»åŠ  artifactï¼ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹ï¼‰
            result["artifact"] = {
                "type": artifact_type,
                "title": f"{expert_config['name']}ç»“æœ",
                "content": cleaned_content,
                "source": f"{expert_key}_expert"
            }

            return result

        except Exception as e:
            print(f"[{expert_key.upper()}] ä¸“å®¶å¤±è´¥: {e}")
            return {
                "output_result": f"{expert_config['name']}å¤±è´¥: {str(e)}",
                "status": "failed",
                "error": str(e),
                "started_at": started_at.isoformat(),
                "completed_at": datetime.now().isoformat()
            }

    return expert_node


def _clean_expert_output(content: str, expert_key: str) -> str:
    """
    æ¸…ç†ä¸“å®¶è¾“å‡ºï¼Œé¿å… task plan JSON æ³„éœ²åˆ°ç”¨æˆ·ç•Œé¢
    
    æœ‰äº›ä¸“å®¶ï¼ˆå¦‚ writerã€plannerï¼‰å¯èƒ½ä¼šè¾“å‡º task plan æ ¼å¼çš„ JSONï¼Œ
    è¿™ä¸æ˜¯ç”¨æˆ·æƒ³è¦çš„ç»“æœã€‚è¿™ä¸ªå‡½æ•°ä¼šæ£€æµ‹å¹¶è½¬æ¢è¿™ç§è¾“å‡ºã€‚
    """
    import json
    
    content_stripped = content.strip()
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ task plan JSON
    if content_stripped.startswith('{') and content_stripped.endswith('}'):
        try:
            data = json.loads(content_stripped)
            # å¦‚æœåŒ…å« tasks å’Œ strategy å­—æ®µï¼Œè¯´æ˜æ˜¯ task plan
            if isinstance(data, dict) and 'tasks' in data and 'strategy' in data:
                print(f"[{expert_key.upper()}] æ£€æµ‹åˆ° task plan JSONï¼Œè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°")
                # è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°
                tasks = data.get('tasks', [])
                strategy = data.get('strategy', '')
                
                lines = ["## æ‰§è¡Œè®¡åˆ’", ""]
                if strategy:
                    lines.append(f"**ç­–ç•¥**: {strategy}")
                    lines.append("")
                
                if tasks:
                    lines.append("**ä»»åŠ¡åˆ—è¡¨**:")
                    for i, task in enumerate(tasks, 1):
                        expert_type = task.get('expert_type', 'unknown')
                        description = task.get('description', '')
                        lines.append(f"{i}. [{expert_type}] {description}")
                
                return "\n".join(lines)
        except json.JSONDecodeError:
            pass  # ä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼Œä¿æŒåŸæ ·
    
    return content

def format_input_data(input_data: Dict) -> str:
    """æ ¼å¼åŒ–è¾“å…¥æ•°æ®ä¸ºæ–‡æœ¬"""
    if not input_data:
        return "ï¼ˆæ— é¢å¤–å‚æ•°ï¼‰"

    lines = []
    for key, value in input_data.items():
        if isinstance(value, (list, dict)):
            lines.append(f"- {key}: {value}")
        else:
            lines.append(f"- {key}: {value}")

    return "\n".join(lines)


# æ„å»ºä¸“å®¶å‡½æ•°æ˜ å°„
DYNAMIC_EXPERT_FUNCTIONS = {
    "search": create_expert_function("search"),
    "coder": create_expert_function("coder"),
    "researcher": create_expert_function("researcher"),
    "analyzer": create_expert_function("analyzer"),
    "writer": create_expert_function("writer"),
    "planner": create_expert_function("planner"),
    "image_analyzer": create_expert_function("image_analyzer"),
}


def initialize_expert_cache(session):
    """
    åˆå§‹åŒ–ä¸“å®¶é…ç½®ç¼“å­˜

    åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ï¼Œé¢„åŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®

    Args:
        session: æ•°æ®åº“ä¼šè¯
    """
    from agents.expert_loader import get_expert_config_cached

    print("[DynamicExpert] Initializing expert cache...")

    # é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶
    for expert_key in DYNAMIC_EXPERT_FUNCTIONS.keys():
        config = get_expert_config_cached(expert_key, session)
        if config:
            print(f"  - Loaded: {config['name']} ({expert_key})")
        else:
            print(f"  - Not found: {expert_key} (will use fallback)")

    print("[DynamicExpert] Expert cache initialized")
