"""
åŠ¨æ€ä¸“å®¶æ‰§è¡Œç³»ç»Ÿï¼ˆä½¿ç”¨æ•°æ®åº“åŠ è½½çš„ Promptï¼‰

é‡æ„ä¸“å®¶æ‰§è¡Œé€»è¾‘ï¼š
1. ä½¿ç”¨ expert_loader ä»æ•°æ®åº“åŠ è½½é…ç½®
2. ä½¿ç”¨åŠ¨æ€æ¨¡å‹å’Œæ¸©åº¦å‚æ•°
3. æ”¯æŒç®¡ç†å‘˜å®æ—¶æ›´æ–° Prompt
"""
import os
from typing import Dict, Any, List
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from langchain_openai import ChatOpenAI
from datetime import datetime

from agents.expert_loader import get_expert_config_cached, refresh_cache
from agents.experts import EXPERT_DESCRIPTIONS
from agents.model_fallback import get_effective_model, get_default_model


def create_expert_function(expert_key: str):
    """
    åˆ›å»ºä¸“å®¶å‡½æ•°å·¥å‚

    æ ¹æ®ä¸“å®¶ç±»å‹åŠ¨æ€ç”Ÿæˆæ‰§è¡Œå‡½æ•°

    Args:
        expert_key: ä¸“å®¶ç±»å‹æ ‡è¯†

    Returns:
        callable: ä¸“å®¶æ‰§è¡Œå‡½æ•°
    """
    async def expert_node(state: Dict[str, Any], llm: ChatOpenAI) -> Dict[str, Any]:
        """
        åŠ¨æ€ä¸“å®¶èŠ‚ç‚¹ï¼šä»æ•°æ®åº“åŠ è½½é…ç½®å¹¶æ‰§è¡Œ

        Args:
            state: å®Œæ•´çš„ AgentState
            llm: LLM å®ä¾‹

        Returns:
            Dict: æ›´æ–°åçš„ AgentState
        """
        # ä»æ•°æ®åº“/ç¼“å­˜åŠ è½½ä¸“å®¶é…ç½®
        expert_config = get_expert_config_cached(expert_key)

        if not expert_config:
            # é™çº§ï¼šä½¿ç”¨ç¡¬ç¼–ç  Prompt
            from agents.experts import EXPERT_PROMPTS
            # ğŸ‘ˆ ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç çš„ gpt-4o
            default_model = get_default_model()
            expert_config = {
                "expert_key": expert_key,
                "name": EXPERT_DESCRIPTIONS.get(expert_key, expert_key),
                "system_prompt": EXPERT_PROMPTS.get(expert_key, ""),
                "model": default_model,
                "temperature": 0.5
            }
            print(f"[DynamicExpert] Using fallback config for '{expert_key}': model={default_model}")

        system_prompt = expert_config["system_prompt"]
        # ğŸ‘ˆ åº”ç”¨æ¨¡å‹å…œåº•æœºåˆ¶
        model = get_effective_model(expert_config.get("model"))
        temperature = expert_config["temperature"]

        print(f"[DynamicExpert] Running {expert_key} with model={model}, temp={temperature}")

        # è·å–å½“å‰ä»»åŠ¡
        task_list = state.get("task_list", [])
        current_index = state.get("current_task_index", 0)

        if current_index >= len(task_list):
            return {"error": "æ²¡æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡"}

        current_task = task_list[current_index]
        description = current_task.get("description", "")
        input_data = current_task.get("input_data", {})

        started_at = datetime.now()

        try:
            # ä½¿ç”¨é…ç½®çš„æ¨¡å‹å’Œæ¸©åº¦å‚æ•°
            llm_with_config = llm.bind(
                model=model,
                temperature=temperature
            )

            response = await llm_with_config.ainvoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"ä»»åŠ¡æè¿°: {description}\n\nè¾“å…¥å‚æ•°:\n{format_input_data(input_data)}")
            ])

            completed_at = datetime.now()
            duration_ms = int((completed_at - started_at).total_seconds() * 1000)

            print(f"[{expert_key.upper()}] ä¸“å®¶å®Œæˆ (è€—æ—¶: {duration_ms/1000:.2f}s)")

            result = {
                "output_result": response.content,
                "status": "completed",
                "started_at": started_at.isoformat(),
                "completed_at": completed_at.isoformat(),
                "duration_ms": duration_ms
            }

            # æ·»åŠ  text artifact
            result["artifact"] = {
                "type": "text",
                "title": f"{expert_config['name']}ç»“æœ",
                "content": response.content,
                "source": f"{expert_key}_expert"
            }

            return result

        except Exception as e:
            print(f"[{expert_key.upper()}] ä¸“å®¶å¤±è´¥: {e}")
            return {
                "output_result": f"{expert_config['name']}å¤±è´¥: {str(e)}",
                "status": "failed",
                "error": str(e),
                "started_at": started_at.isoformat(),
                "completed_at": datetime.now().isoformat()
            }

    return expert_node


def format_input_data(input_data: Dict) -> str:
    """æ ¼å¼åŒ–è¾“å…¥æ•°æ®ä¸ºæ–‡æœ¬"""
    if not input_data:
        return "ï¼ˆæ— é¢å¤–å‚æ•°ï¼‰"

    lines = []
    for key, value in input_data.items():
        if isinstance(value, (list, dict)):
            lines.append(f"- {key}: {value}")
        else:
            lines.append(f"- {key}: {value}")

    return "\n".join(lines)


# æ„å»ºä¸“å®¶å‡½æ•°æ˜ å°„
DYNAMIC_EXPERT_FUNCTIONS = {
    "search": create_expert_function("search"),
    "coder": create_expert_function("coder"),
    "researcher": create_expert_function("researcher"),
    "analyzer": create_expert_function("analyzer"),
    "writer": create_expert_function("writer"),
    "planner": create_expert_function("planner"),
    "image_analyzer": create_expert_function("image_analyzer"),
}


def initialize_expert_cache(session):
    """
    åˆå§‹åŒ–ä¸“å®¶é…ç½®ç¼“å­˜

    åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ï¼Œé¢„åŠ è½½æ‰€æœ‰ä¸“å®¶é…ç½®

    Args:
        session: æ•°æ®åº“ä¼šè¯
    """
    from agents.expert_loader import get_expert_config_cached

    print("[DynamicExpert] Initializing expert cache...")

    # é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶
    for expert_key in DYNAMIC_EXPERT_FUNCTIONS.keys():
        config = get_expert_config_cached(expert_key, session)
        if config:
            print(f"  - Loaded: {config['name']} ({expert_key})")
        else:
            print(f"  - Not found: {expert_key} (will use fallback)")

    print("[DynamicExpert] Expert cache initialized")
