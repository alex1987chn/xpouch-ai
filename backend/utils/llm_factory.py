"""
LLM å·¥å‚æ¨¡å—

ç»Ÿä¸€ç®¡ç†å’Œåˆ›å»º LLM å®ä¾‹ï¼Œå®Œå…¨åŸºäºé…ç½®æ–‡ä»¶ï¼ˆproviders.yamlï¼‰
æ¶ˆé™¤ç¡¬ç¼–ç ï¼Œæ”¯æŒåŠ¨æ€æ·»åŠ æ–°æä¾›å•†

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # è·å–æŒ‡å®šæä¾›å•†çš„ LLM
    llm = get_llm_instance(provider="minimax", streaming=True)
    
    # Router è‡ªåŠ¨é€‰æ‹©æœ€ä½³æä¾›å•†
    router_llm = get_router_llm()
"""

from typing import Optional
from langchain_openai import ChatOpenAI
from providers_config import (
    get_provider_config,
    get_provider_api_key,
    get_best_router_provider,
    is_provider_configured
)
import httpx


def get_llm_instance(
    provider: str,
    model: Optional[str] = None,
    streaming: bool = False,
    temperature: Optional[float] = None,
) -> ChatOpenAI:
    """
    ç»Ÿä¸€çš„ LLM å·¥å‚å‡½æ•° - å®Œå…¨é…ç½®åŒ–
    
    ä» providers.yaml è¯»å–é…ç½®ï¼Œä» .env è¯»å– API Key
    
    Args:
        provider: æä¾›å•†æ ‡è¯†ï¼ˆå¦‚ 'minimax', 'deepseek', 'openai'ï¼‰
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ default_model
        streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
    
    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    
    Raises:
        ValueError: å¦‚æœæä¾›å•†æœªé…ç½®æˆ–ç¼ºå°‘ API Key
    
    ç¤ºä¾‹ï¼š
        >>> llm = get_llm_instance("minimax", streaming=True, temperature=0.1)
        >>> llm = get_llm_instance("deepseek", model="deepseek-reasoner")
    """
    # è¯»å–æä¾›å•†é…ç½®
    config = get_provider_config(provider)
    if not config:
        raise ValueError(
            f"æœªçŸ¥çš„æä¾›å•†: {provider}\n"
            f"è¯·åœ¨ providers.yaml ä¸­æ·»åŠ é…ç½®ï¼Œ"
            f"æˆ–æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®ã€‚"
        )
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨
    if not config.get('enabled', True):
        raise ValueError(
            f"æä¾›å•† {provider} å·²åœ¨é…ç½®ä¸­ç¦ç”¨ (enabled: false)"
        )
    
    # ä»ç¯å¢ƒå˜é‡è¯»å– API Key
    api_key = get_provider_api_key(provider)
    if not api_key:
        env_key = config.get('env_key', f'{provider.upper()}_API_KEY')
        raise ValueError(
            f"æœªé…ç½® {provider} çš„ API Key\n"
            f"è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: {env_key}=your-api-key"
        )
    
    # æ„å»º LLM é…ç½®
    llm_config = {
        'model': model or config.get('default_model'),
        'api_key': api_key,
        'base_url': config.get('base_url'),
        'streaming': streaming,
    }
    
    # æ¸©åº¦å‚æ•°
    if temperature is not None:
        llm_config['temperature'] = temperature
    elif 'temperature' in config:
        llm_config['temperature'] = config['temperature']
    else:
        llm_config['temperature'] = 0.7  # é»˜è®¤å€¼
    
    # ğŸš€ ä¿®å¤ï¼šåˆ›å»ºæ›´å¥å£®çš„ HTTP å®¢æˆ·ç«¯
    # ç¦ç”¨ HTTP/2 å¹¶å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œè§£å†³ "incomplete chunked read" é”™è¯¯
    http_client = httpx.Client(
        http2=False,      # ğŸš¨ å…³é”®ï¼šç¦ç”¨ HTTP/2ï¼Œè§£å†³å¤§éƒ¨åˆ† chunked read é”™è¯¯
        timeout=300.0,    # ğŸš¨ å…³é”®ï¼šç»™æ¨ç†æ¨¡å‹è¶³å¤Ÿçš„æ€è€ƒæ—¶é—´ï¼ˆ5 åˆ†é’Ÿï¼‰
        verify=True        # éªŒè¯ SSL è¯ä¹¦ï¼ˆå®‰å…¨è€ƒè™‘ï¼‰
    )
    llm_config['http_client'] = http_client
    
    return ChatOpenAI(**llm_config)


def get_llm_by_model(model_id: str, streaming: bool = False) -> ChatOpenAI:
    """
    é€šè¿‡æ¨¡å‹ ID è·å– LLM å®ä¾‹
    
    Args:
        model_id: æ¨¡å‹æ ‡è¯†ï¼ˆå¦‚ 'minimax-2.1', 'gpt-4o'ï¼‰
        streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
    
    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    """
    from providers_config import get_model_config
    
    model_config = get_model_config(model_id)
    if not model_config:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ ID: {model_id}")
    
    provider = model_config.get('provider')
    model = model_config.get('model')
    
    return get_llm_instance(provider=provider, model=model, streaming=streaming)


def get_router_llm() -> ChatOpenAI:
    """
    è·å– Router èŠ‚ç‚¹ä¸“ç”¨çš„ LLM å®ä¾‹
    
    è‡ªåŠ¨é€‰æ‹©å·²é…ç½®ä¸”ä¼˜å…ˆçº§æœ€é«˜çš„æä¾›å•†ï¼ˆé»˜è®¤ä¼˜å…ˆ MiniMaxï¼‰
    ä½¿ç”¨è¾ƒä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
    
    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    
    Raises:
        ValueError: å¦‚æœæ²¡æœ‰å¯ç”¨çš„æä¾›å•†
    """
    # è·å–æœ€ä½³ Router æä¾›å•†
    provider = get_best_router_provider()
    
    if not provider:
        raise ValueError(
            "æ²¡æœ‰å¯ç”¨çš„ LLM æä¾›å•†ç”¨äº Router\n"
            "è¯·è‡³å°‘åœ¨ .env ä¸­é…ç½®ä¸€ä¸ªæä¾›å•†çš„ API Key\n"
            "æ”¯æŒçš„æä¾›å•†: minimax, deepseek, openai"
        )
    
    # è¯»å– Router é…ç½®ï¼ˆä» providers.yamlï¼‰
    from providers_config import get_router_config
    router_config = get_router_config()
    
    temperature = router_config.get('temperature', 0.1)
    streaming = router_config.get('streaming', True)
    
    return get_llm_instance(
        provider=provider,
        streaming=streaming,
        temperature=temperature
    )





def get_expert_llm(
    provider: Optional[str] = None,
    model: Optional[str] = None,
    temperature: Optional[float] = None
) -> ChatOpenAI:
    """
    è·å– Expert èŠ‚ç‚¹ä¸“ç”¨çš„ LLM å®ä¾‹

    Args:
        provider: æŒ‡å®šæä¾›å•†ï¼Œé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå·²é…ç½®çš„
        model: æŒ‡å®šæ¨¡å‹
        temperature: ä¸“å®¶èŠ‚ç‚¹å¯è‡ªå®šä¹‰æ¸©åº¦

    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    """
    # å¦‚æœæŒ‡å®šäº†æä¾›å•†
    if provider:
        return get_llm_instance(
            provider=provider,
            model=model,
            streaming=True,
            temperature=temperature
        )

    # é»˜è®¤ä½¿ç”¨ deepseekï¼ˆæ€§ä»·æ¯”é«˜ï¼‰
    if is_provider_configured('deepseek'):
        return get_llm_instance(
            provider='deepseek',
            model=model,
            streaming=True,
            temperature=temperature or 0.7
        )

    # å›é€€åˆ°ä»»ä½•å¯ç”¨çš„
    return get_llm_instance(
        provider=get_best_router_provider(),
        model=model,
        streaming=True,
        temperature=temperature
    )


def get_commander_llm() -> ChatOpenAI:
    """
    è·å– Commander èŠ‚ç‚¹ä¸“ç”¨çš„ LLM å®ä¾‹

    Commander éœ€è¦è¾ƒå¼ºçš„è§„åˆ’èƒ½åŠ›ï¼Œä¼˜å…ˆä½¿ç”¨ DeepSeek æˆ– OpenAI
    é»˜è®¤æ¸©åº¦ä¸º 0.5ï¼ˆè§„åˆ’ä»»åŠ¡éœ€è¦ä¸€å®šçš„åˆ›é€ æ€§ï¼‰

    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    """
    # å°è¯•ä½¿ç”¨ deepseek
    if is_provider_configured('deepseek'):
        return get_llm_instance(provider='deepseek', streaming=True, temperature=0.5)

    # å›é€€åˆ° openai
    if is_provider_configured('openai'):
        return get_llm_instance(provider='openai', streaming=True, temperature=0.5)

    # æœ€åå°è¯•ä»»ä½•å¯ç”¨çš„
    return get_router_llm()


def get_aggregator_llm() -> ChatOpenAI:
    """
    è·å– Aggregator èŠ‚ç‚¹ä¸“ç”¨çš„ LLM å®ä¾‹

    Aggregator ç”¨äºæ€»ç»“å¤šä¸ªä¸“å®¶çš„è¾“å‡ºç»“æœï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€çš„æœ€ç»ˆå›å¤
    ä¼˜å…ˆä½¿ç”¨ DeepSeekï¼ˆæ€§ä»·æ¯”é«˜ï¼Œè¾“å‡ºè´¨é‡ç¨³å®šï¼‰
    é»˜è®¤æ¸©åº¦ä¸º 0.7ï¼ˆæ€»ç»“ä»»åŠ¡éœ€è¦ä¸€å®šçš„åˆ›é€ æ€§ï¼‰

    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    """
    # ä¼˜å…ˆä½¿ç”¨ deepseekï¼ˆæ€»ç»“ä»»åŠ¡æ€§ä»·æ¯”é«˜ï¼‰
    if is_provider_configured('deepseek'):
        return get_llm_instance(provider='deepseek', streaming=True, temperature=0.7)

    # å›é€€åˆ° openai
    if is_provider_configured('openai'):
        return get_llm_instance(provider='openai', streaming=True, temperature=0.7)

    # å°è¯• minimax
    if is_provider_configured('minimax'):
        return get_llm_instance(provider='minimax', streaming=True, temperature=0.7)

    # æœ€åå°è¯•ä»»ä½•å¯ç”¨çš„
    return get_router_llm()


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

def list_available_providers() -> list:
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ï¼ˆå·²é…ç½® API Keyï¼‰æä¾›å•†
    
    Returns:
        list: æä¾›å•†åç§°åˆ—è¡¨
    """
    from providers_config import get_active_providers
    return list(get_active_providers().keys())


def test_provider_connection(provider: str) -> bool:
    """
    æµ‹è¯•æŒ‡å®šæä¾›å•†çš„è¿æ¥æ˜¯å¦æ­£å¸¸
    
    Args:
        provider: æä¾›å•†æ ‡è¯†
    
    Returns:
        bool: è¿æ¥æ˜¯å¦æˆåŠŸ
    """
    try:
        if not is_provider_configured(provider):
            print(f"âŒ {provider}: æœªé…ç½® API Key")
            return False
        
        llm = get_llm_instance(provider=provider)
        # ç®€å•çš„è¿æ¥æµ‹è¯•ï¼ˆå‘é€ä¸€ä¸ªç®€å•è¯·æ±‚ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®ï¼Œä¸å®é™…è°ƒç”¨ API
        print(f"âœ… {provider}: é…ç½®æ­£ç¡®")
        return True
        
    except Exception as e:
        print(f"âŒ {provider}: {e}")
        return False


if __name__ == "__main__":
    # æµ‹è¯•æ‰€æœ‰æä¾›å•†è¿æ¥
    from providers_config import print_provider_status, get_all_providers
    
    print_provider_status()
    
    print("\næµ‹è¯•æä¾›å•†è¿æ¥...")
    for provider in get_all_providers().keys():
        test_provider_connection(provider)
