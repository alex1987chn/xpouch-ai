"""
LLM å·¥å‚æ¨¡å—

ç»Ÿä¸€ç®¡ç†å’Œåˆ›å»º LLM å®ä¾‹ï¼Œå®Œå…¨åŸºäºé…ç½®æ–‡ä»¶ï¼ˆproviders.yamlï¼‰
æ¶ˆé™¤ç¡¬ç¼–ç ï¼Œæ”¯æŒåŠ¨æ€æ·»åŠ æ–°æä¾›å•†

P2 ä¼˜åŒ–: æ·»åŠ  LLM å®ä¾‹ç¼“å­˜æ± ï¼Œå¤ç”¨å®ä¾‹å‡å°‘åˆ›å»ºå¼€é”€

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # è·å–æŒ‡å®šæä¾›å•†çš„ LLM
    llm = get_llm_instance(provider="minimax", streaming=True)
    
    # Router è‡ªåŠ¨é€‰æ‹©æœ€ä½³æä¾›å•†
    router_llm = get_router_llm()
"""

import os
from typing import Optional, Dict, Any
from langchain_openai import ChatOpenAI
from providers_config import (
    get_provider_config,
    get_provider_api_key,
    get_best_router_provider,
    is_provider_configured
)
import httpx

# ============================================================================
# P2 ä¼˜åŒ–: LLM å®ä¾‹ç¼“å­˜æ± 
# ============================================================================

# å…¨å±€ç¼“å­˜å­—å…¸: key -> ChatOpenAI instance
_llm_cache: Dict[str, ChatOpenAI] = {}
_cache_hits = 0
_cache_misses = 0

def _get_cache_key(provider: str, model: Optional[str], streaming: bool, temperature: Optional[float]) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    return f"{provider}:{model or 'default'}:{streaming}:{temperature or 'default'}"

def get_cached_llm(provider: str, model: Optional[str], streaming: bool, temperature: Optional[float]) -> Optional[ChatOpenAI]:
    """ä»ç¼“å­˜è·å– LLM å®ä¾‹"""
    global _cache_hits
    key = _get_cache_key(provider, model, streaming, temperature)
    if key in _llm_cache:
        _cache_hits += 1
        return _llm_cache[key]
    return None

def set_cached_llm(provider: str, model: Optional[str], streaming: bool, temperature: Optional[float], llm: ChatOpenAI) -> None:
    """ç¼“å­˜ LLM å®ä¾‹"""
    global _cache_misses
    key = _get_cache_key(provider, model, streaming, temperature)
    _llm_cache[key] = llm
    _cache_misses += 1

def get_llm_cache_stats() -> Dict[str, Any]:
    """è·å–ç¼“å­˜ç»Ÿè®¡"""
    total = _cache_hits + _cache_misses
    hit_rate = (_cache_hits / total * 100) if total > 0 else 0
    return {
        "cached_instances": len(_llm_cache),
        "cache_hits": _cache_hits,
        "cache_misses": _cache_misses,
        "hit_rate": f"{hit_rate:.1f}%"
    }

def clear_llm_cache() -> None:
    """æ¸…ç©º LLM ç¼“å­˜"""
    global _llm_cache, _cache_hits, _cache_misses
    _llm_cache.clear()
    _cache_hits = 0
    _cache_misses = 0
    print("[LLM Cache] ç¼“å­˜å·²æ¸…ç©º")


# ============================================================================
# æ¨¡å‹å…œåº•æœºåˆ¶ï¼ˆå…¼å®¹æ¥å£ï¼ŒåŸ model_fallback.py åŠŸèƒ½ï¼‰
# ============================================================================

def get_default_model() -> str:
    """
    è·å–é»˜è®¤æ¨¡å‹ï¼ˆæ¯æ¬¡éƒ½ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    
    å…¼å®¹åŸ model_fallback.py æ¥å£
    """
    return os.getenv("MODEL_NAME", "deepseek-chat")


def get_effective_model(configured_model: Optional[str]) -> str:
    """
    è·å–æœ‰æ•ˆçš„æ¨¡å‹åç§°ï¼ˆæ¨¡å‹å…œåº•æœºåˆ¶ï¼‰
    
    é€»è¾‘ï¼š
    1. å¦‚æœæœªé…ç½®æ¨¡å‹ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡ MODEL_NAME æˆ–é»˜è®¤å€¼
    2. å¦‚æœé…ç½®çš„æ˜¯ OpenAI æ¨¡å‹ï¼ˆgpt-å¼€å¤´ï¼‰ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸ºé»˜è®¤æ¨¡å‹
    3. æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡ FORCE_MODEL_FALLBACK=true å¼ºåˆ¶ä½¿ç”¨å…œåº•æ¨¡å‹
    4. è§£ææ¨¡å‹åˆ«åæ˜ å°„ï¼ˆé€šè¿‡ providers_configï¼‰
    
    Args:
        configured_model: æ•°æ®åº“ä¸­é…ç½®çš„æ¨¡å‹åç§°
    
    Returns:
        str: å®é™…åº”è¯¥ä½¿ç”¨çš„æ¨¡å‹åç§°
    """
    # æ¯æ¬¡éƒ½é‡æ–°è¯»å–é»˜è®¤æ¨¡å‹
    default_model = get_default_model()
    
    # å¼ºåˆ¶å…œåº•æ¨¡å¼
    if os.getenv("FORCE_MODEL_FALLBACK", "").lower() == "true":
        print(f"[ModelFallback] å¼ºåˆ¶å…œåº•æ¨¡å¼ï¼Œä½¿ç”¨ '{default_model}'")
        return default_model
    
    # æœªé…ç½®æ—¶ä½¿ç”¨é»˜è®¤
    if not configured_model:
        return default_model
    
    # è§£ææ¨¡å‹åˆ«åæ˜ å°„
    try:
        from providers_config import get_model_config
        model_config = get_model_config(configured_model)
        if model_config and 'model' in model_config:
            resolved_model = model_config['model']
            if resolved_model != configured_model:
                print(f"[ModelFallback] æ¨¡å‹åˆ«åè§£æ: '{configured_model}' -> '{resolved_model}'")
                configured_model = resolved_model
    except ImportError:
        print(f"[ModelFallback] è­¦å‘Š: æ— æ³•å¯¼å…¥ providers_configï¼Œè·³è¿‡æ¨¡å‹åˆ«åè§£æ")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¨¡å‹å…œåº•ï¼ˆOpenAI æ¨¡å‹åœ¨ç¬¬ä¸‰æ–¹ API ä¸Šå¯èƒ½ä¸å¯ç”¨ï¼‰
    openai_models = [
        "gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-4", "gpt-3.5-turbo",
        "gpt-4-vision-preview", "gpt-4-turbo-preview"
    ]
    
    # å¦‚æœä¸æ˜¯ OpenAI æ¨¡å‹ï¼Œä¸éœ€è¦å…œåº•
    if not any(configured_model.startswith(om) for om in openai_models):
        return configured_model
    
    # OpenAI æ¨¡å‹é»˜è®¤éœ€è¦å…œåº•ï¼Œé™¤éæ˜ç¡®é…ç½® ALLOW_OPENAI_MODELS=true
    if os.getenv("ALLOW_OPENAI_MODELS", "").lower() == "true":
        return configured_model
    
    # æ£€æŸ¥ API é…ç½®
    base_url = os.getenv("OPENAI_BASE_URL") or os.getenv("DEEPSEEK_BASE_URL", "")
    if "openai.com" in base_url and os.getenv("ALLOW_OPENAI_MODELS", "").lower() == "true":
        return configured_model
    
    # é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰ gpt- å¼€å¤´çš„æ¨¡å‹éƒ½éœ€è¦å…œåº•
    print(f"[ModelFallback] æ£€æµ‹åˆ° OpenAI æ¨¡å‹ '{configured_model}'ï¼Œåˆ‡æ¢ä¸º '{default_model}'")
    return default_model


def get_llm_instance(
    provider: str,
    model: Optional[str] = None,
    streaming: bool = False,
    temperature: Optional[float] = None,
) -> ChatOpenAI:
    """
    ç»Ÿä¸€çš„ LLM å·¥å‚å‡½æ•° - å®Œå…¨é…ç½®åŒ–
    
    P2 ä¼˜åŒ–: ä½¿ç”¨ç¼“å­˜æ± å¤ç”¨ LLM å®ä¾‹
    
    ä» providers.yaml è¯»å–é…ç½®ï¼Œä» .env è¯»å– API Key
    
    Args:
        provider: æä¾›å•†æ ‡è¯†ï¼ˆå¦‚ 'minimax', 'deepseek', 'openai'ï¼‰
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ default_model
        streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
    
    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    
    Raises:
        ValueError: å¦‚æœæä¾›å•†æœªé…ç½®æˆ–ç¼ºå°‘ API Key
    
    ç¤ºä¾‹ï¼š
        >>> llm = get_llm_instance("minimax", streaming=True, temperature=0.1)
        >>> llm = get_llm_instance("deepseek", model="deepseek-reasoner")
    """
    # P2 ä¼˜åŒ–: æ£€æŸ¥ç¼“å­˜
    cached = get_cached_llm(provider, model, streaming, temperature)
    if cached:
        print(f"[LLM Cache] å‘½ä¸­ç¼“å­˜: {provider}:{model or 'default'}")
        return cached
    
    # è¯»å–æä¾›å•†é…ç½®
    config = get_provider_config(provider)
    if not config:
        raise ValueError(
            f"æœªçŸ¥çš„æä¾›å•†: {provider}\n"
            f"è¯·åœ¨ providers.yaml ä¸­æ·»åŠ é…ç½®ï¼Œ"
            f"æˆ–æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®ã€‚"
        )
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨
    if not config.get('enabled', True):
        raise ValueError(
            f"æä¾›å•† {provider} å·²åœ¨é…ç½®ä¸­ç¦ç”¨ (enabled: false)"
        )
    
    # ä»ç¯å¢ƒå˜é‡è¯»å– API Key
    api_key = get_provider_api_key(provider)
    if not api_key:
        env_key = config.get('env_key', f'{provider.upper()}_API_KEY')
        raise ValueError(
            f"æœªé…ç½® {provider} çš„ API Key\n"
            f"è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: {env_key}=your-api-key"
        )
    
    # æ„å»º LLM é…ç½®
    llm_config = {
        'model': model or config.get('default_model'),
        'api_key': api_key,
        'base_url': config.get('base_url'),
        'streaming': streaming,
    }
    
    # æ¸©åº¦å‚æ•°
    if temperature is not None:
        llm_config['temperature'] = temperature
    elif 'temperature' in config:
        llm_config['temperature'] = config['temperature']
    else:
        llm_config['temperature'] = 0.7  # é»˜è®¤å€¼
    
    # ğŸš€ ä¿®å¤ï¼šåˆ›å»ºæ›´å¥å£®çš„ HTTP å®¢æˆ·ç«¯
    # ç¦ç”¨ HTTP/2 å¹¶å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œè§£å†³ "incomplete chunked read" é”™è¯¯
    http_client = httpx.Client(
        http2=False,      # ğŸš¨ å…³é”®ï¼šç¦ç”¨ HTTP/2ï¼Œè§£å†³å¤§éƒ¨åˆ† chunked read é”™è¯¯
        timeout=600.0,    # ğŸš¨ å…³é”®ï¼šç»™æ¨ç†æ¨¡å‹è¶³å¤Ÿçš„æ€è€ƒæ—¶é—´ï¼ˆ10 åˆ†é’Ÿï¼‰ï¼Œå¯¹é½ gunicorn/nginx
        verify=True        # éªŒè¯ SSL è¯ä¹¦ï¼ˆå®‰å…¨è€ƒè™‘ï¼‰
    )
    llm_config['http_client'] = http_client
    
    # åˆ›å»ºå®ä¾‹
    llm = ChatOpenAI(**llm_config)
    
    # P2 ä¼˜åŒ–: ç¼“å­˜å®ä¾‹
    set_cached_llm(provider, model, streaming, temperature, llm)
    print(f"[LLM Cache] åˆ›å»ºå¹¶ç¼“å­˜: {provider}:{model or 'default'}")
    
    return llm


def get_llm_by_model(model_id: str, streaming: bool = False) -> ChatOpenAI:
    """
    é€šè¿‡æ¨¡å‹ ID è·å– LLM å®ä¾‹
    
    Args:
        model_id: æ¨¡å‹æ ‡è¯†ï¼ˆå¦‚ 'minimax-2.1', 'gpt-4o'ï¼‰
        streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
    
    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    """
    from providers_config import get_model_config
    
    model_config = get_model_config(model_id)
    if not model_config:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ ID: {model_id}")
    
    provider = model_config.get('provider')
    model = model_config.get('model')
    
    return get_llm_instance(provider=provider, model=model, streaming=streaming)


def get_router_llm() -> ChatOpenAI:
    """
    è·å– Router èŠ‚ç‚¹ä¸“ç”¨çš„ LLM å®ä¾‹
    
    è‡ªåŠ¨é€‰æ‹©å·²é…ç½®ä¸”ä¼˜å…ˆçº§æœ€é«˜çš„æä¾›å•†ï¼ˆé»˜è®¤ä¼˜å…ˆ MiniMaxï¼‰
    ä½¿ç”¨è¾ƒä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
    
    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    
    Raises:
        ValueError: å¦‚æœæ²¡æœ‰å¯ç”¨çš„æä¾›å•†
    """
    # è·å–æœ€ä½³ Router æä¾›å•†
    provider = get_best_router_provider()
    
    if not provider:
        raise ValueError(
            "æ²¡æœ‰å¯ç”¨çš„ LLM æä¾›å•†ç”¨äº Router\n"
            "è¯·è‡³å°‘åœ¨ .env ä¸­é…ç½®ä¸€ä¸ªæä¾›å•†çš„ API Key\n"
            "æ”¯æŒçš„æä¾›å•†: minimax, deepseek, openai"
        )
    
    # è¯»å– Router é…ç½®ï¼ˆä» providers.yamlï¼‰
    from providers_config import get_router_config
    router_config = get_router_config()
    
    temperature = router_config.get('temperature', 0.1)
    streaming = router_config.get('streaming', False)  # ğŸ”¥ Router ä¸éœ€è¦æµå¼è¾“å‡º
    
    return get_llm_instance(
        provider=provider,
        streaming=streaming,
        temperature=temperature
    )





def get_expert_llm(
    provider: Optional[str] = None,
    model: Optional[str] = None,
    temperature: Optional[float] = None
) -> ChatOpenAI:
    """
    è·å– Expert èŠ‚ç‚¹ä¸“ç”¨çš„ LLM å®ä¾‹

    Args:
        provider: æŒ‡å®šæä¾›å•†ï¼Œé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå·²é…ç½®çš„
        model: æŒ‡å®šæ¨¡å‹
        temperature: ä¸“å®¶èŠ‚ç‚¹å¯è‡ªå®šä¹‰æ¸©åº¦

    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    """
    # å¦‚æœæŒ‡å®šäº†æä¾›å•†
    if provider:
        return get_llm_instance(
            provider=provider,
            model=model,
            streaming=True,
            temperature=temperature
        )

    # é»˜è®¤ä½¿ç”¨ deepseekï¼ˆæ€§ä»·æ¯”é«˜ï¼‰
    if is_provider_configured('deepseek'):
        return get_llm_instance(
            provider='deepseek',
            model=model,
            streaming=True,
            temperature=temperature or 0.7
        )

    # å›é€€åˆ°ä»»ä½•å¯ç”¨çš„
    return get_llm_instance(
        provider=get_best_router_provider(),
        model=model,
        streaming=True,
        temperature=temperature
    )


def get_commander_llm() -> ChatOpenAI:
    """
    è·å– Commander èŠ‚ç‚¹ä¸“ç”¨çš„ LLM å®ä¾‹

    Commander éœ€è¦è¾ƒå¼ºçš„è§„åˆ’èƒ½åŠ›ï¼Œä¼˜å…ˆä½¿ç”¨ DeepSeek æˆ– OpenAI
    é»˜è®¤æ¸©åº¦ä¸º 0.5ï¼ˆè§„åˆ’ä»»åŠ¡éœ€è¦ä¸€å®šçš„åˆ›é€ æ€§ï¼‰

    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    """
    # å°è¯•ä½¿ç”¨ deepseek
    if is_provider_configured('deepseek'):
        return get_llm_instance(provider='deepseek', streaming=True, temperature=0.5)

    # å›é€€åˆ° openai
    if is_provider_configured('openai'):
        return get_llm_instance(provider='openai', streaming=True, temperature=0.5)

    # æœ€åå°è¯•ä»»ä½•å¯ç”¨çš„
    return get_router_llm()


def get_aggregator_llm() -> ChatOpenAI:
    """
    è·å– Aggregator èŠ‚ç‚¹ä¸“ç”¨çš„ LLM å®ä¾‹

    Aggregator ç”¨äºæ€»ç»“å¤šä¸ªä¸“å®¶çš„è¾“å‡ºç»“æœï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€çš„æœ€ç»ˆå›å¤
    ä¼˜å…ˆä½¿ç”¨ DeepSeekï¼ˆæ€§ä»·æ¯”é«˜ï¼Œè¾“å‡ºè´¨é‡ç¨³å®šï¼‰
    é»˜è®¤æ¸©åº¦ä¸º 0.7ï¼ˆæ€»ç»“ä»»åŠ¡éœ€è¦ä¸€å®šçš„åˆ›é€ æ€§ï¼‰

    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    """
    # ä¼˜å…ˆä½¿ç”¨ deepseekï¼ˆæ€»ç»“ä»»åŠ¡æ€§ä»·æ¯”é«˜ï¼‰
    if is_provider_configured('deepseek'):
        return get_llm_instance(provider='deepseek', streaming=True, temperature=0.7)

    # å›é€€åˆ° openai
    if is_provider_configured('openai'):
        return get_llm_instance(provider='openai', streaming=True, temperature=0.7)

    # å°è¯• minimax
    if is_provider_configured('minimax'):
        return get_llm_instance(provider='minimax', streaming=True, temperature=0.7)

    # æœ€åå°è¯•ä»»ä½•å¯ç”¨çš„
    return get_router_llm()


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

def list_available_providers() -> list:
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ï¼ˆå·²é…ç½® API Keyï¼‰æä¾›å•†
    
    Returns:
        list: æä¾›å•†åç§°åˆ—è¡¨
    """
    from providers_config import get_active_providers
    return list(get_active_providers().keys())


def test_provider_connection(provider: str) -> bool:
    """
    æµ‹è¯•æŒ‡å®šæä¾›å•†çš„è¿æ¥æ˜¯å¦æ­£å¸¸
    
    Args:
        provider: æä¾›å•†æ ‡è¯†
    
    Returns:
        bool: è¿æ¥æ˜¯å¦æˆåŠŸ
    """
    try:
        if not is_provider_configured(provider):
            print(f"âŒ {provider}: æœªé…ç½® API Key")
            return False
        
        llm = get_llm_instance(provider=provider)
        # ç®€å•çš„è¿æ¥æµ‹è¯•ï¼ˆå‘é€ä¸€ä¸ªç®€å•è¯·æ±‚ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®ï¼Œä¸å®é™…è°ƒç”¨ API
        print(f"âœ… {provider}: é…ç½®æ­£ç¡®")
        return True
        
    except Exception as e:
        print(f"âŒ {provider}: {e}")
        return False


if __name__ == "__main__":
    # æµ‹è¯•æ‰€æœ‰æä¾›å•†è¿æ¥
    from providers_config import print_provider_status, get_all_providers
    
    print_provider_status()
    
    print("\næµ‹è¯•æä¾›å•†è¿æ¥...")
    for provider in get_all_providers().keys():
        test_provider_connection(provider)
