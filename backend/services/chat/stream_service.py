"""
SSE æµå¼è¾“å‡ºæ ¸å¿ƒæœåŠ¡

èŒè´£:
- è‡ªå®šä¹‰æ™ºèƒ½ä½“æµå¼/éæµå¼å¤„ç†
- LangGraph å¤æ‚æ¨¡å¼æµå¼/éæµå¼å¤„ç†
- SSE äº‹ä»¶ç”Ÿæˆå’Œè½¬æ¢
- å¿ƒè·³ä¿æ´»æœºåˆ¶

ä¾èµ–:
- backend.services.chat.session_service (æ¶ˆæ¯ä¿å­˜)
- backend.utils.event_generator (SSEäº‹ä»¶ç”Ÿæˆ)
- backend.utils.thinking_parser (Thinkæ ‡ç­¾è§£æ)

æ³¨æ„:
- LangGraph å¯¼å…¥åœ¨æ–¹æ³•å†…éƒ¨è¿›è¡Œï¼Œé˜²æ­¢å¾ªç¯å¼•ç”¨
"""
import os
import asyncio
import uuid
from typing import List, Dict, Optional, AsyncGenerator, Any, Callable
from datetime import datetime
from fastapi.responses import StreamingResponse
from sqlmodel import Session
from langchain_core.messages import BaseMessage

from models import CustomAgent, Thread
from utils.llm_factory import get_llm_instance
from utils.exceptions import AppError
from utils.logger import logger
from providers_config import get_model_config, get_provider_config, get_provider_api_key
from config import HEARTBEAT_INTERVAL, FORCE_HEARTBEAT_INTERVAL, STREAM_TIMEOUT


class StreamService:
    """æµå¼å¤„ç†æœåŠ¡"""
    
    def __init__(self, db_session: Session):
        self.db = db_session
        # å»¶è¿Ÿåˆå§‹åŒ– session_serviceï¼Œé¿å…å¾ªç¯ä¾èµ–é—®é¢˜
        self._session_service = None
    
    @property
    def session_service(self):
        """å»¶è¿Ÿåˆå§‹åŒ– ChatSessionService"""
        if self._session_service is None:
            from .session_service import ChatSessionService
            self._session_service = ChatSessionService(self.db)
        return self._session_service
    
    # ============================================================================
    # è‡ªå®šä¹‰æ™ºèƒ½ä½“æµå¼å¤„ç†
    # ============================================================================
    
    async def handle_custom_agent_stream(
        self,
        custom_agent: CustomAgent,
        messages: List[BaseMessage],
        thread_id: str,
        thread: Thread,
        message_id: Optional[str] = None
    ) -> StreamingResponse:
        """
        è‡ªå®šä¹‰æ™ºèƒ½ä½“æµå¼å“åº”å¤„ç†
        
        Args:
            custom_agent: è‡ªå®šä¹‰æ™ºèƒ½ä½“é…ç½®
            messages: LangChain æ¶ˆæ¯åˆ—è¡¨
            thread_id: çº¿ç¨‹ID
            thread: çº¿ç¨‹å®ä¾‹
            message_id: å‰ç«¯ä¼ å…¥çš„æ¶ˆæ¯ID
            
        Returns:
            StreamingResponse SSEæµ
        """
        async def event_generator():
            full_response = ""
            actual_message_id = message_id or str(uuid.uuid4())
            
            # å¿ƒè·³é…ç½® - ä» config å¯¼å…¥
            last_heartbeat_time = datetime.now()
            
            try:
                # æ„å»º LLM
                llm = await self._build_custom_agent_llm(custom_agent)
                
                # æ£€ç´¢é•¿æœŸè®°å¿†
                messages_with_system = await self._inject_memories(
                    custom_agent, messages, thread.user_id
                )
                
                # è·å–æµè¿­ä»£å™¨
                iterator = llm.astream(messages_with_system)
                
                async def get_next_chunk():
                    try:
                        return await asyncio.wait_for(
                            iterator.__anext__(),
                            timeout=HEARTBEAT_INTERVAL
                        )
                    except StopAsyncIteration:
                        return None
                
                while True:
                    try:
                        chunk = await get_next_chunk()
                        if chunk is None:
                            break
                        
                        content = chunk.content
                        if content:
                            full_response += content
                            yield self._build_message_delta_event(
                                actual_message_id, content
                            )
                    
                    except asyncio.TimeoutError:
                        # å¿ƒè·³ä¿æ´»
                        yield ": keep-alive\n\n"
                        last_heartbeat_time = datetime.now()
                        continue
                    
                    # å¼ºåˆ¶å¿ƒè·³
                    current_time = datetime.now()
                    time_since_last = (current_time - last_heartbeat_time).total_seconds()
                    if time_since_last >= FORCE_HEARTBEAT_INTERVAL:
                        yield ": keep-alive\n\n"
                        last_heartbeat_time = current_time
                
            except Exception as e:
                yield self._build_error_event("STREAM_ERROR", str(e))
            
            # è§£æ thinking å¹¶ä¿å­˜æ¶ˆæ¯
            from utils.thinking_parser import parse_thinking
            clean_content, thinking_data = parse_thinking(full_response)
            
            # ä½¿ç”¨ session_service ä¿å­˜æ¶ˆæ¯
            await self.session_service.save_assistant_message(
                thread_id=thread_id,
                content=full_response,
                thinking_data=thinking_data,
                message_id=actual_message_id
            )
            
            # å‘é€å®Œæˆäº‹ä»¶
            yield self._build_message_done_event(actual_message_id, full_response)
        
        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
    
    async def handle_custom_agent_sync(
        self,
        custom_agent: CustomAgent,
        messages: List[BaseMessage],
        thread_id: str,
        thread: Thread,
        message_id: Optional[str] = None
    ) -> dict:
        """
        è‡ªå®šä¹‰æ™ºèƒ½ä½“éæµå¼å¤„ç†ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
        
        å®é™…å†…éƒ¨ä½¿ç”¨æµå¼è·å–ç»“æœï¼Œä½†è¿”å›å®Œæ•´å“åº”
        """
        full_response = ""
        actual_message_id = message_id or str(uuid.uuid4())
        
        try:
            llm = await self._build_custom_agent_llm(custom_agent)
            messages_with_system = await self._inject_memories(
                custom_agent, messages, thread.user_id
            )
            
            # æµå¼è·å–å®Œæ•´å“åº”
            async for chunk in llm.astream(messages_with_system):
                if chunk.content:
                    full_response += chunk.content
        
        except Exception as e:
            raise AppError(f"è‡ªå®šä¹‰æ™ºèƒ½ä½“è°ƒç”¨å¤±è´¥: {str(e)}")
        
        # è§£æ thinking å¹¶ä¿å­˜
        from utils.thinking_parser import parse_thinking
        clean_content, thinking_data = parse_thinking(full_response)
        
        await self.session_service.save_assistant_message(
            thread_id=thread_id,
            content=full_response,
            thinking_data=thinking_data,
            message_id=actual_message_id
        )
        
        return {
            "role": "assistant",
            "content": full_response,
            "conversationId": thread_id
        }
    
    async def _build_custom_agent_llm(self, custom_agent: CustomAgent):
        """æ„å»ºè‡ªå®šä¹‰æ™ºèƒ½ä½“çš„ LLM å®ä¾‹"""
        model_id = custom_agent.model_id or "deepseek-chat"
        model_config = get_model_config(model_id)
        
        if model_config:
            provider = model_config.get('provider')
            actual_model = model_config.get('model', model_id)
            provider_config = get_provider_config(provider)
            
            if not provider_config:
                raise ValueError(f"æä¾›å•† {provider} æœªé…ç½®")
            
            if not get_provider_api_key(provider):
                raise ValueError(f"æä¾›å•† {provider} çš„ API Key æœªè®¾ç½®")
            
            temperature = model_config.get('temperature', 0.7)
            
            return get_llm_instance(
                provider=provider,
                model=actual_model,
                streaming=True,
                temperature=temperature
            )
        else:
            # Fallback
            return get_llm_instance(streaming=True, model=model_id, temperature=0.7)
    
    async def _inject_memories(
        self,
        custom_agent: CustomAgent,
        messages: List[BaseMessage],
        user_id: str
    ) -> List:
        """æ³¨å…¥é•¿æœŸè®°å¿†åˆ° system prompt"""
        from services.memory_manager import memory_manager
        
        user_query = messages[-1].content if messages else ""
        relevant_memories = await memory_manager.search_relevant_memories(
            user_id, user_query, limit=5
        )
        
        system_prompt = custom_agent.system_prompt
        if relevant_memories:
            system_prompt += f"\n\nã€å…³äºç”¨æˆ·çš„å·²çŸ¥ä¿¡æ¯ã€‘:\n{relevant_memories}\n(è¯·åœ¨å›ç­”æ—¶è‡ªç„¶åœ°åˆ©ç”¨è¿™äº›ä¿¡æ¯)"
        
        result = [("system", system_prompt)]
        result.extend(messages)
        return result
    
    # ============================================================================
    # LangGraph å¤æ‚æ¨¡å¼æµå¼å¤„ç†
    # ============================================================================
    
    async def handle_langgraph_stream(
        self,
        initial_state: dict,
        thread_id: str,
        thread: Thread,
        user_message: str,
        message_id: Optional[str] = None
    ) -> StreamingResponse:
        """
        LangGraph å¤æ‚æ¨¡å¼æµå¼å¤„ç†
        
        Args:
            initial_state: LangGraph åˆå§‹çŠ¶æ€
            thread_id: çº¿ç¨‹ID
            thread: çº¿ç¨‹å®ä¾‹
            user_message: ç”¨æˆ·æ¶ˆæ¯
            message_id: å‰ç«¯ä¼ å…¥çš„æ¶ˆæ¯ID
            
        Returns:
            StreamingResponse SSEæµ
        """
        # åœ¨æ–¹æ³•å†…éƒ¨å¯¼å…¥ LangGraphï¼Œé˜²æ­¢å¾ªç¯å¼•ç”¨
        from agents.graph import create_smart_router_workflow
        from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
        from utils.db import get_db_connection
        from crud.task_session import (
            create_artifacts_batch, update_subtask_status
        )
        
        async def event_generator():
            actual_message_id = message_id or str(uuid.uuid4())
            full_response = ""
            router_decision = "simple"
            
            # æ”¶é›†ä»»åŠ¡åˆ—è¡¨å’Œäº§ç‰©
            collected_task_list = []
            expert_artifacts = {}
            
            async with get_db_connection() as conn:
                checkpointer = AsyncPostgresSaver(conn)
                graph = create_smart_router_workflow(checkpointer=checkpointer)
                
                stream_queue = asyncio.Queue()
                
                config = {
                    "recursion_limit": 100,
                    "configurable": {
                        "thread_id": thread_id,
                        "stream_queue": stream_queue
                    }
                }
                
                # æ³¨å…¥åˆå§‹çŠ¶æ€
                await graph.aupdate_state(config, initial_state)
                
                try:
                    async for token in graph.astream_events(None, config, version="v2"):
                        # ğŸ”¥ ä¿®å¤ï¼šè·³è¿‡éå­—å…¸ç±»å‹çš„ token
                        if not isinstance(token, dict):
                            continue
                        
                        event_type = token.get("event", "")
                        name = token.get("name", "")
                        data = token.get("data", {}) or {}
                        output = data.get("output", {}) or {}
                        
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¤„ç† event_queue ä¸­çš„å¤šä¸ªäº‹ä»¶ï¼ˆåŒ…æ‹¬ router.start å’Œ router.decisionï¼‰
                        if event_type == "on_chain_end" and output and isinstance(output, dict):
                            event_queue = output.get("event_queue", [])
                            for queued_event in event_queue:
                                if queued_event.get("type") == "sse":
                                    yield queued_event["event"]
                        
                        # å¤„ç†å…¶ä»–äº‹ä»¶ï¼ˆæ¶ˆæ¯æµã€task äº‹ä»¶ç­‰ï¼‰
                        event_str = self.transform_langgraph_event(token, actual_message_id)
                        if event_str:
                            yield event_str
                        
                        # æ”¶é›†ä»»åŠ¡æ‰§è¡Œç»“æœ
                        self._collect_execution_results(
                            token, collected_task_list, expert_artifacts
                        )
                        
                        # æ£€æµ‹ router_decision
                        if event_type == "on_chain_end" and name == "router":
                            if output and isinstance(output, dict) and output.get("router_decision"):
                                router_decision = output["router_decision"]
                                # æ›´æ–°çº¿ç¨‹æ¨¡å¼
                                await self._update_thread_mode(thread_id, router_decision)
                
                except Exception as e:
                    import traceback
                    logger.error(f"[StreamService] æµå¼å¤„ç†å¼‚å¸¸: {e}")
                    traceback.print_exc()
                    yield self._build_error_event("GRAPH_ERROR", str(e))
                
                # ğŸ”¥ğŸ”¥ğŸ”¥ HITL æ£€æµ‹ï¼šæ£€æŸ¥æ˜¯å¦å¤„äº interrupt çŠ¶æ€
                # è·å–å½“å‰çŠ¶æ€ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡ï¼ˆè¢« interrupt æš‚åœï¼‰
                final_state = await graph.aget_state(config)
                state_values = final_state.values if final_state else {}
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä»»åŠ¡åˆ—è¡¨ä½†æœªå®Œæˆï¼ˆè¯´æ˜è¢« interrupt æš‚åœï¼‰
                task_list = state_values.get("task_list", [])
                current_task_index = state_values.get("current_task_index", 0)
                
                # å¦‚æœå­˜åœ¨ä»»åŠ¡åˆ—è¡¨ä¸”å½“å‰ä»»åŠ¡ç´¢å¼•ä¸º0ï¼ˆæœªå¼€å§‹æ‰§è¡Œï¼‰ï¼Œè¯´æ˜è¢« HITL ä¸­æ–­
                if task_list and current_task_index == 0 and len(collected_task_list) == 0:
                    logger.info(f"[StreamService] HITL ä¸­æ–­æ£€æµ‹ï¼šä»»åŠ¡è§„åˆ’å®Œæˆï¼Œç­‰å¾…ç”¨æˆ·å®¡æ ¸")
                    
                    # æ„å»ºå½“å‰è®¡åˆ’æ•°æ®
                    current_plan = [
                        {
                            "id": task.get("id", f"task-{i}"),
                            "expert_type": task.get("expert_type", "generic"),
                            "description": task.get("description", ""),
                            "sort_order": i,
                            "status": "pending",
                            "depends_on": task.get("depends_on") or []  # ğŸ”¥ å…³é”®ï¼šä¼ é€’ä¾èµ–å…³ç³»åˆ°å‰ç«¯
                        }
                        for i, task in enumerate(task_list)
                    ]
                    
                    # å‘é€ human.interrupt äº‹ä»¶
                    yield self._build_human_interrupt_event(thread_id, current_plan)
                    return  # ç»“æŸæµï¼Œç­‰å¾…ç”¨æˆ·é€šè¿‡ /chat/resume æ¢å¤
                
                # æ­£å¸¸æµç¨‹ï¼šè·å–æœ€ç»ˆç»“æœ
                last_message = state_values.get("messages", [])[-1] if state_values.get("messages") else None
                
                if last_message:
                    full_response = last_message.content
                    
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    await self._save_langgraph_result(
                        thread_id=thread_id,
                        thread=thread,
                        user_message=user_message,
                        last_message=last_message,
                        router_decision=router_decision,
                        task_list=collected_task_list,
                        expert_artifacts=expert_artifacts,
                        message_id=actual_message_id
                    )
                
                # ğŸ”¥ ä¿®å¤ï¼šåªæœ‰ç®€å•æ¨¡å¼æ‰åœ¨è¿™é‡Œå‘é€ message.done
                # å¤æ‚æ¨¡å¼ç”± aggregator é€šè¿‡ event_queue å‘é€
                if router_decision == "simple":
                    yield self._build_message_done_event(actual_message_id, full_response)
                # å¤æ‚æ¨¡å¼ï¼šmessage.done å·²ç”± aggregator é€šè¿‡ event_queue å‘é€
        
        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
    
    async def handle_langgraph_sync(
        self,
        initial_state: dict,
        thread_id: str,
        thread: Thread,
        user_message: str
    ) -> dict:
        """LangGraph éæµå¼å¤„ç†ï¼ˆå†…éƒ¨ä½¿ç”¨æµå¼ï¼‰"""
        # éæµå¼ä¹Ÿä½¿ç”¨æµå¼è·å–ï¼Œä½†è¿”å›å®Œæ•´ç»“æœ
        full_response = ""
        
        # åœ¨æ–¹æ³•å†…éƒ¨å¯¼å…¥
        from agents.graph import create_smart_router_workflow
        from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
        from utils.db import get_db_connection
        
        async with get_db_connection() as conn:
            checkpointer = AsyncPostgresSaver(conn)
            graph = create_smart_router_workflow(checkpointer=checkpointer)
            
            config = {
                "recursion_limit": 100,
                "configurable": {"thread_id": thread_id}
            }
            
            await graph.aupdate_state(config, initial_state)
            
            # æ‰§è¡Œ
            result = await graph.ainvoke(None, config)
            
            last_message = result.get("messages", [])[-1] if result.get("messages") else None
            router_decision = result.get("router_decision", "simple")
            
            if last_message:
                full_response = last_message.content
                await self._save_langgraph_result(
                    thread_id=thread_id,
                    thread=thread,
                    user_message=user_message,
                    last_message=last_message,
                    router_decision=router_decision,
                    task_list=result.get("task_list", []),
                    expert_artifacts={},
                    message_id=str(uuid.uuid4())
                )
        
        return {
            "role": "assistant",
            "content": full_response,
            "conversationId": thread_id,
            "threadMode": router_decision
        }
    
    async def _save_langgraph_result(
        self,
        thread_id: str,
        thread: Thread,
        user_message: str,
        last_message: Any,
        router_decision: str,
        task_list: List[dict],
        expert_artifacts: dict,
        message_id: str
    ):
        """ä¿å­˜ LangGraph æ‰§è¡Œç»“æœ"""
        from crud.task_session import create_artifacts_batch
        from models import TaskSession, SubTask
        
        # å¤æ‚æ¨¡å¼ï¼šåˆ›å»º TaskSession å’Œ SubTasks
        if router_decision == "complex":
            await self.session_service.update_thread_agent_type(
                thread_id, "ai"
            )
            
            task_session = TaskSession(
                session_id=str(uuid.uuid4()),
                thread_id=thread_id,
                user_query=user_message,
                status="completed",
                final_response=last_message.content,
                created_at=datetime.now(),
                updated_at=datetime.now(),
                completed_at=datetime.now()
            )
            self.db.add(task_session)
            self.db.flush()
            
            # æ›´æ–° thread
            thread.task_session_id = task_session.session_id
            self.db.add(thread)
            
            # ä¿å­˜ SubTasks
            for subtask in task_list:
                # âŒ ç§»é™¤é”™è¯¯çš„ artifacts èµ‹å€¼ï¼ˆartifacts æ˜¯å…³ç³»å­—æ®µï¼‰
                db_subtask = SubTask(
                    id=subtask["id"],
                    expert_type=subtask["expert_type"],
                    task_description=subtask["description"],
                    input_data=subtask.get("input_data", {}),
                    status=subtask.get("status", "completed"),
                    output_result=subtask.get("output_result"),
                    started_at=subtask.get("started_at"),
                    completed_at=subtask.get("completed_at"),
                    created_at=datetime.now(),
                    updated_at=datetime.now(),
                    task_session_id=task_session.session_id
                )
                self.db.add(db_subtask)
                self.db.flush()
                
                # ğŸ”¥ ä¿å­˜ artifactsï¼ˆä½¿ç”¨ task_id åŒ¹é…ï¼‰
                task_id = subtask.get("id")
                logger.info(f"[StreamService] å°è¯•ä¿å­˜ artifacts: task_id={task_id}, expert_artifacts keys={list(expert_artifacts.keys())}")
                
                if task_id and task_id in expert_artifacts:
                    try:
                        logger.info(f"[StreamService] æ‰¾åˆ° artifacts: {len(expert_artifacts[task_id])} ä¸ª")
                        create_artifacts_batch(self.db, db_subtask.id, expert_artifacts[task_id])
                        logger.info(f"[StreamService] âœ… artifacts ä¿å­˜æˆåŠŸ")
                    except Exception as e:
                        logger.error(f"[StreamService] ä¿å­˜ artifacts å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                else:
                    logger.warning(f"[StreamService] âš ï¸ task_id={task_id} åœ¨ expert_artifacts ä¸­æœªæ‰¾åˆ°")
        
        # ä¿å­˜ AI æ¶ˆæ¯
        await self.session_service.save_assistant_message(
            thread_id=thread_id,
            content=last_message.content,
            message_id=message_id
        )
    
    async def _update_thread_mode(self, thread_id: str, mode: str):
        """æ›´æ–°çº¿ç¨‹æ¨¡å¼"""
        thread = self.db.get(Thread, thread_id)
        if thread:
            thread.thread_mode = mode
            self.db.add(thread)
            self.db.commit()
    
    def _collect_execution_results(
        self,
        token,
        task_list: List[dict],
        expert_artifacts: dict
    ):
        """æ”¶é›† LangGraph æ‰§è¡Œç»“æœ"""
        # ğŸ”¥ ä¿®å¤ï¼šè·³è¿‡éå­—å…¸ç±»å‹çš„ token
        if not isinstance(token, dict):
            return
            
        event = token.get("event", "")
        data = token.get("data", {}) or {}
        
        if event == "on_chain_end":
            output = data.get("output", {}) or {}
            if output and isinstance(output, dict) and output.get("__expert_info"):
                # æ”¶é›†ä»»åŠ¡ç»“æœ
                task_result = output.get("__expert_info", {})
                task_list.append({
                    "id": task_result.get("task_id"),
                    "expert_type": task_result.get("expert_type"),
                    "status": task_result.get("status"),
                    "description": output.get("description", ""),
                    "output_result": output.get("output_result"),
                    "input_data": output.get("input_data", {}),
                    "started_at": output.get("started_at"),
                    "completed_at": output.get("completed_at"),
                    "artifact": output.get("artifact")
                })
                
                # æ”¶é›† artifacts
                task_id = task_result.get("task_id")
                artifact_data = output.get("artifact")
                logger.info(f"[_collect_execution_results] æ”¶é›† artifacts: task_id={task_id}, has_artifact={artifact_data is not None}")
                if task_id and artifact_data:
                    if task_id not in expert_artifacts:
                        expert_artifacts[task_id] = []
                    expert_artifacts[task_id].append(artifact_data)
                    logger.info(f"[_collect_execution_results] âœ… artifacts å·²æ”¶é›†: task_id={task_id}, count={len(expert_artifacts[task_id])}")
    
    # ============================================================================
    # å…¬å…±æµå¼æ–¹æ³•ï¼ˆä¾› RecoveryService å¤ç”¨ï¼‰
    # ============================================================================
    
    async def execute_langgraph_stream(
        self,
        thread_id: str,
        stream_queue: asyncio.Queue,
        sse_queue: asyncio.Queue,
        realtime_queue: asyncio.Queue,
        updated_plan: Optional[List[dict]] = None,
        message_id: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        æ‰§è¡Œ LangGraph æµå¼å¤„ç†ï¼ˆä¾› RecoveryService å¤ç”¨ï¼‰
        
        è¿™æ˜¯æ ¸å¿ƒçš„æµå¼æ‰§è¡Œé€»è¾‘ï¼ŒRecoveryService åœ¨æ¸…ç†çŠ¶æ€åè°ƒç”¨æ­¤æ–¹æ³•
        
        Args:
            thread_id: çº¿ç¨‹ID
            stream_queue: æµå¼é˜Ÿåˆ—
            sse_queue: SSE äº‹ä»¶é˜Ÿåˆ—
            realtime_queue: å®æ—¶æ¨é€é˜Ÿåˆ—
            updated_plan: ç”¨æˆ·ä¿®æ”¹åçš„è®¡åˆ’ï¼ˆå¯é€‰ï¼‰
            message_id: å‰ç«¯ä¼ å…¥çš„æ¶ˆæ¯IDï¼ˆç”¨äºå…³è”æµå¼è¾“å‡ºï¼‰
            
        Yields:
            SSE äº‹ä»¶å­—ç¬¦ä¸²
        """
        # åœ¨æ–¹æ³•å†…éƒ¨å¯¼å…¥ï¼Œé˜²æ­¢å¾ªç¯å¼•ç”¨
        from agents.graph import create_smart_router_workflow
        from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
        from utils.db import get_db_connection
        from crud.task_session import create_artifacts_batch
        
        async with get_db_connection() as conn:
            checkpointer = AsyncPostgresSaver(conn)
            graph = create_smart_router_workflow(checkpointer=checkpointer)
            
            config = {
                "recursion_limit": 100,
                "configurable": {
                    "thread_id": thread_id,
                    "stream_queue": realtime_queue
                }
            }
            
            # å¦‚æœæä¾›äº†æ›´æ–°åçš„è®¡åˆ’ï¼Œåº”ç”¨å®ƒ
            if updated_plan:
                await self._apply_updated_plan(graph, config, updated_plan)
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¤–å±‚å¾ªç¯é©±åŠ¨ä»»åŠ¡æ‰§è¡Œç›´åˆ°å®Œæˆ
            # LangGraph çš„ astream_events åœ¨ç¬¬ä¸€ä¸ªå¾ªç¯ç»“æŸåå°±è¿”å›ï¼Œä¸ä¼šè‡ªåŠ¨ç»§ç»­
            # éœ€è¦æ‰‹åŠ¨æ£€æŸ¥çŠ¶æ€å¹¶é©±åŠ¨åç»­ä»»åŠ¡æ‰§è¡Œ
            async def producer():
                try:
                    loop_count = 0
                    max_loops = 50  # é˜²æ­¢æ— é™å¾ªç¯
                    aggregator_executed = False  # ğŸ”¥ æ ‡è®° aggregator æ˜¯å¦å·²æ‰§è¡Œ

                    while loop_count < max_loops:
                        loop_count += 1

                        # è·å–å½“å‰çŠ¶æ€
                        current_state = await graph.aget_state(config)
                        task_list = current_state.values.get("task_list", [])
                        current_index = current_state.values.get("current_task_index", 0)
                        next_node = current_state.values.get("next_node", "")

                        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆäº†ï¼Œæˆ–è€… aggregator å·²ç»æ‰§è¡Œè¿‡
                        if current_index >= len(task_list) or aggregator_executed:
                            break

                        # æ‰§è¡Œä¸€è½® LangGraph
                        async for token in graph.astream_events(None, config, version="v2"):
                            # ğŸ”¥ ä¿®å¤ï¼štoken å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œè·³è¿‡éå­—å…¸ç±»å‹
                            if not isinstance(token, dict):
                                continue

                            event_type = token.get("event", "")
                            metadata = token.get("metadata", {})
                            name = metadata.get("name", "")
                            
                            # ğŸ”¥ æ£€æµ‹ aggregator èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
                            if event_type == "on_chain_start" and name == "aggregator":
                                aggregator_executed = True
                                logger.info(f"[Producer] æ£€æµ‹åˆ° aggregator å¼€å§‹æ‰§è¡Œ (loop {loop_count})")

                            # å¤„ç† event_queue ä¸­çš„äº‹ä»¶ï¼ˆartifact.start/chunk/completed ç­‰ï¼‰
                            if event_type == "on_chain_end":
                                data = token.get("data", {}) or {}
                                output = data.get("output", {}) or {}
                                if output and isinstance(output, dict):
                                    event_queue = output.get("event_queue", [])
                                    for queued_event in event_queue:
                                        if queued_event.get("type") == "sse":
                                            await sse_queue.put({
                                                "type": "sse",
                                                "event": event_str
                                            })
                                    
                                    # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æµ‹ aggregator æ‰§è¡Œå®Œæˆ
                                    # å¦‚æœ aggregator èŠ‚ç‚¹å·²å®Œæˆä¸”æœ‰è¾“å‡ºï¼Œæ ‡è®°ä¸ºå·²æ‰§è¡Œå¹¶è·³å‡º
                                    if name == "aggregator" and output.get("final_response"):
                                        aggregator_executed = True
                                        logger.info(f"[Producer] aggregator æ‰§è¡Œå®Œæˆï¼Œå‡†å¤‡é€€å‡º (loop {loop_count})")
                                        # å‘é€å®Œå½“å‰äº‹ä»¶åç«‹å³é€€å‡ºå†…å±‚å¾ªç¯
                                        break

                            event_str = self.transform_langgraph_event(token, message_id)
                            if event_str:
                                await sse_queue.put({
                                    "type": "sse",
                                    "event": event_str
                                })
                                
                                # ğŸ”¥ å¦‚æœå‘é€äº† message.done äº‹ä»¶ï¼Œè¯´æ˜ aggregator å·²å®Œæˆ
                                if "message.done" in event_str:
                                    logger.info(f"[Producer] å·²å‘é€ message.doneï¼Œæ ‡è®° aggregator å®Œæˆ")
                                    aggregator_executed = True

                            # æ”¶é›† artifacts
                            data = token.get("data", {}) or {}
                            output = data.get("output", {}) or {}
                            if output and isinstance(output, dict) and output.get("artifact"):
                                await stream_queue.put({
                                    "type": "artifact",
                                    "data": output["artifact"]
                                })
                        
                        # ğŸ”¥ å¦‚æœ aggregator å·²æ‰§è¡Œï¼Œé€€å‡ºå¤–å±‚å¾ªç¯
                        if aggregator_executed:
                            logger.info(f"[Producer] aggregator å·²å®Œæˆï¼Œé€€å‡ºå¤–å±‚å¾ªç¯")
                            break

                        # çŸ­æš‚ç­‰å¾…ï¼Œè®©çŠ¶æ€æ›´æ–°
                        await asyncio.sleep(0.1)

                except Exception as e:
                    import traceback
                    logger.error(f"[StreamService] Producer é”™è¯¯: {e}")
                    traceback.print_exc()
                finally:
                    await sse_queue.put({"type": "done"})
            
            # å¯åŠ¨ç”Ÿäº§è€…
            producer_task = asyncio.create_task(producer())
            
            # æ¶ˆè´¹å¹¶ yield äº‹ä»¶
            while True:
                try:
                    item = await asyncio.wait_for(sse_queue.get(), timeout=STREAM_TIMEOUT)
                    if item.get("type") == "done":
                        break
                    if item.get("type") == "sse":
                        yield item["event"]
                except asyncio.TimeoutError:
                    yield ": keep-alive\n\n"
            
            await producer_task
            # message.done ç”± aggregator_node é€šè¿‡ event_queue å‘é€
            # è¿™é‡Œä¸å†é‡å¤å‘é€
    
    async def _apply_updated_plan(
        self,
        graph,
        config: dict,
        updated_plan: List[dict]
    ):
        """
        åº”ç”¨ç”¨æˆ·æ›´æ–°åçš„è®¡åˆ’
        
        ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¿…é¡»æ·»åŠ  HumanMessage æ¥è§¦å‘ Graph ç»§ç»­æ‰§è¡Œï¼Œ
        å¦åˆ™ LangGraph ä¼šè®¤ä¸ºæ²¡æœ‰æ–°è¾“å…¥è€Œè¿›å…¥ END èŠ‚ç‚¹ã€‚
        """
        from langchain_core.messages import HumanMessage
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šåˆå¹¶çŠ¶æ€ï¼Œä¸è¦å®Œå…¨æ›¿æ¢
        # è·å–å½“å‰çŠ¶æ€
        current_state = await graph.aget_state(config)
        current_values = current_state.values
        current_task_list = current_values.get("task_list", [])
        current_expert_results = current_values.get("expert_results", [])
        
        # åˆ›å»ºä»»åŠ¡ ID åˆ°å½“å‰ä»»åŠ¡çš„æ˜ å°„
        current_task_map = {task.get("id"): task for task in current_task_list}
        
        # æ¸…ç†ä¾èµ–å…³ç³»å¹¶åˆå¹¶çŠ¶æ€
        kept_task_ids = {task.get("id") for task in updated_plan}
        merged_plan = []
        
        for task in updated_plan:
            task_id = task.get("id")
            # ğŸ”¥ å…³é”®ï¼šä»å½“å‰çŠ¶æ€æŸ¥æ‰¾å¯¹åº”çš„ä»»åŠ¡ï¼Œä¿ç•™ task_id (Commander ID)
            existing_task = current_task_map.get(task_id)
            
            # å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œä¿ç•™å®Œæ•´çŠ¶æ€ï¼ˆåŒ…æ‹¬ output_result å’Œ task_idï¼‰
            if existing_task and existing_task.get("status") == "completed":
                merged_task = dict(existing_task)
            else:
                # æ–°ä»»åŠ¡æˆ–å¾…æ‰§è¡Œä»»åŠ¡ï¼Œä½¿ç”¨å‰ç«¯æ•°æ®ä½†ä¿ç•™å·²æœ‰è¾“å‡º
                merged_task = dict(task)
                if existing_task:
                    # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿ç•™ task_id (Commander ID) å’Œ output_result
                    merged_task["task_id"] = existing_task.get("task_id") or task.get("task_id")
                    merged_task["output_result"] = existing_task.get("output_result")
                    merged_task["status"] = existing_task.get("status", task.get("status", "pending"))
            
            # ğŸ”¥ å…œåº•ï¼šç¡®ä¿ task_id å­—æ®µå­˜åœ¨ï¼ˆå¦‚æœå‰ç«¯æ²¡ä¼ ï¼Œä»ç°æœ‰çŠ¶æ€å¤åˆ¶ï¼‰
            if not merged_task.get("task_id") and existing_task:
                merged_task["task_id"] = existing_task.get("task_id")
            
            # æ¸…ç†ä¾èµ–å…³ç³»
            if merged_task.get("depends_on"):
                cleaned_deps = [
                    dep for dep in merged_task["depends_on"]
                    if dep in kept_task_ids
                ]
                merged_task["depends_on"] = cleaned_deps if cleaned_deps else None
            
            merged_plan.append(merged_task)
        
        # è®¡ç®—æ­£ç¡®çš„ current_task_indexï¼ˆç¬¬ä¸€ä¸ªå¾…æ‰§è¡Œä»»åŠ¡çš„ä½ç½®ï¼‰
        next_task_index = 0
        for idx, task in enumerate(merged_plan):
            if task.get("status") != "completed":
                next_task_index = idx
                break
        else:
            # æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆäº†
            next_task_index = len(merged_plan)
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ·»åŠ  HumanMessage è§¦å‘æµç¨‹ç»§ç»­
        current_messages = current_values.get("messages", [])
        approval_message = HumanMessage(content="è®¡åˆ’å·²å®¡æ ¸é€šè¿‡ï¼Œè¯·æŒ‰æ–°è®¡åˆ’æ‰§è¡Œä»»åŠ¡ã€‚")
        updated_messages = list(current_messages) + [approval_message]
        
        # æ›´æ–° LangGraph çŠ¶æ€ï¼ˆä¿ç•™å·²å®Œæˆä»»åŠ¡çš„ç»“æœï¼‰
        await graph.aupdate_state(config, {
            "task_list": merged_plan,
            "current_task_index": next_task_index,  # ğŸ”¥ ä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•ï¼Œè€Œä¸æ˜¯é‡ç½®ä¸º 0
            "messages": updated_messages,
            "expert_results": current_expert_results  # ğŸ”¥ ä¿ç•™å·²æœ‰ç»“æœï¼Œè€Œä¸æ˜¯æ¸…ç©º
        })
    
    # ============================================================================
    # äº‹ä»¶è½¬æ¢å’Œæ„å»º
    # ============================================================================
    
    def transform_langgraph_event(self, token, message_id: Optional[str] = None) -> Optional[str]:
        """å°† LangGraph äº‹ä»¶è½¬æ¢ä¸º SSE æ ¼å¼"""
        import json
        
        # ğŸ”¥ ä¿®å¤ï¼štoken å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å…¶ä»–ç±»å‹ï¼Œéœ€è¦å®‰å…¨æ£€æŸ¥
        if not isinstance(token, dict):
            return None
        
        event_type = token.get("event", "")
        
        # ğŸ”¥ ä¿®å¤ï¼šè¿‡æ»¤æ‰ router èŠ‚ç‚¹çš„æ‰€æœ‰ LLM äº‹ä»¶
        # Router åªè´Ÿè´£å†³ç­–ï¼Œä¸åº”è¯¥æœ‰ä»»ä½•æ¶ˆæ¯æµå¼è¾“å‡º
        # LangGraph çš„ add_messages reducer ä¼šè‡ªåŠ¨å°† LLM response æ·»åŠ åˆ° messages åˆ—è¡¨
        # æˆ‘ä»¬éœ€è¦åœ¨äº‹ä»¶å±‚é¢è¿‡æ»¤æ‰è¿™äº›å†…å®¹
        if event_type.startswith("on_chat_model"):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ router ç›¸å…³çš„äº‹ä»¶
            # å¯èƒ½é€šè¿‡ name æˆ– tags æ ‡è¯†
            name = token.get("name", "")
            metadata = token.get("metadata", {})
            tags = metadata.get("tags", [])
            
            # æ£€æŸ¥ run_id æ˜¯å¦ä¸ router ç›¸å…³
            run_id = token.get("run_id", "")
            
            # å¦‚æœäº‹ä»¶å…³è”çš„æ˜¯ router èŠ‚ç‚¹ï¼Œè¿‡æ»¤æ‰
            if "router" in name or "router" in str(tags).lower():
                logger.debug(f"[transform_langgraph_event] è¿‡æ»¤ router äº‹ä»¶: {event_type}")
                return None
            
            # ğŸ”¥ é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæ˜¯ on_chat_model_endï¼Œæ£€æŸ¥ content æ˜¯å¦æ˜¯ JSON æ ¼å¼çš„ decision
            if event_type == "on_chat_model_end":
                data = token.get("data", {}) or {}
                output = data.get("output", {})
                if output and isinstance(output, dict) and "content" in output:
                    content = output["content"]
                    # å¦‚æœ content æ˜¯ { "decision_type": "..." } æ ¼å¼ï¼Œè¿‡æ»¤æ‰
                    if isinstance(content, str) and ('"decision_type"' in content or '{"decision_type"' in content):
                        logger.debug(f"[transform_langgraph_event] è¿‡æ»¤ router decision JSON: {content[:50]}...")
                        return None
        
        # å¤„ç†æ¶ˆæ¯æµ
        if event_type == "on_chat_model_stream":
            data = token.get("data", {})
            chunk = data.get("chunk")
            if chunk and hasattr(chunk, "content") and chunk.content:
                # ğŸ”¥ğŸ”¥ğŸ”¥ P0çƒ­ä¿®ï¼šä¸¥æ ¼è¿‡æ»¤ commander å’Œ expert èŠ‚ç‚¹çš„ message.delta
                # è¿™äº›èŠ‚ç‚¹çš„å†…å®¹åº”é€šè¿‡ä¸“ç”¨äº‹ä»¶å‘é€ï¼ˆplan.thinking/artifact.chunkï¼‰
                # åªæœ‰ aggregator èŠ‚ç‚¹å…è®¸å‘é€ message.delta
                metadata = token.get("metadata", {})
                tags = metadata.get("tags", [])
                node_type = metadata.get("node_type", "")
                
                # æ‹¦æˆªæ¡ä»¶1ï¼šæ˜ç¡®çš„èŠ‚ç‚¹ç±»å‹ä¸º commander æˆ– expert
                if node_type in ["commander", "expert"]:
                    logger.debug(f"[transform_langgraph_event] æ‹¦æˆª {node_type} èŠ‚ç‚¹çš„ message.delta: {chunk.content[:50]}...")
                    return None
                
                # æ‹¦æˆªæ¡ä»¶2ï¼šåŒ…å« streaming å’Œ generic_worker æ ‡ç­¾ï¼ˆå‘åå…¼å®¹ï¼‰
                if "streaming" in tags and "generic_worker" in tags:
                    logger.debug(f"[transform_langgraph_event] GenericWorker æµå¼ä¸“å®¶å†…å®¹è·³è¿‡ message.delta: {chunk.content[:50]}...")
                    return None
                
                # æ‹¦æˆªæ¡ä»¶3ï¼šrouter èŠ‚ç‚¹çš„ä»»ä½•æ¶ˆæ¯ï¼ˆé¢å¤–ä¿é™©ï¼‰
                if "router" in tags or node_type == "router":
                    logger.debug(f"[transform_langgraph_event] æ‹¦æˆª router èŠ‚ç‚¹çš„ message.delta")
                    return None
                
                # åªå‘é€çº¯å‡€æ•°æ®ï¼ŒåŒ…å« message_id ç”¨äºå‰ç«¯æ¶ˆæ¯å…³è”
                # æ³¨æ„ï¼šåªæœ‰ aggregator èŠ‚ç‚¹ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ
                event_data = {"content": chunk.content}
                if message_id:
                    event_data["message_id"] = message_id
                logger.debug(f"[transform_langgraph_event] å…è®¸ message.delta (node_type={node_type}, tags={tags}): {chunk.content[:50]}...")
                return f"event: message.delta\ndata: {json.dumps(event_data)}\n\n"
        
        # å¤„ç† chain äº‹ä»¶
        if event_type == "on_chain_start":
            name = token.get("name", "")
            if name == "generic":
                data = token.get("data", {}) or {}
                input_data = data.get("input", {}) or {}
                task_list = input_data.get("task_list", [])
                current_index = input_data.get("current_task_index", 0)
                if task_list and current_index < len(task_list):
                    task = task_list[current_index]
                    # åªå‘é€çº¯å‡€æ•°æ®ï¼Œä¸åŒ…å« type åŒ…è£…
                    event_data = {
                        "task_id": task.get("id"),
                        "expert_type": task.get("expert_type"),
                        "description": task.get("description"),
                        "started_at": datetime.now().isoformat()
                    }
                    return f"event: task.started\ndata: {json.dumps(event_data)}\n\n"
        
        if event_type == "on_chain_end":
            name = token.get("name", "")
            data = token.get("data", {}) or {}
            output = data.get("output", {}) or {}
            
            # ğŸ”¥ æ³¨æ„ï¼ševent_queue ä¸­çš„äº‹ä»¶å·²åœ¨ handle_langgraph_stream ä¸­å¤„ç†
            # è¿™é‡Œåªå¤„ç†é event_queue çš„äº‹ä»¶ï¼ˆå¦‚ generic workerã€aggregatorï¼‰
            
            # å¤„ç† generic worker å®Œæˆ
            if name == "generic" and output and isinstance(output, dict):
                task_result = output.get("__task_result", {})
                if task_result:
                    # åªå‘é€çº¯å‡€æ•°æ®ï¼Œä¸åŒ…å« type åŒ…è£…
                    event_data = {
                        "task_id": task_result.get("task_id"),
                        "expert_type": task_result.get("expert_type"),
                        "status": "completed",
                        "completed_at": datetime.now().isoformat()
                    }
                    return f"event: task.completed\ndata: {json.dumps(event_data)}\n\n"
            
            # aggregator å®Œæˆï¼šmessage.done ç”± aggregator_node é€šè¿‡ event_queue å‘é€
            # è¿™é‡Œä¸å†é‡å¤å‘é€
        
        return None
    
    def _build_message_delta_event(self, message_id: str, content: str) -> str:
        """æ„å»º message.delta äº‹ä»¶"""
        import json
        from event_types.events import EventType, MessageDeltaData, build_sse_event
        from utils.event_generator import sse_event_to_string
        
        event = build_sse_event(
            EventType.MESSAGE_DELTA,
            MessageDeltaData(message_id=message_id, content=content),
            str(uuid.uuid4())
        )
        return sse_event_to_string(event)
    
    def _build_message_done_event(self, message_id: str, content: str) -> str:
        """æ„å»º message.done äº‹ä»¶"""
        import json
        from event_types.events import EventType, MessageDoneData, build_sse_event
        from utils.event_generator import sse_event_to_string
        
        event = build_sse_event(
            EventType.MESSAGE_DONE,
            MessageDoneData(message_id=message_id, full_content=content),
            str(uuid.uuid4())
        )
        return sse_event_to_string(event)
    
    def _build_error_event(self, code: str, message: str) -> str:
        """æ„å»º error äº‹ä»¶"""
        import json
        from event_types.events import EventType, ErrorData, build_sse_event
        from utils.event_generator import sse_event_to_string
        
        event = build_sse_event(
            EventType.ERROR,
            ErrorData(code=code, message=message),
            str(uuid.uuid4())
        )
        return sse_event_to_string(event)
    
    def _build_human_interrupt_event(self, thread_id: str, current_plan: List[Dict]) -> str:
        """æ„å»º human.interrupt äº‹ä»¶ (HITL)"""
        import json
        from event_types.events import EventType, HumanInterruptData, build_sse_event
        from utils.event_generator import sse_event_to_string
        
        event = build_sse_event(
            EventType.HUMAN_INTERRUPT,
            HumanInterruptData(
                type='plan_review',
                current_plan=current_plan
            ),
            str(uuid.uuid4())
        )
        return sse_event_to_string(event)
